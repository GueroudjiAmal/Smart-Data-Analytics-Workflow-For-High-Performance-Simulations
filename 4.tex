\chapter{Tools}

\epigraph{\textit{If I have seen further, it is by standing on the shoulders of Giants.}} {Issac Newton}



\section{introduction}
the tools that have been used in this work 
maybe I have to motivate the choice

\section{MPI} \amal{je voulais parler de learned lessons from MPI, mais il y aura un decalage avec la section Dask et PDI}

MPI is already a very known programming model, we will not detail it here, however, we will summarize the most important characteristics that make it extremely successful since its appearance. Unlike several programming models that introduce new complex concepts, MPI was built in relatively small, well-defined, and forward-looking concepts\cite{traff_recent_2012}. In the MPI-1 standard, an MPI process runs a program in his private address space and can communicate either through point-to-point message passing with another process or through collectives. The standard has been then extended in several ways however using MPI may only require knowing a few concepts.   

In addition to the strong base of MPI, it was designed to work with other tools. This characteristic is vital because the complexity of software and hardware keeps increasing. It also supports component-oriented software, thanks to communicators and groups, which is very important in designing modular or hierarchical code. Moreover, MPI is a complete model that can be used to write any parallel algorithm and is portable and can be used on platforms ranging from a laptop to a supercomputer\cite{goos_learning_2001}   

All those characteristics, make MPI perfectly fit high-performance applications and be the most used model in HPC.

\section{\dask Distributed}\label{dask.distributed}
\subsection{Overview}
\dask distributed has been chosen to be used as a task-based framework in this work.  
It has three main components: one or more clients, a scheduler, and one or more workers. The client is the entry point to the \dask cluster; it represents an interface between the end-user and \dask. 
The user writes an almost sequential code using the available APIs in \dask. A task graph can be constructed using written functions based on low-level \texttt{Delayed} and \texttt{Future} APIs, or it can be generated automatically by using the available high-level APIs such as \texttt{dask.array} (see listing~\ref{listnumpy} and listing~\ref{listdask}). The task graph is then submitted to the scheduler by calling specific functions such as \texttt{compute} and \texttt{persist}.    

The scheduler analyses the task graph and checks any connected workers; if so, it sends the ready tasks to the idle workers. The workers are multi-threaded processes that perform the computations, store and share the data. 

The source of the data in \dask is usually a storage system. A classical post hoc workflow can be illustrated in figure~\ref{figdaskarchi}. The code in listing~\ref{listdask} will be run following these steps. The client connects first to the scheduler. It reads the metadata from the parallel file system regarding the HDF5 file \texttt{"data.hdf5"} and the dataset \texttt{"dataset1"}. A \texttt{dask.array} object is created using the descriptor. By calling \texttt{mean} method, the task graph is created. Concretely, the \texttt{numpy.mean} function is called on each chunk, and an aggregation tree is created to compute the mean of the whole array. The task graph is submitted to the scheduler, which sends the tasks without any dependencies to the workers first. In this case, those tasks are \texttt{getitem} tasks that read data from the file. 

Note that in \dask, it's only at this stage that the concrete dataset is read, which makes it possible to process data larger than one node's memory in parallel.

Once all the tasks are computed, the result is returned to the client.

\begin{lstlisting}[float, label=listnumpy, language=python, caption=Sequential post hoc mean using numpy]
import numpy as np
import h5py
# Load data from HDF5
dataset1 = h5py.File('data.hdf5',mode='r')['dataset1']
# Compute the mean of the array
computed_mean = np.array(dataset1).mean()
print("Computed mean : ", computed_mean)
\end{lstlisting}

\begin{lstlisting}[float, label=listdask, language=python, caption=Parallel post hoc mean with \dask. Lines differing from the analysis of Listing~\ref{listnumpy} are highlighted]
|\hlline|import dask.array as da
import h5py
|\hlline|# Connect to Dask
|\hlline|client = dask.distributed.Client(address)
# Build a lazy array descriptor from HDF5
dataset1 = h5py.File('data.hdf5',mode='r')['dataset1']|\label{post:hdf5}|
|\hlline|dataset1 = da.from_array(dataset1, chunks=(1,4096,4096))|\label{post:chunk}|
computed_mean = dataset1.mean.compute()
print("Computed mean : ", computed_mean)
\end{lstlisting}

\begin{figure}[tb]\centering
\includegraphics[scale=0.8]{figures/DaskArchiecture.pdf}
\caption{\dask Architecture in a typical post hoc context, one client, $N$ workers connected to the scheduler. 1) The client reads small metadata regarding the needed files from the PFS, 2) creates the \dask data structure and submits a task graph, 3) the workers execute the tasks, 4) some of them read data blocks in parallel from the PFS}
\label{figdaskarchi}
\end{figure}

\subsection{\dask Collections and Futures}

Among other advantages, \dask supports parallel versions of familiar libraries such as \texttt{Numpy} and \texttt{Pandas}, respectively, \texttt{dask.array} and \texttt{dask.dataframe}, with almost similar APIs. 
Thus, they become usable in larger-than-memory problems. In this work, we have been particularly interested in \textit{dask.array}. 

A \textit{dask.array}\cite{rocklin_dask_2015} is a larger-than-memory numpy-like array, it is constructed by smaller blocks of \texttt{numpy} arrays called chunks. Operation in a \textit{dask.array} generate automatically a corresponding task graph, thus called a high-level collection. 

The user can use the \textit{delayed} API to customize and create his own task graph, it may be advantageous when the user has particular processing which is not covered by the high-level collections. 
Moreover, \dask implements a \texttt{future} API, which is immediate rather than lazy.



\subsection{Tasks in \dask Distributed}
A task is a central concept in \dask and all task-based frameworks; it is the smallest piece of work that can be submitted to the scheduler and run by a worker. 

In this part, we will detail this concept in \dask, starting with task graph creation, the task state transition, and finally the task scheduling. We will define and discuss the \textit{pure data tasks} and how they were used in our work.



\subsubsection{Task Graph}\label{sec:taskgraph}

Every script submitted to \dask scheduler is translated first to a task graph, specifically a directed acyclic graph of tasks with data dependencies. The graph is represented as a dictionary, where the keys are any hashable value that is not a task, and the values are computations. 

A task is a tuple that has a callable as a first element, followed by a list of arguments, and an argument may be any valid computation. A computation may be a key present in the graph, a value such as an integer, a task, or a list of computations. For instance \texttt{('function1', arg1, arg2, arg3)} is a task that applies \texttt{'function1'} to \texttt{(arg1, arg2, arg3)} where the arguments are valid computations such as: \texttt{{arg1: 1, arg2: ('function2', arg4), arg3: [('function3', arg5, 0.2), 5]}}.

The task \texttt{('function1', arg1, arg2, arg3)} is equivalent to run \texttt{function1(arg1, arg2, arg3)}, by moving the opening parenthesis one term to the left, the execution of \texttt{function1} is delayed. This representation allows \dask to store this computation as data that can be analyzed by the scheduler later rather than cause immediate execution.   

\subsubsection{Task journey}

Task graph creation is not the only step that is done before getting to the scheduler.
Either the graph is constructed using high-level APIs or \texttt{Delayed} it goes through the following steps:
\begin{itemize}
    \item graph creation: as already presented in~\ref{sec:taskgraph}, the graph is encoded using a python dictionary, and it may include millions of entries. This step is done on the client side, 
    \item graph optimization: \dask tries to optimize the graph and eliminate unnecessary work. This may take some time if the graph is large,
    \item graph serialization: the graph needs to be sent from the client to the scheduler and then to the workers, so it must be converted into bytes before sending it. This is done in the serialization step. 
    \item graph communication: once the graph is serialized on the client side, it is sent to the scheduler.
\end{itemize}
These steps are done after calling the \texttt{compute/persist} and may take some time if the graph is very large. 
Now that the task graph is on the scheduler side, it populates its internal data structures to be able to analyze and schedule the tasks efficiently in the workers.

\subsubsection{Task states}
In the \dask scheduler, a task can be in one of these 6 states: \texttt{"released", "waiting", "no-worker", "processing", "erred", "memory"}. It can move from one to another following stimulus coming either from a client or a worker. Figure~\ref{figdasktaskstate} shows the different states and transitions in \dask. 

\begin{figure}[tb]\centering
\includegraphics[scale=0.8]{figures/TaskStatesSheduler.pdf}
\caption{\dask task states and transitions \amal{it has been changed since then, \dask documentation and the code, but I still have the old version of \dask :p}}
\label{figdasktaskstate}
\end{figure}

When a task is created, it is in the state \texttt{"released"} which means that is known to the scheduler but not actively computing or in memory. It can pass to the \texttt{"waiting"} state that indicates that this task is waiting for dependencies to arrive in memory. It may also pass to the \texttt{"erred"} state if a depending task erred and to the \texttt{"forgotten"} state if it is not anymore needed by an alive client or dependent task. The \texttt{"no\_worker"} state means that the task is ready to be executed, but there is no worker connected, or no appropriate worker is available. 
A task in the \texttt{"waiting"} state passes the \texttt{"processing"} state when all its dependencies are in memory, and it is assigned to a worker to be computed. If it has been completed correctly, then it passes to the \texttt{"memory"} state. 

The \texttt{"deisa"} has been added in the frame of this work, and it represents external pure data tasks that are arrays of data generated by a running program, more details about this state will be found in the next sections. 

\subsection{Pure Data Tasks}
As already mentioned, usually, the source of the data in \dask is a storage system. However, it is possible for a client to send data to connected workers, either by passing by the scheduler or not. This can be done using the existing method in the client API \texttt{scatter}. The \texttt{scatter} function takes as a mandatory parameter: the data that needs to be sent, and other optional parameters such as a boolean that expresses whether or not to pass by the scheduler and a list of workers to whom the data will be sent. 
\texttt{scatter} returns a \texttt{future} to that data. The key of this \texttt{future} is the key of the equivalent pure data task in the \dask scheduler. That is, the data which is sent via a \texttt{scatter} is also a task.


\subsection{Scheduler Internal State}

The scheduler keeps track of the tasks, alive clients and connected workers in its internal data structures that consist of four main objects: the \texttt{SchedulerState}, \texttt{TaskState}, \texttt{ClientState} and \texttt{WorkerState}
The \texttt{SchedulerState} object contains a global view of the internal state and uses the different classes. The \texttt{TaskState} keeps the state of each task (key, state, dependencies, dependents, ...), \texttt{ClientState} and \texttt{WorkerState} keep the state of a client and a worker, respectively.
Figure~\ref{figdaskinternal} shows the four main classes in the \dask scheduler internal state. 

\begin{figure}[tb]\centering
\includegraphics{figures/DaskScheduler.pdf}
\caption{\dask internal classes \amal{move to the implementation section maybe}}
\label{figdaskinternal}
\end{figure}

\subsection{Scheduling in \dask Distributed}
In this section, we will not discuss the scheduling policies in \dask as it is not in the frame of this work. We will devote it to the \texttt{transition} algorithm, which performs state transition.
A task state transition occurs from stimuli, which are state-changing messages from workers and clients to the scheduler. 
The schedule handles those messages by triggering a \texttt{transition} function. Every state transition is implemented as a separate method in the scheduler, for instance, \texttt{transition\_processing\_memory} is the name of the function that performs the transition from the \textit{"processing"} to the \texttt{"memory"} state.

When a transition function is called, it mainly changes the state of the given task and constructs a dictionary of recommendations for the state transitions of other tasks. In addition to the specific transition functions, there is the \texttt{transitions} that is called, as its name indicates, it triggers several transitions by iterating over the recommendations. 

For example, when the scheduler receives the \texttt{"task-finished"} stimuli. The \texttt{transition\_processing\_memory} is called, it switches the task state from \texttt{"processing"} to \texttt{"memory"} and recommends all dependent tasks to switch to the \texttt{"processing"} state. These recommendations are passed then to the \texttt{transitions} function that iterates over until no more recommendation is added.

A set of messages to the clients and the workers is also constructed and sent. For instance, those may contain results needed by a client or a task to be run by a worker.

\amal{pseudo algo?}

\section{\pdi}\label{sec:pdi}
\subsection{Overview}\label{sec:pdioverview}

\pdi\cite{roussel:hal-01587075} data interface is a lightweight library for data handling. It separates simulation core concerns from cross-cutting concerns. It proposes a declarative way to call external libraries from a configuration file rather than inline the calls in the simulation codes. 
PDI is built around three core concepts: 
\begin{itemize}
    \item data store: when data is shared from the simulation, it is made available to \pdi through the data store,
    \item event subsystem: once the data is available in the data store, the event system notifies the data handler plugin,
    \item plugins: they access the data available in the store and process it.   
\end{itemize}
The data layout, the orchestration of the exchanges, and the configuration of the different plugins are described in the specification tree; figure~\ref {figpdiarchi} shows a structure scheme of \pdi\cite{noauthor_pdi_nodate}.  


\begin{figure}[tb]\centering
\includegraphics[scale=0.8]{figures/PDI_schema.jpg}
\caption{\pdi architecture}
\label{figpdiarchi}
\end{figure}

\subsection{\pdi API and Simulation Instrumentation}
\pdi offers a very simple API; its functions can be grouped into three categories. The initialization and finalization functions respectively \texttt{PDI\_init} and \texttt{PDI\_finalize} are used to setup and finalize PDI by releasing its resources. 
The second category consists of a set of functions used to annotate the code. \texttt{PDI\_share}, \texttt{PDI\_reclaim} are use to respectively share data with \pdi and reclaim it. \texttt{PDI\_expose} does both sharing and reclaiming the data from the data store. \textit{PDI\_event} triggers a PDI event and finally, \texttt{PDI\_multi\_expose} exposes several variables and triggers an event. Listing~\ref{ccode} shows an example of a c code instrumentation with \pdi, \pdi API calls are highlighted.

\begin{lstlisting}[float, label=ccode, language=c, caption=\pdi instrumentation of the C simulation code]
int main( int argc, char* argv[] ) {
  MPI_Init(&argc, &argv);
  |\hlline|PDI_init(PC_parse_path("pdi_spec.yml")); |\label{ccode:PDI_init}|
  int rank; PDI_Comm_rank(MPI_COMM_WORLD, &rank);
  config_t cfg = read_config("simulation.yml");
  // share one-off configuration
  |\hlline|PDI_multi_expose("init", |\label{ccode:expose1}|
  |\hlline|    "cfg",  &cfg,  PDI_OUT, |\label{ccode:cfg}|
  |\hlline|    "rank", &rank, PDI_OUT,
  |\hlline|    NULL); |\label{ccode:expose1-end}|
  // our temperature field
  double* temp = malloc(sizeof(double) * cfg.loc[0] * cfg.loc[1]);
  initialize(temp);
  // main loop
  for (int step=0; ii<nb_steps; ++step) {
    do_compute(temp, MPI_COMM_WORLD);|\label{ccode:compute}|
    // share data at every iteration
    |\hlline|PDI_multi_expose("iter",  |\label{ccode:expose2}|
    |\hlline|    "step", &step, PDI_OUT,
    |\hlline|    "temp", temp,  PDI_OUT,
    |\hlline|    NULL); |\label{ccode:expose2-end}|
    MPI_Barrier(MPI_COMM_WORLD);|\label{ccode:barrier}|
  }
  free(temp);
  |\hlline|PDI_finalize();
  MPI_Finalize();
}
\end{lstlisting}

\subsection{\pdi Specification Tree}
As already mentioned in the \ref{sec:pdioverview}, the specification tree describes the data layout, orchestrates the interactions between the code and \pdi, and contains the plugin's configurations. It is specified in a \texttt{yaml} file and is provided to \pdi at the initialization, for instance, in listing~\ref{ccode} line\ref{ccode:PDI_init} the configuration file name is \texttt{"pdi\_spec.yml"}. 

The specification tree contains three mains sections: the \texttt{metadata}, the \texttt{data} and the \texttt{plugins} section. The \texttt{metadata} section contains small variables whose \pdi keeps a copy of, and it may contain, for example, the sizes of a given array. The \texttt{data} section contains the data layout description. It defines the types of data expected in the store. Those data are not copied by \pdi; only pointers to the data are shared. Finally, the \texttt{plugins} section lists the plugins that will be loaded and their configurations. 

Listing~\ref{pdiymldata} shows an example of a \pdi \texttt{yaml} configuration file where a \texttt{types} section is added, where we can define new types such as \texttt{structures}. In this example we find also the \texttt{metadata} section in line~\ref{pdiymldata:metadata}. The \texttt{data} section starts in line~\ref{pdiymldata:data}, it provides a description of the \texttt{temp} field, (line~\ref{pdiymldata:temp}) including its type, subtype and size (respectively in lines~\ref{pdiymldata:temp.type},~\ref{pdiymldata:temp.subsize},~\ref{pdiymldata:temp.size}). 
Finally, the \texttt{plugin} section starts in line~\ref{pdiymldata:plugin}, it loads one plugin: the \texttt{decl\_hdf5} plugin in line~\ref{pdiymldata:declhdf5}. We will detail the \pdi plugin system in section~\ref{plugins}.

\begin{lstlisting}[float, label=pdiymldata, language=yaml, caption=Data description in \pdi YAML file]
types: #[...] including config_t description
metadata: {step: int, cfg: config_t, rank: int} |\label{pdiymldata:metadata}|
data: |\label{pdiymldata:data}|
  temp: # the main temperature field |\label{pdiymldata:temp}|
    type: array |\label{pdiymldata:temp.type}|
    subtype: double |\label{pdiymldata:temp.subsize}|
    size: [ '$cfg.loc[0]', '$cfg.loc[1]' ]  |\label{pdiymldata:temp.size}|
plugins: |\label{pdiymldata:plugin}|
  decl_hdf5: |\label{pdiymldata:declhdf5}|
  - file: data-$step-$rank.h5 
    write:
      temp:
        when: '$step>0' |\label{pdiymldata:when}|
\end{lstlisting}

\subsection{\pdi Plugins}\label{plugins}
\pdi supports loose coupling of simulation codes with libraries. Those libraries are supported in \pdi as plugins and are configured through the specification tree. \pdi offers a list of built-in plugins ranging from IO-specific ones to more generic data handling tools. It also allows the creation of user-specific plugins if the built-in ones are not enough.     

\subsubsection{Built-in Plugins}
The built-in plugins \pdi provides can be grouped into four categories: 
\begin{itemize}
    \item general purpose: include \texttt{mpi}, \texttt{trace}, \texttt{set\_value}.  
    \item trace and debugging: include \texttt{trace} plugin,
    \item IO: include \texttt{decl’hdf5}, \texttt{decl’NetCDF}, \texttt{SIONlib} plugins
    \item fault tolerance: \texttt{FTI}plugin.
\end{itemize}

In listing~\ref{pdiymldata}, one plugin have been used the \texttt{decl\_hdf5} plugin in line~\ref{pdiymldata:declhdf5}. In this example, a file named \textit{data-step-rank.h5} is written by each process in every step greater than 0. The \texttt{\$-expression} gets the actual value of \texttt{rank} and \texttt{step}.    

\subsubsection{User-defined Plugins}
\pdi is extensible, and the user can add his own plugins. For instance, a \texttt{FlowVR} and a \texttt{Melissa} plugin have been added. \texttt{FlowVR}\cite{} is a middleware based on dataflow paradigm for in situ processing and \texttt{Melissa}\cite{} is an in situ framework for sensibility analysis. In the same context of in situ analytics, a plugin for \texttt{sensei} has been developed, and in this work, we have implemented a new plugin to interface with \dask called \deisa plugin.  



