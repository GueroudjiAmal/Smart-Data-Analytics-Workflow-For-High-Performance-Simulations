@INPROCEEDINGS{deisa,
  author={Gueroudji, Amal and Bigot, Julien and Raffin, Bruno},
  booktitle={2021 IEEE 28th International Conference on High Performance Computing, Data, and Analytics (HiPC)}, 
  title={DEISA: Dask-Enabled In Situ Analytics}, 
  year={2021},
  volume={},
  number={},
  pages={11-20},
  doi={10.1109/HiPC53243.2021.00015}}

@inproceedings{ruadulescu1999complexity,
  title={On the complexity of list scheduling algorithms for distributed-memory systems},
  author={R{\u{a}}dulescu, Andrei and Van Gemund, Arjan JC},
  booktitle={Proceedings of the 13th international conference on Supercomputing},
  pages={68--75},
  year={1999}
}

@misc{puurula_benchmarking_2019,
        title = {Benchmarking {Python} {Distributed} {AI} {Backends} with {Wordbatch}},
        url = {https://towardsdatascience.com/benchmarking-python-distributed-ai-backends-with-wordbatch-9872457b785c},
        abstract = {A comparison of the three major backend schedulers: Spark, Dask and Ray},
        language = {en},
        urldate = {2023-03-18},
        journal = {Medium},
        author = {Puurula, Antti},
        month = jun,
        year = {2019},
        file = {Snapshot:/local/home/ag262995/Zotero/storage/SRIGC2PU/benchmarking-python-distributed-ai-backends-with-wordbatch-9872457b785c.html:text/html},
}



@incollection{heirich2017situ,
  title={In situ visualization with task-based parallelism},
  author={Heirich, Alan and Slaughter, Elliott and Papadakis, Manolis and Lee, Wonchan and Biedert, Tim and Aiken, Alex},
  booktitle={Proceedings of the In Situ Infrastructures on Enabling Extreme-Scale Analysis and Visualization},
  pages={17--21},
  year={2017}
}


@phdthesis{gurhe2021,
url = "http://www.theses.fr/2021LILUI005",
title = "Paradigmes de programmation répartie et parallèle utilisant des graphes de tâches pour supercalculateurs post-pétascale",
author = "Gurhem, Jérôme",
year = "2021",
note = "Thèse de doctorat dirigée par Petiton, Serge Informatique et applications Université de Lille (2018-2021) 2021"
}


@misc{large_2023,
        title = {large parallel/collective data set crashes and lustre striping - {HDF5} - {HDF} {Forum}},
        url = {https://forum.hdfgroup.org/t/large-parallel-collective-data-set-crashes-and-lustre-striping/10602/12},
        urldate = {2023-03-06},
        file = {large parallel/collective data set crashes and lustre striping - HDF5 - HDF Forum:/local/home/ag262995/Zotero/storage/AHI7A7RS/12.html:text/html},
}


@article{Garbet_2010,
doi = {10.1088/0029-5515/50/4/043002},
url = {https://dx.doi.org/10.1088/0029-5515/50/4/043002},
year = {2010},
month = {mar},
publisher = {},
volume = {50},
number = {4},
pages = {043002},
author = {X. Garbet and Y. Idomura and L. Villard and T.H. Watanabe},
title = {Gyrokinetic simulations of turbulent transport},
journal = {Nuclear Fusion},
abstract = {This overview is an assessment of the gyrokinetic framework and simulations to compute turbulent transport in fusion plasmas. It covers an introduction to the gyrokinetic theory, the principal numerical techniques which are being used to solve the gyrokinetic equations, fundamentals in gyrokinetic turbulence and the main results which have been brought by simulations with regard to transport in fusion devices and fluctuation measurements.}
}

@inproceedings{grandgirard:cea-03740685,
  TITLE = {{New advances to prepare GYSELA-X code for exascale global gyrokinetic plasma turbulence simulations: porting on GPU and ARM architectures}},
  AUTHOR = {Grandgirard, Virginie and Obrejan, Kevin and Midou, Dorian and Asahi, Y and Bernard, P-E and Bigot, J and Bourne, E and Dechard, J and Dif- Pradalier, G and Donnel, P and Garbet, X and Gueroudji, A and Hager, G and Murai, H and Ould-Ruis, Yacine and Padioleau, T and Nguyen, L and Peybernes, M and Sarazin, Y and Sato, M and Tsuji, M and Vezolle, P},
  URL = {https://hal-cea.archives-ouvertes.fr/cea-03740685},
  BOOKTITLE = {{PASC22 - Conderence on The Platform for Advanced Scientific Computing}},
  ADDRESS = {B{\^a}le (virtual event), Switzerland},
  ORGANIZATION = {{the Association for Computing Machinery (ACM) and the Swiss National Supercomputing Centre (CSCS)}},
  PAGES = {1-21},
  YEAR = {2022},
  MONTH = Jun,
  PDF = {https://hal-cea.archives-ouvertes.fr/cea-03740685/file/2022-06-27_PASC_VGrandgirard_vfinal.pdf},
  HAL_ID = {cea-03740685},
  HAL_VERSION = {v1},
}

@article{Grandgirard_CPC2016,
 abstract = {},
 author = {V. Grandgirard and J. Abiteboul and J. Bigot and T. Cartier-Michaud and N. Crouseilles and G. Dif-Pradalier and Ch. Ehrlacher and D. Esteve and X. Garbet and Ph. Ghendrih and G. Latu and M. Mehrenberger and C. Norscini and Ch. Passeron and F. Rozar and Y. Sarazin and E. Sonnendrücker and A. Strugarek and D. Zarzoso},
 doi = {10.1016/j.cpc.2016.05.007},
 journal = {Computer Physics Communications},
 pages = {35-68},
 title = {A 5D gyrokinetic full-f global semi-lagrangian code for flux-driven ion turbulence simulations},
 url = {http://dx.doi.org/10.1016/j.cpc.2016.05.007},
 volume = {207},
 year = {2016}
}

@article{Padioleau_2019,
doi = {10.3847/1538-4357/ab0f2c},
url = {https://dx.doi.org/10.3847/1538-4357/ab0f2c},
year = {2019},
month = {apr},
publisher = {The American Astronomical Society},
volume = {875},
number = {2},
pages = {128},
author = {Thomas Padioleau and Pascal Tremblin and Edouard Audit and Pierre Kestener and Samuel Kokh},
title = {A High-performance and Portable All-Mach Regime Flow Solver Code with Well-balanced Gravity. Application to Compressible Convection},
journal = {The Astrophysical Journal},
abstract = {Convection is an important physical process in astrophysics well-studied using numerical simulations under the Boussinesq and/or anelastic approximations. However, these approaches reach their limits when compressible effects are important in the high-Mach flow regime, e.g., in stellar atmospheres or in the presence of accretion shocks. In order to tackle these issues, we propose a new high-performance and portable code called “ARK” with a numerical solver well suited for the stratified compressible Navier–Stokes equations. We take a finite-volume approach with machine precision conservation of mass, transverse momentum, and total energy. Based on previous works in applied mathematics, we propose the use of a low-Mach correction to achieve a good precision in both low and high-Mach regimes. The gravity source term is discretized using a well-balanced scheme in order to reach machine precision hydrostatic balance. This new solver is implemented using the Kokkos library in order to achieve high-performance computing and portability across different architectures (e.g., multi-core, many-core, and GP-GPU). We show that the low-Mach correction allows to reach the low-Mach regime with a much better accuracy than a standard Godunov-type approach. The combined well-balanced property and the low-Mach correction allowed us to trigger Rayleigh–Bénard convective modes close to the critical Rayleigh number. Furthermore, we present 3D turbulent Rayleigh–Bénard convection with low diffusion using the low-Mach correction leading to a higher kinetic energy power spectrum. These results are very promising for future studies of high Mach and highly stratified convective problems in astrophysics.}
}


@misc{noauthor_pdidev_2022,
        title = {pdidev / {PDI} {Data} {Interface} · {GitLab}},
        url = {https://gitlab.maisondelasimulation.fr/pdidev/pdi},
        abstract = {the PDI Data Interface {\textbar}{\textbar} github {\textbar}{\textbar} Documentation {\textbar}{\textbar} slack},
        language = {en},
        urldate = {2023-03-06},
        journal = {GitLab},
        month = dec,
        year = {2022},
        file = {Snapshot:/local/home/ag262995/Zotero/storage/UKILD4LC/decl_hdf5.html:text/html},
}



@misc{io500,
        title = {{IO500} - {Submissions}},
        url = {https://io500.org/submissions/graphs},
        urldate = {2023-03-01},
        file = {IO500 - Submissions:/local/home/ag262995/Zotero/storage/8DLGJ657/graphs.html:text/html},
} 

@misc{infiniband,
        title = {{InfiniBand} {Roadmap} - {Advancing} {InfiniBand}},
        url = {https://www.infinibandta.org/infiniband-roadmap/},
        abstract = {The InfiniBand roadmap is intended to keep the rate of Infiniband performance increase in line with systems-level performance gains.},
        language = {en-US},
        urldate = {2023-03-02},
        journal = {InfiniBand Trade Association},
        author = {{sadmin}},
}


@article{fornberg_generation_1988,
        title = {Generation of finite difference formulas on arbitrarily spaced grids},
        volume = {51},
        issn = {0025-5718, 1088-6842},
        url = {https://www.ams.org/mcom/1988-51-184/S0025-5718-1988-0935077-0/},
        doi = {10.1090/S0025-5718-1988-0935077-0},
        abstract = {Advancing research. Creating connections.},
        language = {en},
        number = {184},
        urldate = {2023-02-26},
        journal = {Mathematics of Computation},
        author = {Fornberg, Bengt},
        year = {1988},
        keywords = {Finite difference coefficients, high-order accuracy},
        pages = {699--706},
        file = {Full Text PDF:/local/home/ag262995/Zotero/storage/34CLWEVU/Fornberg - 1988 - Generation of finite difference formulas on arbitr.pdf:application/pdf},
}



@misc{noauthor_principal_nodate,
        title = {Principal {Component} {Analysis} ({PCA}) {Explained} {\textbar} {Built} {In}},
        url = {https://builtin.com/data-science/step-step-explanation-principal-component-analysis},
        abstract = {Principal Component Analysis (PCA) can help reduce dimensionality in large data sets. Learn how to use PCA and understand how it works.},
        language = {en}, 
        urldate = {2023-02-15},
        file = {Snapshot:/local/home/ag262995/Zotero/storage/XX6N83NT/step-step-explanation-principal-component-analysis.html:text/html},
}    

@misc{noauthor_beginners_nodate,
        title = {A {Beginner}'s {Guide} to {Eigenvectors}, {Eigenvalues}, {PCA}, {Covariance} and {Entropy}},
        url = {http://wiki.pathmind.com/eigenvector},
        abstract = {Eigenvectors and their relationship to matrices in plain language and without a great deal of math.},
        language = {en},
        urldate = {2023-02-15},
        journal = {Pathmind},  
        file = {Snapshot:/local/home/ag262995/Zotero/storage/QMWGQD4M/eigenvector.html:text/html},
} 

@article{ho_principal_nodate,
        title = {{PRINCIPAL} {COMPONENTS} {A} {N} {ALYSI} {S} ({PCA})},
        language = {en},
        journal = {Principal Components Analysis},
        author = {Ho, Steven M},
        file = {Ho - PRINCIPAL COMPONENTS A N ALYSI S (PCA).pdf:/local/home/ag262995/Zotero/storage/LC27EZNH/Ho - PRINCIPAL COMPONENTS A N ALYSI S (PCA).pdf:application/pdf},
}  


@misc{pearson_karl_1901_1430636,
  author       = {Pearson, Karl},
  title        = {{LIII. On lines and planes of closest fit to 
                   systems of points in space}},
  month        = nov,
  year         = 1901,
  publisher    = {Zenodo},
  doi          = {10.1080/14786440109462720},
  url          = {https://doi.org/10.1080/14786440109462720}
}


@article{jolliffe2016principal,
  title={Principal component analysis: a review and recent developments},
  author={Jolliffe, Ian T and Cadima, Jorge},
  journal={Philosophical transactions of the royal society A: Mathematical, Physical and Engineering Sciences},
  volume={374},
  number={2065},
  pages={20150202},
  year={2016},
  publisher={The Royal Society Publishing}
}
@article{10.2307/2333955,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2333955},
 author = {Harold Hotelling},
 journal = {Biometrika},
 number = {3/4},
 pages = {321--377},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Relations Between Two Sets of Variates},
 urldate = {2023-02-15},
 volume = {28},
 year = {1936}
}


@incollection{kramer2016scikit,
  title={Scikit-learn},
  author={Kramer, Oliver},
  booktitle={Machine learning for evolution strategies},
  pages={45--53},
  year={2016},
  publisher={Springer}
}
@article{pedregosa2011scikit,
  title={Scikit-learn: Machine learning in Python},
  author={Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  journal={the Journal of machine Learning research},
  volume={12},
  pages={2825--2830},
  year={2011},
  publisher={JMLR. org}
}
@inproceedings{mckinney2010data,
  title={Data structures for statistical computing in python},
  author={McKinney, Wes and others},
  booktitle={Proceedings of the 9th Python in Science Conference},
  volume={445},
  number={1},
  pages={51--56},
  year={2010},
  organization={Austin, TX}
}

@article{van2011numpy,
  title={The NumPy array: a structure for efficient numerical computation},
  author={Van Der Walt, Stefan and Colbert, S Chris and Varoquaux, Gael},
  journal={Computing in science \& engineering},
  volume={13},
  number={2},
  pages={22--30},
  year={2011},
  publisher={IEEE}
}
@ARTICLE{4160250_python_for_scientific_computing,
  author={Oliphant, Travis E.},
  journal={Computing in Science \& Engineering}, 
  title={Python for Scientific Computing}, 
  year={2007},
  volume={9},
  number={3},
  pages={10-20},
  doi={10.1109/MCSE.2007.58}}

@inproceedings{terraz_melissa,
  TITLE = {{Melissa: Large Scale In Transit Sensitivity Analysis Avoiding Intermediate Files}},
  AUTHOR = {Terraz, Th{\'e}ophile and Ribes, Alejandro and Fournier, Yvan and Iooss, Bertrand and Raffin, Bruno},
  URL = {https://hal.inria.fr/hal-01607479},
  BOOKTITLE = {{The International Conference for High Performance Computing, Networking, Storage and Analysis (Supercomputing)}},
  ADDRESS = {Denver, United States},
  PAGES = {1 - 14},
  YEAR = {2017},
  MONTH = Nov,
  KEYWORDS = {Sensitivity Analysis ; Multi-run Simulations ; Ensemble Simulation ; Sobol' Index ; In Transit Processing},
  PDF = {https://hal.inria.fr/hal-01607479/file/main-Sobol-SC-2017-HALVERSION.pdf},
  HAL_ID = {hal-01607479},
  HAL_VERSION = {v1},
}

@article{bosilca2013parsec,
  title={Parsec: Exploiting heterogeneity to enhance scalability},
  author={Bosilca, George and Bouteiller, Aurelien and Danalis, Anthony and Faverge, Mathieu and H{\'e}rault, Thomas and Dongarra, Jack J},
  journal={Computing in Science \& Engineering},
  volume={15},
  number={6},
  pages={36--45},
  year={2013},
  publisher={IEEE}
}
@article{liaw2018tune,
  title={Tune: A research platform for distributed model selection and training},
  author={Liaw, Richard and Liang, Eric and Nishihara, Robert and Moritz, Philipp and Gonzalez, Joseph E and Stoica, Ion},
  journal={arXiv preprint arXiv:1807.05118},
  year={2018}
}

@misc{noauthor_using_nodate,
        title = {Using {Dask} on {Ray} — {Ray} 2.2.0},
        url = {https://docs.ray.io/en/latest/data/dask-on-ray.html},
        urldate = {2023-01-17},
        file = {Using Dask on Ray — Ray 2.2.0:/local/home/ag262995/Zotero/storage/LEJPT384/dask-on-ray.html:text/html},
}


@inproceedings{liang2018rllib,
  title={RLlib: Abstractions for distributed reinforcement learning},
  author={Liang, Eric and Liaw, Richard and Nishihara, Robert and Moritz, Philipp and Fox, Roy and Goldberg, Ken and Gonzalez, Joseph and Jordan, Michael and Stoica, Ion},
  booktitle={International Conference on Machine Learning},
  pages={3053--3062},
  year={2018},
  organization={PMLR}
}

@misc{noauthor_ray_nodate,
        title = {Ray v2 {Architecture}},
        url = {https://docs.google.com/document/d/1tBw9A4j62ruI5omIJbMxly-la5w4q_TjyJgJL_jN2fI/preview?usp=embed_facebook},
        abstract = {Ray v2 Architecture Ray Team, October 2022  This document is public; please use "Viewing" mode to avoid accidental comments.  The goal of this document is to motivate and overview the design of the Ray distributed system (version 2.0+). It is meant as a handbook for: Ray users with low-level sys...},
        language = {fr},
        urldate = {2023-01-17},
        journal = {Google Docs},
        file = {Snapshot:/local/home/ag262995/Zotero/storage/N79GCAQH/preview.html:text/html},
}

@misc{noauthor_ray_AIR_nodate,
        title = {Ray {AIR} {Technical} {Whitepaper}},
        url = {https://docs.google.com/document/d/1bYL-638GN6EeJ45dPuLiPImA8msojEDDKiBx3YzB4_s/preview?usp=embed_facebook},
        abstract = {Ray AIR Technical Whitepaper Ray Team, August 2022  This document is public; please use "Viewing" mode to avoid accidental comments.  This doc overviews the technical design and value proposition of Ray AI Runtime (AIR), a scalable toolkit for ML applications. It should provide an understanding o...},
        language = {fr},
        urldate = {2023-01-17},
        journal = {Google Docs},
        file = {Snapshot:/local/home/ag262995/Zotero/storage/5PJPSW3J/preview.html:text/html},
}


@article{ejarque_enabling_2022,
        title = {Enabling dynamic and intelligent workflows for {HPC}, data analytics, and {AI} convergence},
        volume = {134},
        issn = {0167-739X},
        url = {https://www.sciencedirect.com/science/article/pii/S0167739X22001364},
        doi = {https://doi.org/10.1016/j.future.2022.04.014},
        abstract = {The evolution of High-Performance Computing (HPC) platforms enables the design and execution of progressively larger and more complex workflow applications in these systems. The complexity comes not only from the number of elements that compose the workflows but also from the type of computations they perform. While traditional HPC workflows target simulations and modelling of physical phenomena, current needs require in addition data analytics (DA) and artificial intelligence (AI) tasks. However, the development of these workflows is hampered by the lack of proper programming models and environments that support the integration of HPC, DA, and AI, as well as the lack of tools to easily deploy and execute the workflows in HPC systems. To progress in this direction, this paper presents use cases where complex workflows are required and investigates the main issues to be addressed for the HPC/DA/AI convergence. Based on this study, the paper identifies the challenges of a new workflow platform to manage complex workflows. Finally, it proposes a development approach for such a workflow platform addressing these challenges in two directions: first, by defining a software stack that provides the functionalities to manage these complex workflows; and second, by proposing the HPC Workflow as a Service (HPCWaaS) paradigm, which leverages the software stack to facilitate the reusability of complex workflows in federated HPC infrastructures. Proposals presented in this work are subject to study and development as part of the EuroHPC eFlows4HPC project.},
        journal = {Future Generation Computer Systems},
        author = {Ejarque, Jorge and Badia, Rosa M. and Albertin, Loïc and Aloisio, Giovanni and Baglione, Enrico and Becerra, Yolanda and Boschert, Stefan and Berlin, Julian R. and D’Anca, Alessandro and Elia, Donatello and Exertier, François and Fiore, Sandro and Flich, José and Folch, Arnau and Gibbons, Steven J. and Koldunov, Nikolay and Lordan, Francesc and Lorito, Stefano and Løvholt, Finn and Macías, Jorge and Marozzo, Fabrizio and Michelini, Alberto and Monterrubio-Velasco, Marisol and Pienkowska, Marta and Puente, Josep de la and Queralt, Anna and Quintana-Ortí, Enrique S. and Rodríguez, Juan E. and Romano, Fabrizio and Rossi, Riccardo and Rybicki, Jedrzej and Kupczyk, Miroslaw and Selva, Jacopo and Talia, Domenico and Tonini, Roberto and Trunfio, Paolo and Volpe, Manuela},
        year = {2022},
        keywords = {Distributed computing, High performance computing, HPC-DA-AI convergence, Parallel programming, Workflow development, Workflow orchestration},
        pages = {414--429},
}


@article{tejedor2017pycompss,
  title={PyCOMPSs: Parallel computational workflows in Python},
  author={Tejedor, Enric and Becerra, Yolanda and Alomar, Guillem and Queralt, Anna and Badia, Rosa M and Torres, Jordi and Cortes, Toni and Labarta, Jes{\'u}s},
  journal={The International Journal of High Performance Computing Applications},
  volume={31},
  number={1},
  pages={66--82},
  year={2017},
  publisher={Sage Publications Sage UK: London, England}
}

@inproceedings{treichler2016dependent_partitioning,
  title={Dependent partitioning},
  author={Treichler, Sean and Bauer, Michael and Sharma, Rahul and Slaughter, Elliott and Aiken, Alex},
  booktitle={Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
  pages={344--358},
  year={2016}
}

@article{babuji2017introducing,
  title={Introducing Parsl: a python parallel scripting library},
  author={Babuji, Yadu and Brizius, Alison and Chard, Kyle and Foster, Ian and Katz, Daniel S and Wilde, Michael and Wozniak, Justin},
  journal={Zenodo2017},
  year={2017}
}

@inproceedings{babuji2018parsl,
  title={Parsl: Scalable Parallel Scripting in Python.},
  author={Babuji, Yadu N and Chard, Kyle and Foster, Ian T and Katz, Daniel S and Wilde, Mike and Woodard, Anna and Wozniak, Justin M},
  booktitle={IWSG},
  year={2018}
}

@book{blackheath2016functional_FRP,
  title={Functional reactive programming},
  author={Blackheath, Stephen},
  year={2016},
  publisher={Simon and Schuster}
}

@inproceedings{wan2000functional_FRP,
  title={Functional reactive programming from first principles},
  author={Wan, Zhanyong and Hudak, Paul},
  booktitle={Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
  pages={242--252},
  year={2000}
}

@article{perez2016functional_FRP,
  title={Functional reactive programming, refactored},
  author={Perez, Ivan and B{\"a}renz, Manuel and Nilsson, Henrik},
  journal={ACM SIGPLAN Notices},
  volume={51},
  number={12},
  pages={33--44},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@book{friedman2016introduction_flink,
  title={Introduction to Apache Flink: stream processing for real time and beyond},
  author={Friedman, Ellen and Tzoumas, Kostas},
  year={2016},
  publisher={" O'Reilly Media, Inc."}
}

@inproceedings{hoque2017dynamic,
  title={Dynamic task discovery in parsec: A data-flow task-based runtime},
  author={Hoque, Reazul and Herault, Thomas and Bosilca, George and Dongarra, Jack},
  booktitle={Proceedings of the 8th Workshop on Latest Advances in Scalable Algorithms for Large-Scale Systems},
  pages={1--8},
  year={2017}
}

@phdthesis{augonnet2010starpu,
  title={StarPU: a runtime system for scheduling tasks over accelerator-based multicore machines},
  author={Augonnet, C{\'e}dric and Thibault, Samuel and Namyst, Raymond},
  year={2010},
  school={INRIA}
}

@article{archipoff2017starpu,
  title={StarPU},
  author={Archipoff, Simon and Augonnet, C{\'e}dric and Aumage, Olivier and Beauchamp, Guillaume and Bramas, B{\'e}renger and Buttari, Alfredo and Cassagne, Adrien and Clet-Ortega, J{\'e}r{\^o}me and Cojean, Terry and Collin, Nicolas and others},
  year={2017}
}

@inproceedings{augonnet2009starpu,
  title={StarPU: a unified platform for task scheduling on heterogeneous multicore architectures},
  author={Augonnet, C{\'e}dric and Thibault, Samuel and Namyst, Raymond and Wacrenier, Pierre-Andr{\'e}},
  booktitle={European Conference on Parallel Processing},
  pages={863--874},
  year={2009},
  organization={Springer}
}

@inproceedings{vora2011hadoop,
  title={Hadoop-HBase for large-scale data},
  author={Vora, Mehul Nalin},
  booktitle={Proceedings of 2011 International Conference on Computer Science and Network Technology},
  volume={1},
  pages={601--605},
  year={2011},
  organization={IEEE}
}

@book{hintjens2013zeromq,
  title={ZeroMQ: messaging for many applications},
  author={Hintjens, Pieter},
  year={2013},
  publisher={" O'Reilly Media, Inc."}
}

@article{carbone2015apache_flink,
  title={Apache flink: Stream and batch processing in a single engine},
  author={Carbone, Paris and Katsifodimos, Asterios and Ewen, Stephan and Markl, Volker and Haridi, Seif and Tzoumas, Kostas},
  journal={Bulletin of the IEEE Computer Society Technical Committee on Data Engineering},
  volume={36},
  number={4},
  year={2015},
  publisher={IEEE Computer Society}
}

@misc{noauthor_ascent_nodate,
        title = {Ascent — {Ascent} 0.8.0 documentation},
        url = {https://ascent.readthedocs.io/en/latest/},
        urldate = {2023-01-09},
}

@InCollection{Larsen_ascent,
  author =    "Matthew Larsen and Eric Brugger and Hank Childs and Cyrus Harrison",
  title =     {{Ascent: A Flyweight In Situ Library for Exascale Simulations}},
  booktitle = {{In Situ Visualization For Computational Science}},
  pages = { 255 -- 279 },
  publisher = {Mathematics and Visualization book series from Springer Publishing},
  year = 2022,
  month = may,
  address = { Cham, Switzerland },
}

@misc{git_sensei_2022,
        title = {{SENSEI}-insitu/{SENSEI}},
        url = {https://github.com/SENSEI-insitu/SENSEI},
        abstract = {SENSEI ∙ Scalable in situ analysis and visualization},
        urldate = {2023-01-09},
        publisher = {SENSEI-insitu},
        month = dec,
        year = {2022},
        note = {original-date: 2021-06-03T15:25:01Z},
        keywords = {insitu, scalable, visualization},
}


@misc{noauthor_sensei_web,
        title = {sensei},
        url = {https://sensei-insitu.org/index.html},
        abstract = {SENSEI in situ},
        language = {en},
        urldate = {2023-01-09},
        journal = {sensei},
}


@inproceedings{kasim_survey_2008,
        address = {Berlin, Heidelberg},
        series = {Lecture {Notes} in {Computer} {Science}},
        title = {Survey on {Parallel} {Programming} {Model}},
        isbn = {978-3-540-88140-7},
        doi = {10.1007/978-3-540-88140-7_24},
        abstract = {The development of microprocessors design has been shifting to multi-core architectures. Therefore, it is expected that parallelism will play a significant role in future generations of applications. Throughout the years, there has been a myriad number of parallel programming models proposed. In choosing a parallel programming model, not only the performance aspect is important, but also qualitative the aspect of how well parallelism is abstracted to developers. A model with a well abstraction of parallelism leads to a higher application-development productivity. In this paper, we propose seven criteria to qualitatively evaluate parallel programming models. Our focus is on how parallelism is abstracted and presented to application developers. As a case study, we use these criteria to investigate six well-known parallel programming models in the HPC community.},
        language = {en},
        booktitle = {Network and {Parallel} {Computing}},
        publisher = {Springer},
        author = {Kasim, Henry and March, Verdi and Zhang, Rita and See, Simon},
        editor = {Cao, Jian and Li, Minglu and Wu, Min-You and Chen, Jinjun},
        year = {2008},
        keywords = {CUDA, distributed memory, Fortress, MPI, OpenMP, Pthreads, shared memory, UPC},    
        pages = {266--275},
        file = {Full Text PDF:/local/home/ag262995/Zotero/storage/33GWVI3C/Kasim et al. - 2008 - Survey on Parallel Programming Model.pdf:application/pdf},
}   

@article{partee2021using_smartsim,
  title={Using machine learning at scale in hpc simulations with smartsim: An application to ocean climate modeling},
  author={Partee, Sam and Ellis, Matthew and Rigazzi, Alessandro and Bachman, Scott and Marques, Gustavo and Shao, Andrew and Robbins, Benjamin},
  journal={arXiv preprint arXiv:2104.09355},
  year={2021}
}

@misc{noauthor_conduit_nodate,
        title = {Conduit — {Conduit} 0.8.5 documentation},
        url = {https://llnl-conduit.readthedocs.io/en/latest/},
        urldate = {2023-01-10},
}


@INPROCEEDINGS{Babel,
  author={Petruzza, Steve and Treichler, Sean and Pascucci, Valerio and Bremer, Peer-Timo},
  booktitle={2018 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
  title={BabelFlow: An Embedded Domain Specific Language for Parallel Analysis and Visualization}, 
  year={2018},
  volume={},
  number={},
  pages={463-473},
  doi={10.1109/IPDPS.2018.00056}}

@inproceedings{triggers_ascent,
author = {Larsen, Matthew and Woods, Amy and Marsaglia, Nicole and Biswas, Ayan and Dutta, Soumya and Harrison, Cyrus and Childs, Hank},
title = {A Flexible System for in Situ Triggers},
year = {2018},
isbn = {9781450365796},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281464.3281468},
doi = {10.1145/3281464.3281468},
abstract = {Triggers are an important mechanism for adapting visualization, analysis, and storage actions. With this work, we describe the Ascent in situ infrastructure's system for triggers. This system splits triggers into two components: when to perform an action and what actions to perform. The decision for when to perform an action can be based on different types of factors, such as mesh topology, scalar fields, or performance data. The actions to perform are also varied, ranging from the traditional action of saving simulation state to disk to performing arbitrary visualizations and analyses. We also include details on the implementation and short examples demonstrating how the system can be used.},
booktitle = {Proceedings of the Workshop on In Situ Infrastructures for Enabling Extreme-Scale Analysis and Visualization},
pages = {1–6},
numpages = {6},
location = {Dallas, Texas, USA},
series = {ISAV '18}
}






@misc{bauer_paraview_2016,
        title = {{ParaView} {Catalyst} {Computes} {Particle} {Paths} {In} {Situ}},
        url = {https://blog.kitware.com/paraview-catalyst-computes-particle-paths-in-situ/},
        abstract = {Rotorcraft, such as helicopters, exhibit cyclic behavior as they fly in their respective wakes. This behavior makes it difficult for analysts to evaluate results from ... Read More},
        language = {en-US},
        urldate = {2020-12-26},
        journal = {Kitware Blog},
        author = {Bauer, y and Wissink, rew and Potsdam, Mark and on, Buvana Jayaraman},
        month = oct,
        year = {2016},
        file = {Snapshot:/local/home/ag262995/Zotero/storage/KU3KK85E/paraview-catalyst-computes-particle-paths-in-situ.html:text/html},
}

@misc{noauthor_vtk,
        title = {{VTK} - {The} {Visualization} {Toolkit}},
        url = {https://vtk.org/},
        language = {en-US},
        urldate = {2023-01-09},
}


@misc{cscsch_situ_2022,
        title = {In {Situ} {Analysis} and {Visualization} with {ParaView} {Catalyst} and {Ascent} - {Part} 2},
        url = {https://www.youtube.com/watch?v=SFgest3c-ck},
        urldate = {2023-01-09},
        author = {{cscsch}},
        year = {2022},
}

@misc{noauthor_paraview_nodate,
        title = {{ParaView}: {ParaView} {In} {Situ}},
        url = {https://kitware.github.io/paraview-docs/latest/cxx/group__Insitu.html},
        urldate = {2023-01-09},
        file = {ParaView\: ParaView In Situ:/local/home/ag262995/Zotero/storage/SKHL3TQ8/group__Insitu.html:text/html},
} 

@misc{noauthor_using_paraview,
        title = {Using {ParaView} for {High}-{Performance} {Computing} + {In} {Situ}},
        url = {https://www.paraview.org/hpc-insitu/},
        abstract = {The ParaView team continues to be on the cutting edge of using high-performance computing to address your visualization needs.},
        language = {en-US},
        urldate = {2023-01-09},
        file = {Snapshot:/local/home/ag262995/Zotero/storage/6NKZ4H7A/hpc-insitu.html:text/html},
}


@article{thoman_taxonomy_2018,
        title = {A taxonomy of task-based parallel programming technologies for high-performance computing},
        volume = {74},
        issn = {0920-8542, 1573-0484},
        url = {http://link.springer.com/10.1007/s11227-018-2238-4},
        doi = {10.1007/s11227-018-2238-4},
        language = {en},
        number = {4},
        urldate = {2023-01-06},
        journal = {The Journal of Supercomputing},
        author = {Thoman, Peter and Dichev, Kiril and Heller, Thomas and Iakymchuk, Roman and Aguilar, Xavier and Hasanov, Khalid and Gschwandtner, Philipp and Lemarinier, Pierre and Markidis, Stefano and Jordan, Herbert and Fahringer, Thomas and Katrinis, Kostas and Laure, Erwin and Nikolopoulos, Dimitrios S.},
        month = apr,
        year = {2018},
        pages = {1422--1434},
        file = {Thoman et al. - 2018 - A taxonomy of task-based parallel programming tech.pdf:/local/home/ag262995/Zotero/storage/BFAL3AQS/Thoman et al. - 2018 - A taxonomy of task-based parallel programming tech.pdf:application/pdf},
} 

@article{dean2008mapreduce,
  title={MapReduce: simplified data processing on large clusters},
  author={Dean, Jeffrey and Ghemawat, Sanjay},
  journal={Communications of the ACM},
  volume={51},
  number={1},
  pages={107--113},
  year={2008},
  publisher={ACM New York, NY, USA}
}

@article{ketata_parallel_2016,
        title = {Parallel {Programming} {Models}: {A} {Survey}},
        volume = {4},
        abstract = {Parallel programming and the design of efficient parallel programs is a development area of growing importance. Parallel programming models are almost used to integrate parallel software concepts into a sequential code. These models represent an abstraction of the hardware capabilities to the programmer. In fact, a model is a bridge between the application to be parallelized and the machine organization. Up to now, a variety of programming models have been developed, each having its own approach. This paper enumerates various existing parallel programming models in the literature. The purpose is to perform a comparative evaluation of the mostly used ones, namely MapReduce, Cilk, Cilk++, OpenMP and MPI, within some extracted features.},
        language = {en},
        number = {04},
        journal = {International Journal of Engineering Research},
        author = {Ketata, Rim and Kriaa, Lobna and Saidane, Leila Azzouz},
        year = {2016},
        file = {Ketata et al. - 2016 - Parallel Programming Models A Survey.pdf:/local/home/ag262995/Zotero/storage/36KUMJEQ/Ketata et al. - 2016 - Parallel Programming Models A Survey.pdf:application/pdf},
}

@techreport{mpi_report,
author = {Kamburugamuve, Supun and Wickramasinghe, Pulasthi and Ekanayake, Saliya and Fox, Geoffrey},
year = {2017},
month = {01},
pages = {},
title = {Anatomy of machine learning algorithm implementations in MPI, Spark, and Flink}
}
  
@inproceedings{Folk1999HDF5,
  added-at = {2020-03-17T21:21:42.000+0100},
  author = {Folk, Mike and Cheng, Albert and Yates, Kim},
  biburl = {https://www.bibsonomy.org/bibtex/23a90a825aeabbe4334a4c96738e197e9/salotz},
  booktitle = {Proceedings of Supercomputing},
  interhash = {3284af3669cf1506e458e0b67e0aefa3},
  intrahash = {3a90a825aeabbe4334a4c96738e197e9},
  keywords = {software},
  pages = {5--33},
  timestamp = {2020-03-17T21:21:42.000+0100},
  title = {HDF5: A file format and I/O library for high performance computing applications},
  volume = 99,
  year = 1999
}




@INPROCEEDINGS{1592942_netcdf,
  author={Jianwei Li and Wei-keng Liao and Choudhary, A. and Ross, R. and Thakur, R. and Gropp, W. and Latham, R. and Siegel, A. and Gallagher, B. and Zingale, M.},
  booktitle={SC '03: Proceedings of the 2003 ACM/IEEE Conference on Supercomputing}, 
  title={Parallel netCDF: A High-Performance Scientific I/O Interface}, 
  year={2003},
  volume={},
  number={},
  pages={39-39},
  doi={10.1109/SC.2003.10053}}

@inproceedings{mpiio,
author = {Thakur, Rajeev and Gropp, William and Lusk, Ewing},
title = {On Implementing MPI-IO Portably and with High Performance},
year = {1999},
isbn = {1581131232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/301816.301826},
doi = {10.1145/301816.301826},
booktitle = {Proceedings of the Sixth Workshop on I/O in Parallel and Distributed Systems},
pages = {23–32},
numpages = {10},
location = {Atlanta, Georgia, USA},
series = {IOPADS '99}
}


@phdthesis{Estelle_integration_2018,
        type = {Theses},
        title = {Integration of {High}-{Performance} {Task}-{Based} {In} {Situ} for {Molecular} {Dynamics} on {Exascale} {Computers}},
        url = {https://hal.archives-ouvertes.fr/tel-01949170},
        urldate = {2023-01-02},
        school = {Université Grenoble Alpes},
        author = {Dirand, Estelle},
        month = nov,
        year = {2018},
        note = {Issue: 2018GREAM065},
        keywords = {Calcul haute performance, Dynamique moléculaire, High performance computing, In situ, Molecular dynamics, Scientific visualization, Visualisation scientifique},
}


@inproceedings{larsen_strawman_2015,
        address = {Austin TX USA},
        title = {Strawman: {A} {Batch} {In} {Situ} {Visualization} and {Analysis} {Infrastructure} for {Multi}-{Physics} {Simulation} {Codes}},
        isbn = {978-1-4503-4003-8},
        shorttitle = {Strawman},
        url = {https://dl.acm.org/doi/10.1145/2828612.2828625},
        doi = {10.1145/2828612.2828625},
        abstract = {We present Strawman, a system designed to explore the in situ visualization and analysis needs of simulation code teams planning for multi-physics calculations on exascale architectures. Strawman’s design derives from key requirements from a diverse set of simulation code teams, including lightweight usage of shared resources, batch processing, ability to leverage modern architectures, and ease-ofuse both for software integration and for usage during simulation runs. We describe the Strawman system, the key technologies it depends on, and our experiences integrating Strawman into three proxy simulations. Our ﬁndings show that Strawman’s design meets our target requirements, and that some of its concepts may be worthy of integration into our community in situ implementations.},
        language = {en},
        urldate = {2022-10-27},
        booktitle = {Proceedings of the {First} {Workshop} on {In} {Situ} {Infrastructures} for {Enabling} {Extreme}-{Scale} {Analysis} and {Visualization}},
        publisher = {ACM},
        author = {Larsen, Matthew and Brugger, Eric and Childs, Hank and Eliot, Jim and Griffin, Kevin and Harrison, Cyrus},
        month = nov,
        year = {2015},
        pages = {30--35},
        file = {Larsen et al. - 2015 - Strawman A Batch In Situ Visualization and Analys.pdf:/local/home/ag262995/Zotero/storage/AR3I8HN3/Larsen et al. - 2015 - Strawman A Batch In Situ Visualization and Analys.pdf:application/pdf},
}

@INPROCEEDINGS{4919639,
  author={Kulkarni, Milind and Burtscher, Martin and Cascaval, Calin and Pingali, Keshav},
  booktitle={2009 IEEE International Symposium on Performance Analysis of Systems and Software}, 
  title={Lonestar: A suite of parallel irregular programs}, 
  year={2009},
  volume={},
  number={},
  pages={65-76},
  doi={10.1109/ISPASS.2009.4919639}}

  
@article{shankar_looking_2017,
        title = {Looking into the {Black} {Box}: {Holding} {Intelligent} {Agents} {Accountable} [{NUJS} {Law} {Review}]},
        volume = {10},
        shorttitle = {Looking into the {Black} {Box}},
        abstract = {Since the 1950s, mathematicians and scientists have theorised the concept of artificial intelligence and tried to understand the relationship it would have with humans. Although, originally viewed as the creation of human-esque machines, modern artificial intelligence tends to be applied to situations involving complex information and intelligent application of reasoning. Taking many different forms, the information technology industry has begun to actively invest in the creation of artificial intelligence systems at a never-seen-before scale. These systems have already begun to appear in common digital technology available today. The complexity of these systems offers both benefits and dangers to the community at large. A matter of particular concern is the obfuscated nature in which these systems work, creating a 'black box' over the internal functioning of the system, which, in extreme circumstances, could lead to a denial of legal and human rights. Currently, most artificial intelligence systems can be characterised as intelligent agents, as they take into consideration past knowledge, goals, values, and environmental observations to evaluate the situation and take actions appropriately. The conception of artificial intelligence systems as intelligent agents allows for a focused understanding of this novel legal problem , based upon which evaluations relating to accountability can be better framed. In this paper, I will focus on why it is important to hold artificial intelligence accountable and the most significant obstacles that prevent this goal from being achieved.},
        author = {Shankar, Srivats},
        month = jan,
        year = {2017},
        pages = {451},
        file = {Full Text PDF:/local/home/ag262995/Zotero/storage/PMRN57GH/Shankar - 2017 - Looking into the Black Box Holding Intelligent Ag.pdf:application/pdf},
}

@ARTICLE{592312,  
    author={Patterson, D. and Anderson, T. and Cardwell, N. and Fromm, R. and Keeton, K. and Kozyrakis, C. and Thomas, R. and Yelick, K.},  
    journal={IEEE Micro},   
    title={A case for intelligent RAM},   
    year={1997},  
    volume={17},  
    number={2},  
    pages={34-44},  
    doi={10.1109/40.592312}
    }

@misc{lustre,
        title = {Lustre},
        url = {https://www.lustre.org/},
        urldate = {2022-12-02},
        file = {Lustre:/local/home/ag262995/Zotero/storage/A5CVK2M4/www.lustre.org.html:text/html},
}


@article{gpfs,
        title = {{IBM} {Spectrum} {Scale} (formerly {GPFS})},
        language = {en},
        author = {Quintero, Dino and Bolinches, Luis and Chaudhary, Puneet and Davis, Willard and Duersch, Steve and Fachim, Carlos Henrique and Socoliuc, Andrei and Weiser, Olaf},
        pages = {550},
        file = {Quintero et al. - IBM Spectrum Scale (formerly GPFS).pdf:/local/home/ag262995/Zotero/storage/KL6CVVQ2/Quintero et al. - IBM Spectrum Scale (formerly GPFS).pdf:application/pdf},
}


@inproceedings{grandgirard:cea-03740685,
  TITLE = {{New advances to prepare GYSELA-X code for exascale global gyrokinetic plasma turbulence simulations: porting on GPU and ARM architectures}},
  AUTHOR = {Grandgirard, Virginie and Obrejan, Kevin and Midou, Dorian and Asahi, Y and Bernard, P-E and Bigot, J and Bourne, E and Dechard, J and Dif- Pradalier, G and Donnel, P and Garbet, X and Gueroudji, A and Hager, G and Murai, H and Ould-Ruis, Yacine and Padioleau, T and Nguyen, L and Peybernes, M and Sarazin, Y and Sato, M and Tsuji, M and Vezolle, P},
  URL = {https://hal-cea.archives-ouvertes.fr/cea-03740685},
  BOOKTITLE = {{PASC22 - Conderence on The Platform for Advanced Scientific Computing}},
  ADDRESS = {B{\^a}le (virtual event), Switzerland},
  ORGANIZATION = {{the Association for Computing Machinery (ACM) and the Swiss National Supercomputing Centre (CSCS)}},
  PAGES = {1-21},
  YEAR = {2022},
  MONTH = Jun,
  PDF = {https://hal-cea.archives-ouvertes.fr/cea-03740685/file/2022-06-27_PASC_VGrandgirard_vfinal.pdf},
  HAL_ID = {cea-03740685},
  HAL_VERSION = {v1},
}


@article{amal_handling_nodate,
        title = {Handling {IO} data with {PDI} and {Optimizing} away {IO} with {PDI}/{Deisa}},
        language = {en},
        author = {Amal, GUEROUDJI and Julien, BIGOT and Bruno, RAFFIN and Sieroci, Karol and Ould-Rouis, Yacine},
        pages = {24},
        file = {Amal et al. - Handling IO data with PDI and Optimizing away IO w.pdf:/local/home/ag262995/Zotero/storage/FUJ8IHB9/Amal et al. - Handling IO data with PDI and Optimizing away IO w.pdf:application/pdf},
}

@misc{PDSW,
        title = {International {Parallel} {Data} {Systems} {Workshop}},
        url = {http://www.pdsw.org/index.shtml},
        urldate = {2022-12-02},
        file = {International Parallel Data Systems Workshop:/local/home/ag262995/Zotero/storage/QVNG8NMF/index.html:text/html},
}


@misc{hpcda,
        title = {{HPCDA} 2021 ­— workshop on {HPC} \& {HPDA} in situ co-execution},
        url = {https://hpcda.github.io/},
        abstract = {HP-C/DA 2021 Workshop (HPCDA) {\textbar} the workshop on the in situ co-execution of high-performance computing (HPC) and data analysis (HPDA)},
        language = {en-us},
        urldate = {2022-12-02},
        journal = {HPCDA 2021 ­— workshop on HPC \& HPDA in situ co-execution},
        file = {Snapshot:/local/home/ag262995/Zotero/storage/9IGLL866/hpcda.github.io.html:text/html},
}

@misc{compass,
        title = {Programme},
        url = {https://2021.compas-conference.fr//programme/},
        language = {fr},
        urldate = {2022-12-02},
        journal = {Compas 2021},
        file = {Snapshot:/local/home/ag262995/Zotero/storage/SBKJ8CSI/programme.html:text/html},
}

@book{shafi_efficient_2021,
        title = {Efficient {MPI}-based {Communication} for {GPU}-{Accelerated} {Dask} {Applications}},
        abstract = {Dask is a popular parallel and distributed computing framework, which rivals Apache Spark to enable task-based scalable processing of big data. The Dask Distributed library forms the basis of this computing engine and provides support for adding new communication devices. It currently has two communication devices: one for TCP and the other for high-speed networks using UCX-Py -- a Cython wrapper to UCX. This paper presents the design and implementation of a new communication backend for Dask -- called MPI4Dask -- that is targeted for modern HPC clusters built with GPUs. MPI4Dask exploits mpi4py over MVAPICH2-GDR, which is a GPU-aware implementation of the Message Passing Interface (MPI) standard. MPI4Dask provides point-to-point asynchronous I/O communication coroutines, which are non-blocking concurrent operations defined using the async/await keywords from the Python's asyncio framework. Our latency and throughput comparisons suggest that MPI4Dask outperforms UCX by 6x for 1 Byte message and 4x for large messages (2 MBytes and beyond) respectively. We also conduct comparative performance evaluation of MPI4Dask with UCX using two benchmark applications: 1) sum of cuPy array with its transpose, and 2) cuDF merge. MPI4Dask speeds up the overall execution time of the two applications by an average of 3.47x and 3.11x respectively on an in-house cluster built with NVIDIA Tesla V100 GPUs for 1-6 Dask workers. We also perform scalability analysis of MPI4Dask against UCX for these applications on TACC's Frontera (GPU) system with upto 32 Dask workers on 32 NVIDIA Quadro RTX 5000 GPUs and 256 CPU cores. MPI4Dask speeds up the execution time for cuPy and cuDF applications by an average of 1.71x and 2.91x respectively for 1-32 Dask workers on the Frontera (GPU) system.},
        author = {Shafi, Aamir and Hashmi, Jahanzeb and Subramoni, Hari and K., Dhabaleswar and {Panda}},
        month = jan,
        year = {2021},
}



@article{yan_comparison_2018,
        title = {Comparison between pure {MPI} and hybrid {MPI}-{OpenMP} parallelism for {Discrete} {Element} {Method} ({DEM}) of ellipsoidal and poly-ellipsoidal particles},
        volume = {6},
        doi = {10.1007/s40571-018-0213-8},
        abstract = {Parallel computing of 3D Discrete Element Method (DEM) simulations can be achieved in different modes, and two of them are pure MPI and hybrid MPI-OpenMP. The hybrid MPI-OpenMP mode allows flexibly combined mapping schemes on contemporary multiprocessing supercomputers. This paper profiles computational components and floating-point operation features of complex-shaped 3D DEM, develops a space decomposition-based MPI parallelism and various thread-based OpenMP parallelism, and carries out performance comparison and analysis from intranode to internode scales across four orders of magnitude of problem size (namely, number of particles). The influences of memory/cache hierarchy, processes/threads pinning, variation of hybrid MPI-OpenMP mapping scheme, ellipsoid versus poly-ellipsoid are carefully examined. It is found that OpenMP is able to achieve high efficiency in interparticle contact detection, but the unparallelizable code prevents it from achieving the same high efficiency for overall performance; pure MPI achieves not only lower computational granularity (thus higher spatial locality of particles) but also lower communication granularity (thus faster MPI transmission) than hybrid MPI-OpenMP using the same computational resources; the cache miss rate is sensitive to the memory consumption shrinkage per processor, and the last level cache contributes most significantly to the strong superlinear speedup among all of the three cache levels of modern microprocessors; in hybrid MPI-OpenMPI mode, as the number of MPI processes increases (and the number of threads per MPI processes decreases accordingly), the total execution time decreases, until the maximum performance is obtained at pure MPI mode; the processes/threads pinning on NUMA architectures improves performance significantly when there are multiple threads per process, whereas the improvement becomes less pronounced when the number of threads per process decreases; both the communication time and computation time increase substantially from ellipsoids to poly-ellipsoids. Overall, pure MPI outperforms hybrid MPI-OpenMP in 3D DEM modeling of ellipsoidal and poly-ellipsoidal particles.},
        journal = {Computational Particle Mechanics},
        author = {Yan, Beichuan and Regueiro, Richard},
        month = nov,
        year = {2018},
        file = {Full Text PDF:/local/home/ag262995/Zotero/storage/NGY8EZAQ/Yan and Regueiro - 2018 - Comparison between pure MPI and hybrid MPI-OpenMP .pdf:application/pdf},
}         



@article{valiant1990bsp,
  title={A bridging model for parallel computation},
  author={Valiant, Leslie G},
  journal={Communications of the ACM},
  volume={33},
  number={8},
  pages={103--111},
  year={1990},
  publisher={ACM New York, NY, USA}
}
@incollection{goos_learning_2001,
	address = {Berlin, Heidelberg},
	title = {Learning from the {Success} of {MPI}},
	volume = {2228},
	isbn = {978-3-540-43009-4 978-3-540-45307-9},
	url = {http://link.springer.com/10.1007/3-540-45307-5_8},
	abstract = {The Message Passing Interface (MPI) has been extremely successful as a portable way to program high-performance parallel computers. This success has occurred in spite of the view of many that message passing is di cult and that other approaches, including automatic parallelization and directive-based parallelism, are easier to use. This paper argues that MPI has succeeded because it addresses all of the important issues in providing a parallel programming model.},
	language = {en},
	urldate = {2022-09-21},
	booktitle = {High {Performance} {Computing} — {HiPC} 2001},
	publisher = {Springer Berlin Heidelberg},
	author = {Gropp, William D.},
	editor = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Monien, Burkhard and Prasanna, Viktor K. and Vajapeyam, Sriram},
	year = {2001},
	doi = {10.1007/3-540-45307-5_8},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {81--92},
	file = {Gropp - 2001 - Learning from the Success of MPI.pdf:/local/home/ag262995/Zotero/storage/MX4ZLGVD/Gropp - 2001 - Learning from the Success of MPI.pdf:application/pdf},
}

@book{traff_recent_2012,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Recent {Advances} in the {Message} {Passing} {Interface}: 19th {European} {MPI} {Users}’ {Group} {Meeting}, {EuroMPI} 2012, {Vienna}, {Austria}, {September} 23-26, 2012. {Proceedings}},
	volume = {7490},
	isbn = {978-3-642-33517-4 978-3-642-33518-1},
	shorttitle = {Recent {Advances} in the {Message} {Passing} {Interface}},
	url = {http://link.springer.com/10.1007/978-3-642-33518-1},
	language = {en},
	urldate = {2022-09-21},
	publisher = {Springer Berlin Heidelberg},
	editor = {Träff, Jesper Larsson and Benkner, Siegfried and Dongarra, Jack J. and Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
	year = {2012},
	doi = {10.1007/978-3-642-33518-1},
	file = {Träff et al. - 2012 - Recent Advances in the Message Passing Interface .pdf:/local/home/ag262995/Zotero/storage/YISDFM83/Träff et al. - 2012 - Recent Advances in the Message Passing Interface .pdf:application/pdf},
}

@article{ahrens_paraview_2005,
	title = {{ParaView}: {An} {End}-{User} {Tool} for {Large} {Data} {Visualization}},
	shorttitle = {{ParaView}},
	journal = {Visualization Handbook},
	author = {Ahrens, J. and Geveci, Berk and Law, Charles},
	month = jan,
	year = {2005},
	file = {Full Text PDF:/local/home/ag262995/Zotero/storage/BZEIQCFG/Ahrens et al. - 2005 - ParaView An End-User Tool for Large Data Visualiz.pdf:application/pdf}
}

@misc{using_ucxdask,
	title = {Using with {Dask} — ucx-py 0.20.0a+32.g46e361b.dirty documentation},
	url = {https://ucx-py.readthedocs.io/en/latest/dask.html},
	urldate = {2021-05-19},
	file = {Using with Dask — ucx-py 0.20.0a+32.g46e361b.dirty documentation:/local/home/ag262995/Zotero/storage/6QB94YJE/dask.html:text/html}
}

@misc{noauthor_dask_chunk,
	title = {Dask documentation - {Chunks}},
	url = {array-chunks.html},
	abstract = {Chunks},
	language = {en},
	urldate = {2021-05-19},
	journal = {Dask  documentation},
	file = {Snapshot:/local/home/ag262995/Zotero/storage/H78KLE52/array-chunks.html:text/html}
}

@misc{noauthor_openucx,
	title = {{OpenUCX} — {OpenUCX} documentation},
	url = {https://openucx.readthedocs.io/en/master/},
	urldate = {2021-05-19},
	file = {OpenUCX — OpenUCX documentation:/local/home/ag262995/Zotero/storage/2SAGC4W6/master.html:text/html}
}


@inproceedings{barroso_benchmarking_2016_omnipath,
	address = {Padova, Italy},
	title = {Benchmarking message queue libraries and network technologies to transport large data volume in the {ALICE} {O} system},
	isbn = {978-1-5090-2014-0},
	url = {http://ieeexplore.ieee.org/document/7543162/},
	doi = {10.1109/RTC.2016.7543162},
	abstract = {ALICE (A Large Ion Collider Experiment) is the heavy-ion detector designed to study the physics of strongly interacting matter and the quark-gluon plasma at the CERN LHC (Large Hadron Collider).},
	language = {en},
	urldate = {2021-05-19},
	booktitle = {2016 {IEEE}-{NPSS} {Real} {Time} {Conference} ({RT})},
	publisher = {IEEE},
	author = {Barroso, V. Chibante and Fuchs, U. and Wegrzynek, A.},
	month = jun,
	year = {2016},
	pages = {1--5},
	file = {Barroso et al. - 2016 - Benchmarking message queue libraries and network t.pdf:/local/home/ag262995/Zotero/storage/ZDZD5ZZM/Barroso et al. - 2016 - Benchmarking message queue libraries and network t.pdf:application/pdf}
}

@misc{noauthor_pdi_nodate,
	title = {{PDI}: {Core} {Concepts}},
	url = {https://pdi.dev/master/Concepts.html},
	urldate = {2022-09-21},
	file = {PDI\: Core Concepts:/local/home/ag262995/Zotero/storage/QMNIVU5B/Concepts.html:text/html},
}

@unpublished{roussel:hal-01587075,
  TITLE = {{PDI, an approach to decouple I/O concerns from high-performance simulation codes}},
  AUTHOR = {Roussel, Corentin and Keller, Kai and Gaalich, Mohamed and Bautista Gomez, Leonardo and Bigot, Julien},
  URL = {https://hal.archives-ouvertes.fr/hal-01587075},
  NOTE = {working paper or preprint},
  YEAR = {2017},
  MONTH = Sep,
  KEYWORDS = {I/O ; Separation of concern ; HPC ; Checkpointing ; Software- engineering},
  PDF = {https://hal.archives-ouvertes.fr/hal-01587075/file/paper.pdf},
  HAL_ID = {hal-01587075},
  HAL_VERSION = {v1},
}

@article{grandgirard20165d,
  title={A 5D gyrokinetic full-f global semi-Lagrangian code for flux-driven ion turbulence simulations},
  author={Grandgirard, Virginie and Abiteboul, J{\'e}r{\'e}mie and Bigot, Julien and Cartier-Michaud, Thomas and Crouseilles, Nicolas and Dif-Pradalier, Guilhem and Ehrlacher, Ch and Esteve, Damien and Garbet, Xavier and Ghendrih, Ph and others},
  journal={Computer Physics Communications},
  volume={207},
  pages={35--68},
  year={2016},
  publisher={Elsevier}
}

@INPROCEEDINGS{arrayUDF-SC2018,
  author={X. {Xing} and B. {Dong} and J. {Ajo-Franklin} and K. {Wu}},
  booktitle={2018 IEEE/ACM Machine Learning in HPC Environments (MLHPC)}, 
  title={Automated Parallel Data Processing Engine with Application to Large-Scale Feature Extraction}, 
  year={2018},
  volume={},
  number={},
  pages={37-46},
  doi={10.1109/MLHPC.2018.8638638}}

@misc{eniac,
        title = {{ENIAC} team - {Détours}},
        url = {https://detours.canal.fr/saviez-premier-ordinateur-de-lhistoire-a-ete-cree-femmes/eniac-team/},
        urldate = {2022-11-30},
        file = {ENIAC team - Détours:/local/home/ag262995/Zotero/storage/3QH3RL4J/eniac-team.html:text/html},
}


@misc{frontier-image,
        title = {Oak {Ridge}'s exascale '{Frontier}' system named world's most powerful supercomputer on {Top500}},
        url = {https://www.datacenterdynamics.com/en/news/oak-ridges-exascale-frontier-system-named-worlds-most-powerful-supercomputer-on-top500/},
        abstract = {But may still be behind a secret Chinese supercomputer},
        language = {en},
        urldate = {2022-11-30},
        author = {comment, Sebastian Moss Be the first to},
        file = {Snapshot:/local/home/ag262995/Zotero/storage/UCUT2M8C/oak-ridges-exascale-frontier-system-named-worlds-most-powerful-supercomputer-on-top500.html:text/html},
}

@book{schneck_supercomputer_1987,
        address = {Boston, MA},
        series = {The {Kluwer} {International} {Series} in {Engineering} and {Computer} {Science}},
        title = {Supercomputer {Architecture}},
        volume = {31},
        isbn = {978-1-4615-7959-5 978-1-4615-7957-1},
        url = {http://link.springer.com/10.1007/978-1-4615-7957-1},
        language = {en},
        urldate = {2022-11-30},
        publisher = {Springer US},
        author = {Schneck, Paul B.},
        editor = {DeGroot, Doug},
        year = {1987},
        doi = {10.1007/978-1-4615-7957-1},
        file = {Schneck - 1987 - Supercomputer Architecture.pdf:/local/home/ag262995/Zotero/storage/LCFY389G/Schneck - 1987 - Supercomputer Architecture.pdf:application/pdf},
}



@misc{erotokritou_hpc_2019,
        title = {{HPC} supports first black hole image},
        url = {https://prace-ri.eu/hpc-supports-first-black-hole-image/},
        abstract = {On 10 April 2019 the first-ever image of a black hole was unveiled. This once-in-a-lifetime achievement would not have happened without the support of a global team of dedicated scientists, and of European supercomputers.},
        language = {en-US},
        urldate = {2022-11-30},
        journal = {PRACE},
        author = {Erotokritou, Stelios},
        month = nov,
        year = {2019},
        file = {Snapshot:/local/home/ag262995/Zotero/storage/GDA9962R/hpc-supports-first-black-hole-image.html:text/html},
}

@misc{ibm,
        title = {What is {HPC}? {Introduction} to high-performance computing {\textbar} {IBM}},
        shorttitle = {What is {HPC}?},
        url = {https://www.ibm.com/topics/hpc},
        abstract = {High-performance computing (HPC) technology harnesses the power of supercomputers or computer clusters to solve complex problems requiring massive computation.},
        language = {en-us},
        urldate = {2022-11-30},
}

@article{dongarra_trends_2006,
        title = {Trends in high-performance computing: {A} historical overview and examination of future developments},
        volume = {22},
        shorttitle = {Trends in high-performance computing},
        doi = {10.1109/MCD.2006.1598076},
        abstract = {In this article, major recent trends and changes in the high-performance computing (HPC) market place were analyzed. Within the first half of this decade clusters of PCs and workstations have become the prevalent architecture for many HPC application areas in all ranges of performance. However, the Earth Simulator vector system demonstrated that many scientific applications could benefit greatly from other computer architectures. At the same time, there is renewed broad interest in the scientific HPC community for new hardware architectures and new programming paradigms. The IBM BlueGene/L system is one early example of a shifting design focus for large-scale system. The DARPA HPCS program has the declared goal of building a Petaflops computer system by the end of the decade using novel computer architectures.},
        journal = {Circuits and Devices Magazine, IEEE},
        author = {Dongarra, Jack},
        month = feb,
        year = {2006},
        pages = {22--27},
        file = {Full Text PDF:/local/home/ag262995/Zotero/storage/NB9WWH5Y/Dongarra - 2006 - Trends in high-performance computing A historical.pdf:application/pdf},
}

@book{nitecki_influence_2008,
        title = {Influence of funding on advances in librarianship},
        isbn = {978-1-84855-372-9},
        abstract = {Throughout this volume, the influence of research funding on advances in libraries and librarianship is addressed from two perspectives: funding agents and specific initiatives. A collection of chapters concentrate on a number of questions: Do the agendas of those agencies and foundations that fund research in the profession shape the topics of sponsored inquiry and methodologies used to gather evidence for research that advances libraries and librarianship? What are the trends in the questions funded, in the areas of librarianship supported, and perhaps of greatest interest, in the impact funders have made on our understanding of libraries, librarianship, and solving problems that face them? The traditions of 'Advances in Librarianship' offer an appropriate forum to explore these questions through a collection of in depth reviews of the literature and practice.},
        language = {en},
        publisher = {Emerald Group Publishing},
        author = {Nitecki, Danuta A. and Abels, Eileen G.},
        month = dec,
        year = {2008},
        note = {Google-Books-ID: Z9FmRPoqwB8C},
        keywords = {Language Arts \& Disciplines / Library \& Information Science / Archives \& Special Libraries, Language Arts \& Disciplines / Library \& Information Science / Collection Development},
}
~

@article{JISC,
        title = {JISC New Technology Initiative Proposal},
        date = {1992},
        language = {en},
        author = {},
        pages = {},
}
@misc{frontier,
        title = {Frontier},
        url = {https://www.olcf.ornl.gov/frontier/},
        urldate = {2022-11-30},
        file = {Frontier:/local/home/ag262995/Zotero/storage/WS6VLR75/frontier.html:text/html},
}


@misc{top500,
        title = {June 2022 {\textbar} {TOP500}},
        url = {https://www.top500.org/lists/top500/2022/06/},
        urldate = {2022-11-30},
        file = {June 2022 | TOP500:/local/home/ag262995/Zotero/storage/6SD5RQ46/06.html:text/html},
}

@article{hpc,
        title = {High {Performance} {Computing} – {Past}, {Present} and {Future}},
        language = {en},
        author = {Hey, Anthony J G},
        pages = {11},
        file = {Hey - High Performance Computing – Past, Present and Fut.pdf:/local/home/ag262995/Zotero/storage/B5WI3XWZ/Hey - High Performance Computing – Past, Present and Fut.pdf:application/pdf},
}


@misc{ruche,
        title = {Cluster overview - {Mesocentre} {Documentation}},
        url = {https://mesocentre.pages.centralesupelec.fr/user_doc/ruche/01_cluster_overview/},
        urldate = {2022-11-28},
        file = {Cluster overview - Mesocentre Documentation:/local/home/ag262995/Zotero/storage/NGD7XLGL/01_cluster_overview.html:text/html},
}        

@misc{slurm_relative,
        title = {Slurm {Workload} {Manager} - srun},
        url = {https://slurm.schedmd.com/srun.html#OPT_relative},
        urldate = {2022-11-28},
        file = {Slurm Workload Manager - srun:/local/home/ag262995/Zotero/storage/GUNJQ2F5/srun.html:text/html},
}   
@inproceedings{Heirich-isav2017,
 author = {A. Heirich and E. Slaughter and M. Papadakis and W. Lee  and T. Biedert and A. Aiken},
 title = {In Situ Visualization with Task-based Parallelism},
 booktitle = {Workshop on In Situ Infrastructures on Enabling Extreme-Scale Analysis and Visualization (ISAV'17)},
 series = {ISAV'17},
 year = {2017},
 isbn = {978-1-4503-5139-3},
 location = {Denver, CO, USA},
 pages = {17--21},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/3144769.3144771},
 doi = {10.1145/3144769.3144771},
 acmid = {3144771},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Legion, exascale, image compositor, in situ visualization, sort-last, task-based},
} 

@inproceedings{zheng2013goldrush,
 author = {Zheng, Fang and Yu, Hongfeng and Hantas, Can and Wolf, Matthew and Eisenhauer, Greg and Schwan, Karsten and Abbasi, Hasan and Klasky, Scott},
 title = {{G}old{R}ush: {R}esource {E}fficient in {S}itu {S}cientific {D}ata {A}nalytics {U}sing {F}ine-grained {I}nterference {A}ware {E}xecution},
 booktitle = {Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},
 series = {SC '13},
 year = {2013},
 isbn = {978-1-4503-2378-9},
 location = {Denver, Colorado},
 pages = {78:1--78:12},
 articleno = {78},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/2503210.2503279},
 doi = {10.1145/2503210.2503279},
 acmid = {2503279},
 publisher = {ACM},
 address = {New York, NY, USA},
} 


@article{tejedor_compss,
        title = {Infrastructure-{Agnostic} {Programming} and {Interoperable} {Execution} in {Heterogeneous} {Grids}},
        abstract = {In distributed environments, no matter the type of infrastructure (cluster, grid, cloud), portability of applications and interoperability are always a major concern. Such infrastructures have a high variety of characteristics, which brings a need for systems that abstract the application from the particular details of each infrastructure. In addition, managing parallelisation and distribution also complicates the work of the programmer.},
        language = {en},
        author = {Tejedor, Enric},
        pages = {12},
        file = {Tejedor - Infrastructure-Agnostic Programming and Interopera.pdf:/local/home/ag262995/Zotero/storage/IA2KSYH9/Tejedor - Infrastructure-Agnostic Programming and Interopera.pdf:application/pdf},
}

@inproceedings{slaughter_pygion_2019,
	address = {Denver, CO, USA},
	title = {Pygion: {Flexible}, {Scalable} {Task}-{Based} {Parallelism} with {Python}},
	isbn = {978-1-72815-979-9},
	shorttitle = {Pygion},
	url = {https://ieeexplore.ieee.org/document/9062721/},
	doi = {10.1109/PAW-ATM49560.2019.00011},
	language = {en},
	urldate = {2021-05-24},
	booktitle = {2019 {IEEE}/{ACM} {Parallel} {Applications} {Workshop}, {Alternatives} {To} {MPI} ({PAW}-{ATM})},
	publisher = {IEEE},
	author = {Slaughter, Elliott and Aiken, Alex},
	month = nov,
	year = {2019},
	pages = {58--72},
	file = {Slaughter and Aiken - 2019 - Pygion Flexible, Scalable Task-Based Parallelism .pdf:/local/home/ag262995/Zotero/storage/NNV83MGT/Slaughter and Aiken - 2019 - Pygion Flexible, Scalable Task-Based Parallelism .pdf:application/pdf}
}


@article{InSituLiuMa:2007,
  author={Kwan-Liu Ma and Chaoli Wang and Hongfeng Yu and Anna Tikhonova},
  title={In-situ processing and visualization for ultrascale simulations},
  journal={Journal of Physics: Conference Series},
  volume={78},
  number={1},
  pages={012043},
  url={http://stacks.iop.org/1742-6596/78/i=1/a=012043},
  year={2007},
  abstract={The growing power of parallel supercomputers gives scientists the ability to simulate more complex problems at higher fidelity, leading to many high-impact scientific advances. To maximize the utilization of the vast amount of data generated by these simulations, scientists also need scalable solutions for studying their data to different extents and at different abstraction levels. As we move into peta- and exa-scale computing, simply dumping as much raw simulation data as the storage capacity allows for post-processing analysis and visualization is no longer a viable approach. A common practice is to use a separate parallel computer to prepare data for subsequent analysis and visualization. A naive realization of this strategy not only limits the amount of data that can be saved, but also turns I/O into a performance bottleneck when using a large parallel system. We conjecture that the most plausible solution for the peta- and exa-scale data problem is to reduce or transform the data in-situ as it is being generated, so the amount of data that must be transferred over the network is kept to a minimum. In this paper, we discuss different approaches to in-situ processing and visualization as well as the results of our preliminary study using large-scale simulation codes on massively parallel supercomputers.}
}

@article{jin2008adaptive_adios,
  title={Adaptive io system (adios)},
  author={Jin, Chen and Klasky, Scott and Hodson, Stephen and Yu, Weikuan and Lofstead, Jay and Abbasi, Hasan and Schwan, Karsten and Wolf, Matthew and Liao, Wei-keng and Choudhary, Alok and others},
  journal={Cray User’s Group},
  year={2008}
}


@article{godoy_adios2_2020,
	title = {{ADIOS} 2: {The} {Adaptable} {Input} {Output} {System}. {A} framework for high-performance data management},
	volume = {12},
	issn = {2352-7110},
	shorttitle = {{ADIOS} 2},
	url = {https://www.sciencedirect.com/science/article/pii/S2352711019302560},
	doi = {10.1016/j.softx.2020.100561},
	abstract = {We present ADIOS 2, the latest version of the Adaptable Input Output (I/O) System. ADIOS 2 addresses scientific data management needs ranging from scalable I/O in supercomputers, to data analysis in personal computer and cloud systems. Version 2 introduces a unified application programming interface (API) that enables seamless data movement through files, wide-area-networks, and direct memory access, as well as high-level APIs for data analysis. The internal architecture provides a set of reusable and extendable components for managing data presentation and transport mechanisms for new applications. ADIOS 2 bindings are available in C++11, C, Fortran, Python, and Matlab and are currently used across different scientific communities. ADIOS 2 provides a communal framework to tackle data management challenges as we approach the exascale era of supercomputing.},
	language = {en},
	urldate = {2021-02-10},
	journal = {SoftwareX},
	author = {Godoy, William F. and Podhorszki, Norbert and Wang, Ruonan and Atkins, Chuck and Eisenhauer, Greg and Gu, Junmin and Davis, Philip and Choi, Jong and Germaschewski, Kai and Huck, Kevin and Huebl, Axel and Kim, Mark and Kress, James and Kurc, Tahsin and Liu, Qing and Logan, Jeremy and Mehta, Kshitij and Ostrouchov, George and Parashar, Manish and Poeschel, Franz and Pugmire, David and Suchyta, Eric and Takahashi, Keichi and Thompson, Nick and Tsutsumi, Seiji and Wan, Lipeng and Wolf, Matthew and Wu, Kesheng and Klasky, Scott},
	month = jul,
	year = {2020},
	keywords = {Data science, Exascale computing, High-performance computing (HPC), In-situ, Luster GPFS file systems, RDMA, Scalable I/O, Staging},
	pages = {100561},
	file = {ScienceDirect Full Text PDF:/local/home/ag262995/Zotero/storage/C6XWMHXF/Godoy et al. - 2020 - ADIOS 2 The Adaptable Input Output System. A fram.pdf:application/pdf;ScienceDirect Snapshot:/local/home/ag262995/Zotero/storage/SQXQ7VTN/S2352711019302560.html:text/html}
}




@inproceedings{libsim11,
 author = {Whitlock, Brad and Favre, Jean M. and Meredith, Jeremy S.},
 title = {{P}arallel {I}n {S}itu {C}oupling of {S}imulation with a {F}ully {F}eatured {V}isualization {S}ystem},
 booktitle = {11th Eurographics conference on Parallel Graphics and Visualization},
 OPTseries = {EG PGV'11},
 year = {2011},
 isbn = {978-3-905674-32-3},
 location = {Llandudno},
 pages = {101--109},
 numpages = {9},
 OPTurl = {http://dx.doi.org/10.2312/EGPGV/EGPGV11/101-109},
 OPTdoi = {10.2312/EGPGV/EGPGV11/101-109},
 acmid = {2386245},
 OPTpublisher = {Eurographics Association},
 OPTaddress = {Aire-la-Ville, Switzerland, Switzerland},
}


@INPROCEEDINGS{catalyst11, 
author={Fabian, N. and Moreland, K. and Thompson, D. and Bauer, A.C. and Marion, P. and Geveci, B. and Rasquin, M. and Jansen, K.E.}, 
booktitle={Large Data Analysis and Visualization Workshop(LDAV'11)}, 
title={{T}he {P}araView {C}oprocessing {L}ibrary: a {S}calable, {G}eneral {P}urpose {I}n {S}itu {V}isualization {L}ibrary}, 
year={2011}, 
OPTmonth={Oct}, 
pages={89-96}, 
keywords={data analysis;data compression;data visualisation;feature extraction;multiprocessing systems;open systems;software libraries;CPU capability;ParaView coprocessing library;data compression;disk write speed;high performance computing;in situ visualization library;interactive post-processing;library scalability;readable data extraction;scientific simulation;Adaptation models;Data mining;Data models;Data visualization;Libraries;Pipelines;Sockets;coprocessing;in situ;scaling;simulation}, 
OPTdoi={10.1109/LDAV.2011.6092322},}


@inproceedings{Larsen-alpine-isav17,
author = {Larsen, Matthew and Ahrens, James and Ayachit, Utkarsh and Brugger, Eric and Childs, Hank and Geveci, Berk and Harrison, Cyrus},
title = {The ALPINE In Situ Infrastructure: Ascending from the Ashes of Strawman},
year = {2017},
isbn = {9781450351393},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3144769.3144778},
doi = {10.1145/3144769.3144778},
abstract = {This paper introduces ALPINE, a flyweight in situ infrastructure. The infrastructure is designed for leading-edge supercomputers, and has support for both distributed-memory and shared-memory parallelism. It can take advantage of computing power on both conventional CPU architectures and on many-core architectures such as NVIDIA GPUs or the Intel Xeon Phi. Further, it has a flexible design that supports for integration of new visualization and analysis routines and libraries. The paper describes ALPINE's interface choices and architecture, and also reports on initial experiments performed using the infrastructure.},
booktitle = {Proceedings of the In Situ Infrastructures on Enabling Extreme-Scale Analysis and Visualization},
pages = {42–46},
numpages = {5},
keywords = {HPC, Scientific Visualization, In Situ},
location = {Denver, CO, USA},
series = {ISAV'17}
}

@inproceedings{10.1145/277650.277725,
author = {Frigo, Matteo and Leiserson, Charles E. and Randall, Keith H.},
title = {The Implementation of the Cilk-5 Multithreaded Language},
year = {1998},
isbn = {0897919874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/277650.277725},
doi = {10.1145/277650.277725},
abstract = {The fifth release of the multithreaded language Cilk uses a provably good "work-stealing" scheduling algorithm similar to the first system, but the language has been completely redesigned and the runtime system completely reengineered. The efficiency of the new implementation was aided by a clear strategy that arose from a theoretical analysis of the scheduling algorithm: concentrate on minimizing overheads that contribute to the work, even at the expense of overheads that contribute to the critical path. Although it may seem counterintuitive to move overheads onto the critical path, this "work-first" principle has led to a portable Cilk-5 implementation in which the typical cost of spawning a parallel thread is only between 2 and 6 times the cost of a C function call on a variety of contemporary machines. Many Cilk programs run on one processor with virtually no degradation compared to equivalent C programs. This paper describes how the work-first principle was exploited in the design of Cilk-5's compiler and its runtime system. In particular, we present Cilk-5's novel "two-clone" compilation strategy and its Dijkstra-like mutual-exclusion protocol for implementing the ready deque in the work-stealing scheduler.},
booktitle = {Proceedings of the ACM SIGPLAN 1998 Conference on Programming Language Design and Implementation},
pages = {212–223},
numpages = {12},
keywords = {critical path, multithreading, work, parallel computing, programming language, runtime system},
location = {Montreal, Quebec, Canada},
series = {PLDI '98}
}

@article{kooburat_extending_nodate,
	title = {Extending {Task}-based {Programming} {Model} beyond {Shared} {Memory} {Systems}},
	abstract = {Task-based programming model allows programmer to easily exploit the computation power of chip multiprocessing because he can express fine-grained parallelism without worrying about overhead. However, it is normally used in shared memory system which allows implicit communication and synchronization. As a result, this programming model cannot be used to efficiently utilize distributed memory system such as commodity cluster computers.},
	language = {en},
	author = {Kooburat, Thawan and Hwang, MinJae},
	pages = {7},
	file = {Kooburat and Hwang - Extending Task-based Programming Model beyond Shar.pdf:/local/home/ag262995/Zotero/storage/77FX3W8Y/Kooburat and Hwang - Extending Task-based Programming Model beyond Shar.pdf:application/pdf},
}

@incollection{BRINSKIY2015305_MPI_3,
title = {Chapter 16 - MPI-3 Shared Memory Programming Introduction},
editor = {James Reinders and Jim Jeffers},
booktitle = {High Performance Parallelism Pearls},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {305-319},
year = {2015},
isbn = {978-0-12-803819-2},
doi = {https://doi.org/10.1016/B978-0-12-803819-2.00025-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128038192000252},
author = {Mikhail Brinskiy and Mark Lubin and James Dinan},
keywords = {Message passing, MPI, Hybrid parallel programming, Shared memory, Remote memory access, RMA, One-sided communication},
abstract = {This chapter discusses an interprocess shared memory extension, which was added in MPI 3.0 standard, and how it can be used to enhance communication efficiency and to enable memory footprint optimization. A simple 1-D ring “hello world” example is shown and then extended for several node runs. Using a modified MPPTEST benchmark, up to 4.9× improvement over a standard point-to-point communication approach on one Intel Xeon Phi coprocessor is shown. This approach is shown to have potential benefits with halo exchanges even for multinode cases. Improvements up to 1.8× with two Intel Xeon Phi coprocessors are shown along with demonstrating that the benefits of MPI SHM grow with increases in the number of intranode neighbors for halo exchanges.}
}

@article{belikov2013survey,
  title={A survey of high-level parallel programming models},
  author={Belikov, Evgenij and Deligiannis, Pantazis and Totoo, Prabhat and Aljabri, Malak and Loidl, Hans-Wolfgang},
  journal={Heriot-Watt University, Edinburgh, UK},
  volume={1},
  number={2},
  pages={2--2},
  year={2013}
}

@article{nestmann_building_2017,
        title = {Building a {Consistent} {Taxonomy} for {Parallel} {Programming} {Models}},
        issn = {1617-5468},
        url = {https://dl.gi.de/handle/20.500.12116/4020},
        doi = {10.18420/IN2017_245},
        abstract = {Parallel programming has been a challenge for developers and software engineers for over two decades now. To lower the complexity of parallel programs, a lot of different parallel programming models (like ACTORs) or supporting libraries (like MPI) have been introduced. These models and libraries support different features and have individual hardware requirements.},
        language = {en},
        urldate = {2023-01-05},
        author = {Nestmann, Markus},
        year = {2017},
        note = {ISBN: 9783885796695
Publisher: Gesellschaft für Informatik, Bonn},
        keywords = {Overview, Parallel Programming Models, Taxonomy},
        file = {Nestmann - 2017 - Building a Consistent Taxonomy for Parallel Progra.pdf:/local/home/ag262995/Zotero/storage/UY8BBCRQ/Nestmann - 2017 - Building a Consistent Taxonomy for Parallel Progra.pdf:application/pdf},
}


@incollection{VITOROVIC2014203_parallel_programming_models,
title = {Chapter Five - Manual Parallelization Versus State-of-the-Art Parallelization Techniques: The SPEC CPU2006 as a Case Study},
editor = {Ali Hurson},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {92},
pages = {203-251},
year = {2014},
issn = {0065-2458},
doi = {https://doi.org/10.1016/B978-0-12-420232-0.00005-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780124202320000052},
author = {Aleksandar Vitorović and Milo V. Tomašević and Veljko M. Milutinović},
keywords = {Parallelization tools, Parallelization techniques, Parallelization patterns, Parallelization overhead prediction model, SPEC CPU2006, Autoparallelization, Manual parallelization},
abstract = {Being multiprocessors (both on-chip and/or off-chip), modern computer systems can automatically exploit the benefits of parallel programs, but their resources remain underutilized in executing still-prevailing sequential applications. An obvious solution is in the parallelization of such applications. The first part overviews the broad issues in parallelization. Various parallelization approaches and contemporary software and hardware tools for extracting parallelism from sequential applications are studied. It also attempts to identify typical code patterns amenable for parallelization. The second part represents a case study where the SPEC CPU2006 suite is considered as a representative collection of typical sequential applications. Following that, it discusses the possibilities and potentials of automatic parallelization and vectorization of the sequential C++ applications from the CPU2006 suite. Since these potentials are generally limited, it explores the issues in manual parallelization of these applications. After previously identified patterns are applied by source-to-source code modifications, the effects of parallelization are evaluated by profiling and executing on two representative parallel machines. Finally, the presented results are carefully discussed.}
}

@inproceedings{gurhem_current_2020_task,
        address = {Cham},
        title = {A {Current} {Task}-{Based} {Programming} {Paradigms} {Analysis}},
        isbn = {978-3-030-50426-7},
        abstract = {Task-based paradigm models can be an alternative to MPI. The user defines atomic tasks with a defined input and output with the dependencies between them. Then, the runtime can schedule the tasks and data migrations efficiently over all the available cores while reducing the waiting time between tasks. This paper focus on comparing several task-based programming models between themselves using the LU factorization as benchmark.},
        booktitle = {Computational {Science} – {ICCS} 2020},
        publisher = {Springer International Publishing},
        author = {Gurhem, Jérôme and Petiton, Serge G.},
        editor = {Krzhizhanovskaya, Valeria V. and Závodszky, Gábor and Lees, Michael H. and Dongarra, Jack J. and Sloot, Peter M. A. and Brissos, Sérgio and Teixeira, João},
        year = {2020},
        pages = {203--216},
}


@article{cilk5,
author = {Frigo, Matteo and Leiserson, Charles E. and Randall, Keith H.},
title = {The Implementation of the Cilk-5 Multithreaded Language},
year = {1998},
issue_date = {May 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {5},
issn = {0362-1340},
url = {https://doi.org/10.1145/277652.277725},
doi = {10.1145/277652.277725},
abstract = {The fifth release of the multithreaded language Cilk uses a provably good "work-stealing" scheduling algorithm similar to the first system, but the language has been completely redesigned and the runtime system completely reengineered. The efficiency of the new implementation was aided by a clear strategy that arose from a theoretical analysis of the scheduling algorithm: concentrate on minimizing overheads that contribute to the work, even at the expense of overheads that contribute to the critical path. Although it may seem counterintuitive to move overheads onto the critical path, this "work-first" principle has led to a portable Cilk-5 implementation in which the typical cost of spawning a parallel thread is only between 2 and 6 times the cost of a C function call on a variety of contemporary machines. Many Cilk programs run on one processor with virtually no degradation compared to equivalent C programs. This paper describes how the work-first principle was exploited in the design of Cilk-5's compiler and its runtime system. In particular, we present Cilk-5's novel "two-clone" compilation strategy and its Dijkstra-like mutual-exclusion protocol for implementing the ready deque in the work-stealing scheduler.},
journal = {SIGPLAN Not.},
month = {may},
pages = {212–223},
numpages = {12},
keywords = {work, multithreading, parallel computing, runtime system, programming language, critical path}
}


@inproceedings{loring-pythoninsitu-isav2018,
author = {Loring, Burlen and Myers, Andrew and Camp, David and Bethel, E. Wes},
title = {Python-Based in Situ Analysis and Visualization},
year = {2018},
isbn = {9781450365796},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281464.3281465},
doi = {10.1145/3281464.3281465},
abstract = {This work focuses on enabling the use of Python-based methods for the purpose of performing in situ analysis and visualization. This approach facilitates access to and use of a rapidly growing collection of Python-based, third-party libraries for analysis and visualization, as well as lowering the barrier to entry for user-written Python analysis codes. Beginning with a simulation code that is instrumented to use the SENSEI in situ interface, we present how to couple it with a Python-based data consumer, which may be run in situ, and in parallel at the same concurrency as the simulation. We present two examples that demonstrate the new capability. One is an analysis of the reaction rate in a proxy simulation of a chemical reaction on a 2D substrate, while the other is a coupling of an AMR simulation to Yt, a parallel visualization and analysis library written in Python. In the examples, both the simulation and Python in situ method run in parallel on a large-scale HPC platform.},
booktitle = {Proceedings of the Workshop on In Situ Infrastructures for Enabling Extreme-Scale Analysis and Visualization},
pages = {19–24},
numpages = {6},
keywords = {Python, in situ visualization, in situ analysis},
location = {Dallas, Texas, USA},
series = {ISAV '18}
}

@misc{noauthor_what_catalyst,
        title = {What is {Catalyst}? — {Catalyst} documentation},
        url = {https://catalyst-in-situ.readthedocs.io/en/latest/introduction.html},
        urldate = {2023-01-09},
}

@misc{catalyst, 
 title= {catalyst user’s guide paraview},
 url={https://www.paraview.org/files/catalyst/docs/ParaViewCatalystUsersGuide_v2.pdf},
 urldate = {2023-01-09}
 } 
@misc{noauthor_visit_works,
        title = {1. {Understanding} how {VisIt} works — {VisIt} {User} {Manual} 3.2.2 documentation},
        url = {https://visit-sphinx-github-user-manual.readthedocs.io/en/develop/intro/Understanding_how_VisIt_works.html},
        urldate = {2023-01-09},
}

@inproceedings{bethel_sensei_2022,
        address = {Cham},
        title = {The {SENSEI} {Generic} {In} {Situ} {Interface}: {Tool} and {Processing} {Portability} at {Scale}},
        isbn = {978-3-030-81627-8},
        abstract = {One key challenge when doing in situ processing is the investment required to add code to numerical simulations needed to take advantage of in situ processing. Such instrumentation code is often specialized, and tailored to a specific in situ method or infrastructure. Then, if a simulation wants to use other in situ tools, each of which has its own bespoke API [4], then the simulation code team will quickly become overwhelmed with having a different set of instrumentation APIs, one per in situ tool or method. In an ideal situation, such instrumentation need happen only once, and then the instrumentation API provides access to a large diversity of tools. In this way, a data producer's instrumentation need not be modified if the user desires to take advantage of a different set of in situ tools. The SENSEI generic in situ interface addresses this challenge, which means that SENSEI-instrumented codes enjoy the benefit of being able to use a diversity of tools at scale, tools that include Libsim, Catalyst, Ascent, as well as user-defined methods written in C++ or Python. SENSEI has been shown to scale to greater than 1M-way concurrency on HPC platforms, and provides support for a rich and diverse collection of common scientific data models. This chapter presents the key design challenges that enable tool and processing portability at scale, some performance analysis, and example science applications of the methods.},
        booktitle = {In {Situ} {Visualization} for {Computational} {Science}},
        publisher = {Springer International Publishing},
        author = {Bethel, E. Wes and Loring, Burlen and Ayachit, Utkarsh and Camp, David and Duque, Earl P. N. and Ferrier, Nicola and Insley, Joseph and Gu, Junmin and Kress, James and O'Leary, Patrick and Pugmire, David and Rizzi, Silvio and Thompson, David and Weber, Gunther H. and Whitlock, Brad and Wolf, Matthew and Wu, Kesheng},
        editor = {Childs, Hank and Bennett, Janine C. and Garth, Christoph},
        year = {2022},
        pages = {281--306},
}

 
@book{childs_situ_libsim_2022,
        title = {In {Situ} {Visualization} for {Computational} {Science}},
        isbn = {978-3-030-81627-8},
        abstract = {This book provides an overview of the emerging field of in situ visualization, i.e. visualizing simulation data as it is generated. In situ visualization is a processing paradigm in response to recent trends in the development of high-performance computers. It has great promise in its ability to access increased temporal resolution and leverage extensive computational power. However, the paradigm also is widely viewed as limiting when it comes to exploration-oriented use cases. Furthermore, it will require visualization systems to become increasingly complex and constrained in usage. As research efforts on in situ visualization are growing, the state of the art and best practices are rapidly maturing. Specifically, this book contains chapters that reflect state-of-the-art research results and best practices in the area of in situ visualization. Our target audience are researchers and practitioners from the areas of mathematics computational science, high-performance computing, and computer science that work on or with in situ techniques, or desire to do so in future.},
        language = {en},
        publisher = {Springer Nature},
        author = {Childs, Hank},
        year = {2022},
        note = {Google-Books-ID: JURuEAAAQBAJ},
}


@misc{noauthor_about_visit,
        title = {About {VisIt}},
        url = {https://visit-dav.github.io/visit-website/about/},
        abstract = {»VisIt« is an Open Source, interactive, scalable, visualization, animation and analysis tool for Unix, Windows and Mac},
        language = {en},
        urldate = {2023-01-09},
        journal = {VisIt Home},
}


 
@misc{paraview_catalyst_examples_2022,
        title = {Kitware/{ParaViewCatalystExampleCode}-{MOVED}-},
        copyright = {BSD-3-Clause},
        url = {https://github.com/Kitware/ParaViewCatalystExampleCode-MOVED-},
        abstract = {Example problems and snippets of code to demonstrate ParaView's Catalyst.},
        urldate = {2023-01-09},
        publisher = {Kitware, Inc.},
        month = nov,
        year = {2022},
        note = {original-date: 2013-03-12T15:18:32Z},
}



@article{schroeder_visualizing_2000_vtk,
	title = {Visualizing with {VTK}: a tutorial},
	volume = {20},
	issn = {1558-1756},
	shorttitle = {Visualizing with {VTK}},
	doi = {10.1109/38.865875},
	abstract = {We introduce basic concepts behind the Visualization Toolkit (VTK). An overview of the system, plus some detailed examples, will assist in learning this system. The tutorial targets researchers of any discipline who have 2D or 3D data and want more control over the visualization process than a turn-key system can provide. It also assists developers who would like to incorporate VTK into an application as a visualization or data processing engine.},
	number = {5},
	journal = {IEEE Computer Graphics and Applications},
	author = {Schroeder, W. J. and Avila, L. S. and Hoffman, W.},
	month = sep,
	year = {2000},
	note = {Conference Name: IEEE Computer Graphics and Applications},
	keywords = {2D data, 3D data, Business, Cameras, Companies, Computer graphics, data processing engine, Data visualization, Linux, Open source software, Programming, Rendering (computer graphics), Tutorial, Visualization, visualization engine, Visualization Toolkit},
	pages = {20--27},
	file = {IEEE Xplore Full Text PDF:/local/home/ag262995/Zotero/storage/9DT5RHBB/Schroeder et al. - 2000 - Visualizing with VTK a tutorial.pdf:application/pdf;IEEE Xplore Abstract Record:/local/home/ag262995/Zotero/storage/MHR8EU4K/865875.html:text/html}
}

@article{moreland_vtk-m_2016,
	title = {{VTK}-m: {Accelerating} the {Visualization} {Toolkit} for {Massively} {Threaded} {Architectures}},
	volume = {36},
	issn = {1558-1756},
	shorttitle = {{VTK}-m},
	doi = {10.1109/MCG.2016.48},
	abstract = {One of the most critical challenges for high-performance computing (HPC) scientific visualization is execution on massively threaded processors. Of the many fundamental changes we are seeing in HPC systems, one of the most profound is a reliance on new processor types optimized for execution bandwidth over latency hiding. Our current production scientific visualization software is not designed for these new types of architectures. To address this issue, the VTK-m framework serves as a container for algorithms, provides flexible data representation, and simplifies the design of visualization algorithms on new and future computer architecture.},
	number = {3},
	journal = {IEEE Computer Graphics and Applications},
	author = {Moreland, K. and Sewell, C. and Usher, W. and Lo, L. and Meredith, J. and Pugmire, D. and Kress, J. and Schroots, H. and Ma, K. and Childs, H. and Larsen, M. and Chen, C. and Maynard, R. and Geveci, B.},
	month = may,
	year = {2016},
	note = {Conference Name: IEEE Computer Graphics and Applications},
	keywords = {Algorithm design and analysis, algorithmic structures, Computational modeling, computer architecture, Computer architecture, computer graphics, data structures, data visualisation, Data visualization, execution bandwidth over latency hiding, flexible data representation, high-performance computing, high-performance computing scientific visualization, HPC scientific visualization, HPC systems, massively threaded architectures, massively threaded processors, Message systems, multi-threading, parallel algorithms, parallel processing, Parallel processing, scientific visualization software, Software engineering, visualization software, visualization toolkit, VTK-m framework},
	pages = {48--58},
	file = {IEEE Xplore Full Text PDF:/local/home/ag262995/Zotero/storage/422WW42Z/Moreland et al. - 2016 - VTK-m Accelerating the Visualization Toolkit for .pdf:application/pdf;IEEE Xplore Abstract Record:/local/home/ag262995/Zotero/storage/ZLHUXE8Y/7466740.html:text/html}
}

@inproceedings {222605_ray,
author = {Philipp Moritz and Robert Nishihara and Stephanie Wang and Alexey Tumanov and Richard Liaw and Eric Liang and Melih Elibol and Zongheng Yang and William Paul and Michael I. Jordan and Ion Stoica},
title = {Ray: A Distributed Framework for Emerging {AI} Applications},
booktitle = {13th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 18)},
year = {2018},
isbn = {978-1-939133-08-3},
address = {Carlsbad, CA},
pages = {561--577},
url = {https://www.usenix.org/conference/osdi18/presentation/moritz},
publisher = {{USENIX} Association},
month = oct,
}


@inproceedings{rocklin_dask_2015,
	address = {Austin, Texas},
	title = {Dask: {Parallel} {Computation} with {Blocked} algorithms and {Task} {Scheduling}},
	shorttitle = {Dask},
	url = {https://conference.scipy.org/proceedings/scipy2015/matthew_rocklin.html},
	doi = {10.25080/Majora-7b98e3ed-013},
	abstract = {Dask enables parallel and out-of-core computation. We couple blocked algorithms with dynamic and memory aware task scheduling to achieve a parallel and out-of-core NumPy clone. We show how this extends the effective scale of modern hardware to larger datasets and discuss how these ideas can be more broadly applied to other parallel collections.},
	language = {en},
	urldate = {2020-12-27},
	author = {Rocklin, Matthew},
	year = {2015},
	pages = {126--132},
	file = {Rocklin - 2015 - Dask Parallel Computation with Blocked algorithms.pdf:/local/home/ag262995/Zotero/storage/E6FCJHJE/Rocklin - 2015 - Dask Parallel Computation with Blocked algorithms.pdf:application/pdf}
}
@inproceedings{ayachit_sensei_2016,
	address = {Salt Lake City, UT, USA},
	title = {The {SENSEI} {Generic} {In} {Situ} {Interface}},
	isbn = {978-1-5090-3872-5},
	url = {http://ieeexplore.ieee.org/document/7836400/},
	doi = {10.1109/ISAV.2016.013},
	abstract = {The SENSEI generic in situ interface is an API that promotes code portability and reusability. From the simulation view, a developer can instrument their code with the SENSEI API and then make make use of any number of in situ infrastructures. From the method view, a developer can write an in situ method using the SENSEI API, then expect it to run in any number of in situ infrastructures, or be invoked directly from a simulation code, with little or no modiﬁcation. This paper presents the design principles underlying the SENSEI generic interface, along with some simpliﬁed coding examples.},
	language = {en},
	urldate = {2020-12-27},
	booktitle = {2016 {Second} {Workshop} on {In} {Situ} {Infrastructures} for {Enabling} {Extreme}-{Scale} {Analysis} and {Visualization} ({ISAV})},
	publisher = {IEEE},
	author = {Ayachit, Utkarsh and Whitlock, Brad and Wolf, Matthew and Loring, Burlen and Geveci, Berk and Lonie, David and Bethel, E. Wes},
	month = nov,
	year = {2016},
	pages = {40--44},
	file = {Ayachit et al. - 2016 - The SENSEI Generic In Situ Interface.pdf:/local/home/ag262995/Zotero/storage/V6HXT3T9/Ayachit et al. - 2016 - The SENSEI Generic In Situ Interface.pdf:application/pdf}
}

@article{hanwell_visualization_2015_vtk,
	title = {The {Visualization} {Toolkit} ({VTK}): {Rewriting} the rendering code for modern graphics cards},
	volume = {1-2},
	issn = {2352-7110},
	shorttitle = {The {Visualization} {Toolkit} ({VTK})},
	url = {http://www.sciencedirect.com/science/article/pii/S2352711015000035},
	doi = {10.1016/j.softx.2015.04.001},
	abstract = {The Visualization Toolkit (VTK) is an open source, permissively licensed, cross-platform toolkit for scientific data processing, visualization, and data analysis. It is over two decades old, originally developed for a very different graphics card architecture. Modern graphics cards feature fully programmable, highly parallelized architectures with large core counts. VTK’s rendering code was rewritten to take advantage of modern graphics cards, maintaining most of the toolkit’s programming interfaces. This offers the opportunity to compare the performance of old and new rendering code on the same systems/cards. Significant improvements in rendering speeds and memory footprints mean that scientific data can be visualized in greater detail than ever before. The widespread use of VTK means that these improvements will reap significant benefits.},
	language = {en},
	urldate = {2020-12-27},
	journal = {SoftwareX},
	author = {Hanwell, Marcus D. and Martin, Kenneth M. and Chaudhary, Aashish and Avila, Lisa S.},
	month = sep,
	year = {2015},
	keywords = {Data analysis, Scientific data, Toolkit, Visualization},
	pages = {9--12},
	file = {ScienceDirect Full Text PDF:/local/home/ag262995/Zotero/storage/FXN2XDZE/Hanwell et al. - 2015 - The Visualization Toolkit (VTK) Rewriting the ren.pdf:application/pdf;ScienceDirect Snapshot:/local/home/ag262995/Zotero/storage/V76AF729/S2352711015000035.html:text/html}
}
@article{childs_visit_nodate,
	title = {{VisIt}: {An} {End}-{User} {Tool} for {Visualizing} and {Analyzing} {Very} {Large} {Data}},
	language = {en},
	author = {Childs, Hank},
	pages = {17},
	file = {Childs - VisIt An End-User Tool for Visualizing and Analyz.pdf:/local/home/ag262995/Zotero/storage/ENAG3GC3/Childs - VisIt An End-User Tool for Visualizing and Analyz.pdf:application/pdf}
}

@inproceedings{dorier_damaris_2012,
	address = {Beijing, China},
	title = {Damaris: {How} to {Efficiently} {Leverage} {Multicore} {Parallelism} to {Achieve} {Scalable}, {Jitter}-free {I}/{O}},
	shorttitle = {Damaris},
	url = {https://hal.inria.fr/hal-00715252},
	abstract = {With exascale computing on the horizon, the performance variability of I/O systems represents a key challenge in sustaining high performance. In many HPC applications, I/O is concurrently performed by all processes, which leads to I/O bursts. This causes resource contention and substantial variability of I/O performance, which significantly impacts the overall application performance and, most importantly, its predictability over time. In this paper, we propose a new approach to I/O, called Damaris, which leverages dedicated I/O cores on each multicore SMP node, along with the use of shared-memory, to efficiently perform asynchronous data processing and I/O in order to hide this variability. We evaluate our approach on three different platforms including the Kraken Cray XT5 supercomputer (ranked 11th in Top500), with the CM1 atmospheric model, one of the target HPC applications for the Blue Waters postpetascale supercomputer project. By overlapping I/O with computation and by gathering data into large files while avoiding synchronization between cores, our solution brings several benefits: 1) it fully hides jitter as well as all I/O-related costs, which makes simulation performance predictable; 2) it increases the sustained write throughput by a factor of 15 compared to standard approaches; 3) it allows almost perfect scalability of the simulation up to over 9,000 cores, as opposed to state-of-the-art approaches which fail to scale; 4) it enables a 600{\textbackslash}\% compression ratio without any additional overhead, leading to a major reduction of storage requirements.},
	urldate = {2020-09-24},
	booktitle = {{CLUSTER} 2012 - {IEEE} {International} {Conference} on {Cluster} {Computing}},
	publisher = {IEEE},
	author = {Dorier, Matthieu and Antoniu, Gabriel and Cappello, Franck and Snir, Marc and Orf, Leigh},
	month = sep,
	year = {2012},
	file = {HAL PDF Full Text:/local/home/ag262995/Zotero/storage/9B6Y3GPJ/Dorier et al. - 2012 - Damaris How to Efficiently Leverage Multicore Par.pdf:application/pdf}
}


@article{dorier_damaris,
author = {Dorier, Matthieu and Antoniu, Gabriel and Cappello, Franck and Snir, Marc and Sisneros, Robert and Yildiz, Orcun and Ibrahim, Shadi and Peterka, Tom and Orf, Leigh},
title = {Damaris: Addressing Performance Variability in Data Management for Post-Petascale Simulations},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2329-4949},
url = {https://doi.org/10.1145/2987371},
doi = {10.1145/2987371},
abstract = {With exascale computing on the horizon, reducing performance variability in data management tasks (storage, visualization, analysis, etc.) is becoming a key challenge in sustaining high performance. This variability significantly impacts the overall application performance at scale and its predictability over time.In this article, we present Damaris, a system that leverages dedicated cores in multicore nodes to offload data management tasks, including I/O, data compression, scheduling of data movements, in situ analysis, and visualization. We evaluate Damaris with the CM1 atmospheric simulation and the Nek5000 computational fluid dynamic simulation on four platforms, including NICS’s Kraken and NCSA’s Blue Waters. Our results show that (1) Damaris fully hides the I/O variability as well as all I/O-related costs, thus making simulation performance predictable; (2) it increases the sustained write throughput by a factor of up to 15 compared with standard I/O approaches; (3) it allows almost perfect scalability of the simulation up to over 9,000 cores, as opposed to state-of-the-art approaches that fail to scale; and (4) it enables a seamless connection to the VisIt visualization software to perform in situ analysis and visualization in a way that impacts neither the performance of the simulation nor its variability.In addition, we extended our implementation of Damaris to also support the use of dedicated nodes and conducted a thorough comparison of the two approaches—dedicated cores and dedicated nodes—for I/O tasks with the aforementioned applications.},
journal = {ACM Trans. Parallel Comput.},
month = {oct},
articleno = {15},
numpages = {43},
keywords = {dedicated cores, Damaris, in situ visualization, I/O, Exascale computing, dedicated nodes}
}


@phdthesis{dorier_addressing_2014,
	type = {phdthesis},
	title = {Addressing the {Challenges} of {I}/{O} {Variability} in {Post}-{Petascale} {HPC} {Simulations}},
	url = {https://tel.archives-ouvertes.fr/tel-01099105},
	abstract = {Million-core supercomputers have become a reality in 2012 with LLNL's Sequoia supercomputer. Following Moore's law, Exascale machines (capable of 10E18 floating point operations per second) are expected by 2018. Such an immense computational power is used in many research areas, including earth sciences, biology, climate, or cosmology, where large-scale simulations are conducted to understand physical phenomena better. These simulations aim to replace real experiments that are either too expensive, irreproducible or simply unfeasible. But larger simulations on larger machines lead to the production of larger amounts of data. These data need to be efficiently stored and processed in order to retrieve scientific insights. The traditional approach to data management consists of storing the output of the simulation during its run, move it and analyze it later offline. With an increasing gap between the performance of storage systems and the computation capabilities of recent post-Petascale supercomputers, this approach becomes unsustainable. This Ph.D. thesis explores new approaches to data management for post-Petascale supercomputers. We first introduce the Damaris approach, which leverages the multicore nature of recent machines to offload data-management tasks into dedicated cores. We study in particular how Damaris can be used to hide the variability in I/O (Input/Output) performance, and to provide in situ visualization capabilities to simulations in a way that does not impact their performance. We then use Damaris to evaluate the energy consumption of various data management approaches, including the use of dedicated I/O nodes. We then study the effect of multi-application I/O contention on the performance of the storage system. We propose the CALCioM approach, which provides a coordination layer between distinct applications to mitigate I/O interference. In regard to access patterns, it has been observed that most applications have a repetitive behavior with respect to I/O, and that a model of this behavior can be useful to many systems (including CALCioM, but also any scheduler, caching or prefetching system). Based on this, we propose Omnisc'IO, an approach that leverages grammars to predict the spatial and temporal access patterns of HPC simulations at run time. This thesis includes results of experiments conducted with real scientific simulations, including CM1, GTC, LAMMPS and Nek5000, on real petascale and post-petascale platforms, including NCSA's Blue Waters, ORNL's Titan, NICS's Kraken and ANL's Intrepid.},
	language = {en},
	urldate = {2020-09-24},
	school = {Ecole Normale Supérieure de Rennes},
	author = {Dorier, Matthieu},
	month = dec,
	year = {2014},
	file = {Full Text PDF:/local/home/ag262995/Zotero/storage/MCA8W8PJ/Dorier - 2014 - Addressing the Challenges of IO Variability in Po.pdf:application/pdf;Snapshot:/local/home/ag262995/Zotero/storage/LA57TXFT/tel-01099105.html:text/html}
}

@article{mckinney_python_nodate,
	title = {Python for {Data} {Analysis}},
	language = {en},
	author = {McKinney, Wes},
	pages = {541},
	file = {McKinney - Python for Data Analysis.pdf:/local/home/ag262995/Zotero/storage/BZMEMBLB/McKinney - Python for Data Analysis.pdf:application/pdf}
}

@article{laufer2022high_adios2_insitu,
  title={High Performance Parallel I/O and In-Situ Analysis in the WRF Model with ADIOS2},
  author={Laufer, Michael and Fredj, Erick},
  journal={arXiv preprint arXiv:2201.08228},
  year={2022}
}

@inproceedings{arcila_flowvr_2006,
        address = {Rocquencourt, France},
        title = {{FlowVR}: {A} {Framework} {For} {Distributed} {Virtual} {Reality} {Applications}},
        booktitle = {{1iAre} {journA}({C})es de l'{Association} {FranAaise} de {RA}({C}){alitA}({C}) {Virtuelle}, {AugmentA}({C})e, {Mixte} et d'{Interaction} {3D}},
        author = {Arcila, T. and Allard, J. and MA(C)nier, C. and Boyer, E. and Raffin, B.},
        month = nov,
        year = {2006},
}


@inproceedings{allard_distributed_2006_flowvr,
        address = {Alexandria, USA},
        title = {Distributed {Physical} {Based} {Simulations} for {Large} {VR} {Applications}},
        booktitle = {{IEEE} {Virtual} {Reality} {Conference}},
        author = {Allard, J. and Raffin, B.},
        month = mar,
        year = {2006},
}


@inproceedings{allard_flowvr_2004,
        address = {Pisa, Italia},
        title = {{FlowVR}: a {Middleware} for {Large} {Scale} {Virtual} {Reality} {Applications}},
        booktitle = {Proceedings of {Euro}-par 2004},
        author = {Allard, J. and Gouranton, V. and Lecointre, L. and Limet, S. and Melin, E. and Raffin, B. and Robert, S.},
        month = aug,
        year = {2004},
}

@inproceedings{bennett_combining_2012_dataspaces,
        title = {Combining in-situ and in-transit processing to enable extreme-scale scientific analysis},
        doi = {10.1109/SC.2012.31},
        abstract = {With the onset of extreme-scale computing, I/O constraints make it increasingly difficult for scientists to save a sufficient amount of raw simulation data to persistent storage. One potential solution is to change the data analysis pipeline from a post-process centric to a concurrent approach based on either in-situ or in-transit processing. In this context computations are considered in-situ if they utilize the primary compute resources, while in-transit processing refers to offloading computations to a set of secondary resources using asynchronous data transfers. In this paper we explore the design and implementation of three common analysis techniques typically performed on large-scale scientific simulations: topological analysis, descriptive statistics, and visualization. We summarize algorithmic developments, describe a resource scheduling system to coordinate the execution of various analysis workflows, and discuss our implementation using the DataSpaces and ADIOS frameworks that support efficient data movement between in-situ and in-transit computations. We demonstrate the efficiency of our lightweight, flexible framework by deploying it on the Jaguar XK6 to analyze data generated by S3D, a massively parallel turbulent combustion code. Our framework allows scientists dealing with the data deluge at extreme scale to perform analyses at increased temporal resolutions, mitigate I/O costs, and significantly improve the time to insight.},
        booktitle = {{SC} '12: {Proceedings} of the {International} {Conference} on {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
        author = {Bennett, Janine C. and Abbasi, Hasan and Bremer, Peer-Timo and Grout, Ray and Gyulassy, Attila and Jin, Tong and Klasky, Scott and Kolla, Hemanth and Parashar, Manish and Pascucci, Valerio and Pebay, Philippe and Thompson, David and Yu, Hongfeng and Zhang, Fan and Chen, Jacqueline},
        month = nov,
        year = {2012},
        note = {ISSN: 2167-4337},
        keywords = {Algorithm design and analysis, Analytical models, Computational modeling, Data models, Data transfer, Data visualization, Processor scheduling},
        pages = {1--9},
}   


@article{docan_dataspaces_2012,
	title = {{DataSpaces}: an interaction and coordination framework for coupled simulation workflows},
	volume = {15},
	issn = {1573-7543},
	shorttitle = {{DataSpaces}},
	url = {https://doi.org/10.1007/s10586-011-0162-y},
	doi = {10.1007/s10586-011-0162-y},
	abstract = {Emerging high-performance distributed computing environments are enabling new end-to-end formulations in science and engineering that involve multiple interacting processes and data-intensive application workflows. For example, current fusion simulation efforts are exploring coupled models and codes that simultaneously simulate separate application processes, such as the core and the edge turbulence. These components run on different high performance computing resources, need to interact at runtime with each other and with services for data monitoring, data analysis and visualization, and data archiving. As a result, they require efficient and scalable support for dynamic and flexible couplings and interactions, which remains a challenge. This paper presents DataSpaces a flexible interaction and coordination substrate that addresses this challenge. DataSpaces essentially implements a semantically specialized virtual shared space abstraction that can be associatively accessed by all components and services in the application workflow. It enables live data to be extracted from running simulation components, indexes this data online, and then allows it to be monitored, queried and accessed by other components and services via the space using semantically meaningful operators. The underlying data transport is asynchronous, low-overhead and largely memory-to-memory. The design, implementation, and experimental evaluation of DataSpaces using a coupled fusion simulation workflow is presented.},
	language = {en},
	number = {2},
	urldate = {2020-09-24},
	journal = {Cluster Computing},
	author = {Docan, Ciprian and Parashar, Manish and Klasky, Scott},
	month = jun,
	year = {2012},
	pages = {163--181},
	file = {Springer Full Text PDF:/local/home/ag262995/Zotero/storage/F4NK8XIE/Docan et al. - 2012 - DataSpaces an interaction and coordination framew.pdf:application/pdf}
}

@inproceedings{romanus_persistent_2016,
	address = {New York, NY, USA},
	series = {{DIDC} '16},
	title = {Persistent {Data} {Staging} {Services} for {Data} {Intensive} {In}-situ {Scientific} {Workflows}},
	isbn = {978-1-4503-4352-7},
	url = {https://doi.org/10.1145/2912152.2912157},
	doi = {10.1145/2912152.2912157},
	abstract = {Scientific simulation workflows executing on very large scale computing systems are essential modalities for scientific investigation. The increasing scales and resolution of these simulations provide new opportunities for accurately modeling complex natural and engineered phenomena. However, the increasing complexity necessitates managing, transporting, and processing unprecedented amounts of data, and as a result, researchers are increasingly exploring data-staging and in-situ workflows to reduce data movement and data-related overheads. However, as these workflows become more dynamic in their structures and behaviors, data staging and in-situ solutions must evolve to support new requirements. In this paper, we explore how the service-oriented concept can be applied to extreme-scale in-situ workflows. Specifically, we explore persistent data staging as a service and present the design and implementation of DataSpaces as a Service, a service-oriented data staging framework. We use a dynamically coupled fusion simulation workflow to illustrate the capabilities of this framework and evaluate its performance and scalability.},
	urldate = {2020-09-24},
	booktitle = {Proceedings of the {ACM} {International} {Workshop} on {Data}-{Intensive} {Distributed} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Romanus, Melissa and Zhang, Fan and Jin, Tong and Sun, Qian and Bui, Hoang and Parashar, Manish and Choi, Jong and Janhunen, Saloman and Hager, Robert and Klasky, Scott and Chang, Choong-Seock and Rodero, Ivan},
	month = jun,
	year = {2016},
	keywords = {as a service, coupled workflows, data management, data staging, dynamic workflow, in-memory, in-situ, node-local, scientific workflows},
	pages = {37--44},
	file = {Full Text PDF:/local/home/ag262995/Zotero/storage/ZG9CR3JY/Romanus et al. - 2016 - Persistent Data Staging Services for Data Intensiv.pdf:application/pdf}
}

@techreport{peterka_ascr_2019,
	title = {{ASCR} {Workshop} on {In} {Situ} {Data} {Management}: {Enabling} {Scientific} {Discovery} from {Diverse} {Data} {Sources}},
	shorttitle = {{ASCR} {Workshop} on {In} {Situ} {Data} {Management}},
	url = {https://www.osti.gov/biblio/1493245},
	abstract = {The U.S. Department of Energy's Office of Scientific and Technical Information},
	language = {English},
	urldate = {2020-09-24},
	institution = {USDOE Office of Science (SC) (United States)},
	author = {Peterka, Tom and Bard, Deborah and Bennett, Janine and Bethel, E. Wes and Oldfield, Ron and Pouchard, Line and Sweeney, Christine and Wolf, Matthew},
	month = feb,
	year = {2019},
	doi = {10.2172/1493245},
	file = {Full Text PDF:/local/home/ag262995/Zotero/storage/2LDDSGC4/Peterka et al. - 2019 - ASCR Workshop on In Situ Data Management Enabling.pdf:application/pdf;Snapshot:/local/home/ag262995/Zotero/storage/KRXI9VWP/1493245.html:text/html}
}

@article{liu_hello_2014_adios,
	title = {Hello {ADIOS}: the challenges and lessons of developing leadership class {I}/{O} frameworks: {HELLO} {ADIOS}},
	volume = {26},
	issn = {15320626},
	shorttitle = {Hello {ADIOS}},
	url = {http://doi.wiley.com/10.1002/cpe.3125},
	doi = {10.1002/cpe.3125},
	abstract = {Applications running on leadership platforms are more and more bottlenecked by storage I/O. In an effort to combat the increasing disparity between I/O throughput and compute capability, we created ADIOS in 2005. Focusing on putting users ﬁrst with a Service Oriented Architecture, we combined cutting edge research into new I/O techniques with a design effort to create near optimal I/O methods. As a result, ADIOS provides the highest level of synchronous I/O performance for a number of mission critical applications at various DOE Leadership Computing Facilities. Meanwhile ADIOS is leading the push for next generation techniques including staging and data processing pipelines. In this paper we describe the startling observations we have made in the last half decade of I/O research and development, and elaborate the lessons we have learned along this journey. We also detail some of the challenges that remain as we look towards the coming Exascale era. Copyright c 0000 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {7},
	urldate = {2020-09-24},
	journal = {Concurrency and Computation: Practice and Experience},
	author = {Liu, Qing and Logan, Jeremy and Tian, Yuan and Abbasi, Hasan and Podhorszki, Norbert and Choi, Jong Youl and Klasky, Scott and Tchoua, Roselyne and Lofstead, Jay and Oldfield, Ron and Parashar, Manish and Samatova, Nagiza and Schwan, Karsten and Shoshani, Arie and Wolf, Matthew and Wu, Kesheng and Yu, Weikuan},
	month = may,
	year = {2014},
	pages = {1453--1473},
	file = {Liu et al. - 2014 - Hello ADIOS the challenges and lessons of develop.pdf:/local/home/ag262995/Zotero/storage/G99SMT4M/Liu et al. - 2014 - Hello ADIOS the challenges and lessons of develop.pdf:application/pdf}
}

@misc{redis,
        title = {Redis},
        url = {https://redis.io/},
        abstract = {Redis is an open source (BSD licensed), in-memory data structure store, used as a database, cache, and message broker},
        language = {en},
        urldate = {2022-10-27},
        journal = {Redis},
        file = {Snapshot:/local/home/ag262995/Zotero/storage/TGCVNXTB/redis.io.html:text/html},
}

@misc{site_introduction_smartsim,
        title = {Introduction — {SmartSim} 0.4.1 documentation},
        url = {https://www.craylabs.org/docs/overview.html},
        urldate = {2022-10-27},
        file = {Introduction — SmartSim 0.4.1 documentation:/local/home/ag262995/Zotero/storage/ZHX67EXC/overview.html:text/html},}

@misc{openMP,
        title = {Home},
        url = {https://www.openmp.org/},
        abstract = {Latest News
View Monthly Archives  Tweets by OpenMP\_ARB
Upcoming Events
View All Events    Get all the latest API specifications, technical report drafts},
        language = {en-GB},
        urldate = {2022-10-27},
        journal = {OpenMP},
        author = {{tim.lewis}},
        file = {Snapshot:/local/home/ag262995/Zotero/storage/54LZIAXE/www.openmp.org.html:text/html},
}


@article{idreos2019learning_key_value,
  title={Learning key-value store design},
  author={Idreos, Stratos and Dayan, Niv and Qin, Wilson and Akmanalp, Mali and Hilgard, Sophie and Ross, Andrew and Lennon, James and Jain, Varun and Gupta, Harshita and Li, David and others},
  journal={arXiv preprint arXiv:1907.05443},
  year={2019}
}



@incollection{childs2022situ,
  title={In Situ Visualization for Computational Science: Background and Foundational Topics},
  author={Childs, Hank and Bennett, Janine C and Garth, Christoph},
  booktitle={In Situ Visualization for Computational Science},
  pages={1--8},
  year={2022},
  publisher={Springer}
}


@article{zheng_flexio_nodate,
        title = {{FlexIO}: {Location}-flexible {Execution} of {In} {Situ} {Data} {Analytics} for {Large} {Scale} {Scientific} {Applications}},
        abstract = {Increasingly severe I/O bottlenecks on High-End Computing machines are prompting scientists to process simulation output data while simulations are running and before placing data on disk – ”in situ” and/or ”in-transit”. There are several options in placing in-situ data analytics along the I/O path: on compute nodes, on staging nodes dedicated to analytics, or after data is stored on persistent storage. Different placements have different impact on end to end performance and cost. The consequence is a need for flexibility in the location of in situ data analytics. The FlexIO facility described in this paper supports flexible placement of in situ analytics, by offering simple abstractions and methods that help developers exploit the opportunities and trade-offs in performing analytics at different levels of the I/O hierarchy. Experimental results with several large-scale scientific applications demonstrate the importance of flexibility in analytics placement.},
        language = {en},
        author = {Zheng, Fang and Zou, Hongbo and Eisenhauer, Greg and Schwan, Karsten and Wolf, Matthew and Dayal, Jai and Nguyen, Tuan-Anh and Cao, Jianting and Abbasi, Hasan and Klasky, Scott and Podhorszki, Norbert and Yu, Hongfeng},
        file = {Zheng et al. - FlexIO Location-flexible Execution of In Situ Dat.pdf:/local/home/ag262995/Zotero/storage/TDK7JWKZ/Zheng et al. - FlexIO Location-flexible Execution of In Situ Dat.pdf:application/pdf},
}    

@article{in_situ_methodes,
author = {Bauer, A. C. and Abbasi, H. and Ahrens, J. and Childs, H. and Geveci, B. and Klasky, S. and Moreland, K. and O'Leary, P. and Vishwanath, V. and Whitlock, B. and Bethel, E. W.},
title = {In Situ Methods, Infrastructures, and Applications on High Performance Computing Platforms},
journal = {Computer Graphics Forum},
volume = {35},
number = {3},
pages = {577-597},
keywords = {Categories and Subject Descriptors (according to ACM CCS), I.3.2 Computer Graphics: Graphics Systems—Distributed/network graphics},
doi = {https://doi.org/10.1111/cgf.12930},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.12930},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12930},
abstract = {Abstract The considerable interest in the high performance computing (HPC) community regarding analyzing and visualization data without first writing to disk, i. e., in situ processing, is due to several factors. First is an I/O cost savings, where data is analyzed/visualized while being generated, without first storing to a filesystem. Second is the potential for increased accuracy, where fine temporal sampling of transient analysis might expose some complex behavior missed in coarse temporal sampling. Third is the ability to use all available resources, CPU's and accelerators, in the computation of analysis products. This STAR paper brings together researchers, developers and practitioners using in situ methods in extreme-scale HPC with the goal to present existing methods, infrastructures, and a range of computational science and engineering applications using in situ analysis and visualization.},
year = {2016}
}

@inproceedings{ayachit2021catalyst,
  title={Catalyst Revised: Rethinking the ParaView in Situ Analysis and Visualization API},
  author={Ayachit, Utkarsh and Bauer, Andrew C and Boeckel, Ben and Geveci, Berk and Moreland, Kenneth and O’Leary, Patrick and Osika, Tom},
  booktitle={International Conference on High Performance Computing},
  pages={484--494},
  year={2021},
  organization={Springer}
}

@article{whitlock2010getting,
  title={Getting data into VisIt},
  author={Whitlock, Brad},
  journal={Lawrence Livermore National Laboratory},
  year={2010}
}

@misc{smartsim_2022,
        title = {Using {Machine} {Learning} at scale in numerical simulations with {SmartSim}: {An} application to ocean climate modeling {\textbar} {Elsevier} {Enhanced} {Reader}},
        shorttitle = {Using {Machine} {Learning} at scale in numerical simulations with {SmartSim}},
        url = {https://reader.elsevier.com/reader/sd/pii/S1877750322001065?token=3C2209B5F0A6FC2C970A8433BDA25929224F81C74C20F9A352B3AF7661C49FBD9AC6152AB10B92F8CD6F436671CA3565&originRegion=eu-west-1&originCreation=20221027124459},
        language = {en},
        urldate = {2022-10-27},
        doi = {10.1016/j.jocs.2022.101707},
        file = {Snapshot:/local/home/ag262995/Zotero/storage/92WY4NF7/S1877750322001065.html:text/html},
}


@inproceedings{mommessin_automatic_2017,
	title = {Automatic {Data} {Filtering} for {In} {Situ} {Workflows}},
	doi = {10.1109/CLUSTER.2017.35},
	abstract = {In situ workflows contain tasks that exchange messages composed of several data fields. However, a consumer task may not necessarily need all the data fields from its producer. For example, a molecular dynamics simulation can produce atom positions, velocities, and forces; but some analyses require only atom positions. The user should decide whether to specialize the output of a producer task for a particular consumer and get better performance or to send more data than required by the consumer. The first option limits task portability, while the second wastes resources. In this paper, we introduce contracts for in situ tasks. A contract specifies for a producer each data field available for output and for a consumer the data fields needed as input. Comparing a producer and consumer contract allows automatic selection of the data fields a producer has to send for that consumer. We integrated our contracts mechanism within Decaf, a middleware for building and executing in situ workflows. Contracts enable to automatically extract at the producer the data the consumer needs. We evaluate the cost and performance of message extraction at runtime with both synthetic examples and a real scientific workflow coupling a molecular dynamics simulation with three different data analytics codes. Our contract-based automatic data extraction removes the need to specialize producers while entailing small overheads.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Cluster} {Computing} ({CLUSTER})},
	author = {Mommessin, C. and Dreher, M. and Raffin, B. and Peterka, T.},
	month = sep,
	year = {2017},
	note = {ISSN: 2168-9253},
	keywords = {Analytical models, atom positions, automatic data extraction, automatic data filtering, automatic selection, Communication channels, Computational modeling, Contracts, contracts mechanism, data fields, data handling, Data models, Decaf, exchange messages, information filtering, middleware, Middleware, molecular dynamics simulation, several data fields, situ workflows, workflow management software},
	pages = {370--378},
	file = {Submitted Version:/local/home/ag262995/Zotero/storage/ERY8ACYA/Mommessin et al. - 2017 - Automatic Data Filtering for In Situ Workflows.pdf:application/pdf;IEEE Xplore Abstract Record:/local/home/ag262995/Zotero/storage/MNK98V37/8048949.html:text/html}
}

@inproceedings{zhang_miqs_2019,
	address = {New York, NY, USA},
	series = {{SC} '19},
	title = {{MIQS}: metadata indexing and querying service for self-describing file formats},
	isbn = {978-1-4503-6229-0},
	shorttitle = {{MIQS}},
	url = {https://doi.org/10.1145/3295500.3356146},
	doi = {10.1145/3295500.3356146},
	abstract = {Scientific applications often store datasets in self-describing data file formats, such as HDF5 and netCDF. Regrettably, to efficiently search the metadata within these files remains challenging due to the sheer size of the datasets. Existing solutions extract the metadata and store it in external database management systems (DBMS) to locate desired data. However, this practice introduces significant overhead and complexity in extraction and querying. In this research, we propose a novel {\textless}u{\textgreater}M{\textless}/u{\textgreater}etadata {\textless}u{\textgreater}I{\textless}/u{\textgreater}ndexing and {\textless}u{\textgreater}Q{\textless}/u{\textgreater}uerying {\textless}u{\textgreater}S{\textless}/u{\textgreater}ervice (MIQS), which removes the external DBMS and utilizes in-memory index to achieve efficient metadata searching. MIQS follows the self-contained data management paradigm and provides portable and schema-free metadata indexing and querying functionalities for self-describing file formats. We have evaluated MIQS with the state-of-the-art MongoDB-based metadata indexing solution. MIQS achieved up to 99\% time reduction in index construction and up to 172kx search performance improvement with up to 75\% reduction in memory footprint.},
	urldate = {2020-09-24},
	booktitle = {Proceedings of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Wei and Byna, Suren and Tang, Houjun and Williams, Brody and Chen, Yong},
	month = nov,
	year = {2019},
	keywords = {HDF5 metadata management, metadata search},
	pages = {1--24},
	file = {Full Text PDF:/local/home/ag262995/Zotero/storage/FEWN5RKS/Zhang et al. - 2019 - MIQS metadata indexing and querying service for s.pdf:application/pdf}
}

@inproceedings{dreher_flexible_2014,
	title = {A {Flexible} {Framework} for {Asynchronous} {In} {Situ} and {In} {Transit} {Analytics} for {Scientific} {Simulations}},
	url = {https://hal.inria.fr/hal-00941413},
	abstract = {High performance computing systems are today composed of tens of thousands of processors and deep memory hierarchies. The next generation of machines will further increase the unbalance between I/O capabilities and processing power. To reduce the pressure on I/Os, the in situ analytics paradigm proposes to process the data as closely as possible to where and when the data are produced. Processing can be embedded in the simulation code, executed asynchronously on helper cores on the same nodes, or performed in transit on staging nodes dedicated to analytics. Today, software environ- nements as well as usage scenarios still need to be investigated before in situ analytics become a standard practice. In this paper we introduce a framework for designing, deploying and executing in situ scenarios. Based on a com- ponent model, the scientist designs analytics workflows by first developing processing components that are next assembled in a dataflow graph through a Python script. At runtime the graph is instantiated according to the execution context, the framework taking care of deploying the application on the target architecture and coordinating the analytics workflows with the simulation execution. Component coordination, zero- copy intra-node communications or inter-nodes data transfers rely on per-node distributed daemons. We evaluate various scenarios performing in situ and in transit analytics on large molecular dynamics systems sim- ulated with Gromacs using up to 1664 cores. We show in particular that analytics processing can be performed on the fraction of resources the simulation does not use well, resulting in a limited impact on the simulation performance (less than 6\%). Our more advanced scenario combines in situ and in transit processing to compute a molecular surface based on the Quicksurf algorithm.},
	language = {en},
	urldate = {2020-09-24},
	publisher = {IEEE Computer Science Press},
	author = {Dreher, Matthieu and Raffin, Bruno},
	month = may,
	year = {2014},
	file = {Full Text PDF:/local/home/ag262995/Zotero/storage/5W8JVW6X/Dreher and Raffin - 2014 - A Flexible Framework for Asynchronous In Situ and .pdf:application/pdf;Snapshot:/local/home/ag262995/Zotero/storage/NJV2NKK5/en.html:text/html}
}

@inproceedings{sousa2012dataflow,
  title={Dataflow programming concept, languages and applications},
  author={Sousa, Tiago Boldt},
  booktitle={Doctoral Symposium on Informatics Engineering},
  volume={130},
  year={2012}
}

@article{dreher_exavis,
  TITLE = {{ExaViz: a Flexible Framework to Analyse, Steer and Interact with Molecular Dynamics Simulations}},
  AUTHOR = {Dreher, Matthieu and Prevoteau-Jonquet, Jessica and Trellet, Mikael and Piuzzi, Marc and Baaden, Marc and Raffin, Bruno and F{\'e}rey, Nicolas and Robert, Sophie and Limet, S{\'e}bastien},
  URL = {https://hal.inria.fr/hal-00942627},
  JOURNAL = {{Faraday Discussions}},
  PUBLISHER = {{Royal Society of Chemistry}},
  SERIES = {Molecular simulations and visualization},
  VOLUME = {169},
  PAGES = {119-142},
  YEAR = {2014},
  MONTH = May,
  DOI = {10.1039/C3FD00142C},
  PDF = {https://hal.inria.fr/hal-00942627/file/faraday.pdf},
  HAL_ID = {hal-00942627},
  HAL_VERSION = {v1},
}

@phdthesis{dreher_methodes_2015,
	type = {These de doctorat},
	title = {Méthodes {In}-{Situ} et {In}-{Transit} : vers un continuum entre les applications interactives et offines à grande échelle},
	copyright = {Licence Etalab},
	shorttitle = {Méthodes {In}-{Situ} et {In}-{Transit}},
	url = {https://www.theses.fr/2015GREAM076},
	abstract = {Les simulations paralllèles sont devenues des outils indispensables dans de nombreux domaines scientifiques. Pour simuler des phénomènes complexes, ces simulations sont exécutées sur de grandes machines parallèles. La puissance de calcul de ces machines n'a cessé de monter permettant ainsi le traitement de simulations de plus en plus imposantes. En revanche, les systèmes d'I/O nécessaires à la sauvegarde des données produites par les simulations ont suivit une croissance beaucoup plus faible. Actuellement déjà, il est difficile pour les scientifiques de sauvegarder l'ensemble des données désirées et d'avoir suffisament de puissance de calcul pour les analyser par la suite. A l'ère de l'Exascale, on estime que moins de 1\% des données produites par une simulation pourronts être sauvegardées. Ces données sont pourtant une des clés vers des découvertes scientifiques majeures. Les traitements in-situ sont une solution prometteuse à ce problème. Le principe est d'effectuer des analyses alors que la simulation est en cours d'exécution et que les données sont encore en mémoire. Cette approche permet d'une part d'éviter le goulot d'étranglement au niveau des I/O mais aussi de profiter de la puissance de calcul offerte par les machines parallèles pour effectuer des traitements lourds en calcul. Dans cette thèse, nous proposons d'utiliser le paradigme du dataflow pour permettre la construction d'applications in-situ complexes. Pour cela, nous utilisons l'intergiciel FlowVR permettant de coupler des codes parallèles hétérogènes en créant des canaux de communication entre eux afin de former un graphe. FlowVR dispose de suffisament de flexibilité pour permettre plusieurs stratégies de placement des processus d'analyses que cela soit sur les nœuds de la simulation, sur des cœurs dédiés ou des nœuds dédiés. De plus, les traitements in-situ peuvent être exécutés de manière asynchrone permettant ainsi un faible impact sur les performances de la simulation. Pour démontrer la flexibilité de notre approche, nous nous sommes intéressés au cas à la dynamique moléculaire et plus particulièrement Gromacs, un code de simulation de dynamique moléculaire couramment utilisé par les biologistes pouvant passer à l'échelle sur plusieurs milliers de coeurs. En étroite collaboration avec des experts du domaine biologique, nous avons contruit plusieurs applications. Notre première application consiste à permettre à un utilisateur de guider une simulation de dynamique moléculaire vers une configuration souhaitée. Pour cela, nous avons couplé Gromacs à un visualiseur et un bras haptique. Grâce à l'intégration de forces émises par l'utilisateur, celui ci peut guider des systèmes moléculaires de plus d'un million d'atomes. Notre deuxième application se concentre sur les simulations longues sur les grandes machines parallèles. Nous proposons de remplacer la méthode native d'écriture de Gromacs et de la déporter dans notre infrastructure en utilisant deux méthodes distinctes. Nous proposons également un algorithme de rendu parallèle pouvant s'adapter à différentes configurations de placements. Notre troisième application vise à étudier les usages que peuvent avoir les biologistes avec les applications in-situ. Nous avons développé une infrastructure unifiée permettant d'effectuer des traitements aussi bien sur des simulations intéractives, des simulations longues et en post-mortem.},
	urldate = {2020-09-24},
	school = {Université Grenoble Alpes (ComUE)},
	author = {Dreher, Matthieu},
	collaborator = {Raffin, Bruno},
	month = feb,
	year = {2015},
	keywords = {004, Data visualisation, In-Situ, Parallelism, Parallélisme, Parallélisme (informatique), Visualisation, Visualization},
	annote = {Sous la direction de  Bruno Raffin. Soutenue le 25-02-2015,à l'Université Grenoble Alpes (ComUE) , dans le cadre de   École doctorale mathématiques, sciences et technologies de l'information, informatique (Grenoble) , en partenariat avec  Institut national de recherche en informatique et en automatique (France). Unité de recherche (Grenoble, Isère)   (laboratoire)  .}
}

@techreport{dreher_decaf_2017,
	title = {Decaf: {Decoupled} {Dataflows} for {In} {Situ} {High}-{Performance} {Workflows}},
	shorttitle = {Decaf},
	url = {https://www.osti.gov/biblio/1372113-decaf-decoupled-dataflows-situ-high-performance-workflows},
	abstract = {The U.S. Department of Energy's Office of Scientific and Technical Information},
	language = {English},
	number = {ANL/MCS-TM-371},
	urldate = {2020-09-24},
	institution = {Argonne National Lab. (ANL), Argonne, IL (United States)},
	author = {Dreher, M. and Peterka, T.},
	month = jul,
	year = {2017},
	doi = {10.2172/1372113},
	file = {Full Text PDF:/local/home/ag262995/Zotero/storage/N9T3AW4W/Dreher and Peterka - 2017 - Decaf Decoupled Dataflows for In Situ High-Perfor.pdf:application/pdf;Snapshot:/local/home/ag262995/Zotero/storage/MDMHV73Z/1372113-decaf-decoupled-dataflows-situ-high-performance-workflows.html:text/html}
}

@inproceedings{dreher_bredala_2016,
	title = {Bredala: {Semantic} {Data} {Redistribution} for {In} {Situ} {Applications}},
	shorttitle = {Bredala},
	doi = {10.1109/CLUSTER.2016.30},
	abstract = {In situ processing is a promising solution to the problem of imbalance between computational capabilities and I/O bandwidth in current and future supercomputers. Initially designed for staging I/O, in situ middleware now can support a wide range of domains such as visualization, machine learning, filtering, and feature tracking. Doing so requires in situ middleware to manage complex heterogeneous codes using different data structures. Data need to be transformed and reorganized along the data path to fit the analysis needs. However, redistributing complex data structures is difficult. In many cases, arbitrarily splitting the arrays of a data structure destroys the semantic integrity of the data. We present Bredala, a lightweight library to annotate a data model with enough information to preserve the semantic integrity of the data during a redistribution. Bredala allows developers to describe how to split and merge a data model safely, operations usually done by in situ middleware. We evaluate the cost and performance of our library in a molecular dynamics application. We show that our data model can simplify the workflow graph of large-scale applications, improve the reusability of tasks, and offer an efficient alternative to redistribute the data.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Cluster} {Computing} ({CLUSTER})},
	author = {Dreher, Matthieu and Peterka, Tom},
	month = sep,
	year = {2016},
	note = {ISSN: 2168-9253},
	keywords = {Arrays, Bredala, code coupling, complex data structure redistribution, complex heterogeneous codes, computational capabilities, data integrity, data model, data model merging, data model splitting, data models, Data models, data structures, Data visualization, feature tracking, flow graphs, I-O bandwidth, in situ, in situ applications, in situ middleware, large-scale applications, Libraries, machine learning, merging, middleware, Middleware, molecular dynamics application, parallel machines, semantic data integrity, semantic data redistribution, Semantics, supercomputers, task reusability, workflow, workflow graph},
	pages = {279--288},
	file = {IEEE Xplore Full Text PDF:/local/home/ag262995/Zotero/storage/UNXAYLHH/Dreher and Peterka - 2016 - Bredala Semantic Data Redistribution for In Situ .pdf:application/pdf;IEEE Xplore Abstract Record:/local/home/ag262995/Zotero/storage/IP239H3P/7776520.html:text/html}
}

@inproceedings{gu_hdf5_2019,
	address = {New York, NY, USA},
	series = {{ISAV} '19},
	title = {{HDF5} as a vehicle for in transit data movement},
	isbn = {978-1-4503-7723-2},
	url = {https://doi.org/10.1145/3364228.3364237},
	doi = {10.1145/3364228.3364237},
	abstract = {For in transit processing, one of the fundamental challenges is the efficient movement of data from producers to consumers. Exploiting the flexibility offered by the SENSEI generic in situ framework, we have developed a number of different in transit data transport mechanisms. In this work, we focus on the transport mechanism that leverages the HDF5 parallel I/O library, and investigate the performance characteristics of this transport mechanism. For in transit use cases at scale on HPC platforms, one might expect that an in transit data transport mechanism that uses faster layers of the storage hierarchy, such as DRAM memory, would always outperform a transport that uses slower layers of the storage hierarchy, such as an NVRAM-based persistent storage presented as a distributed file system. However, our test results show that the performance of the transport using NVRAM is competitive with the transport that uses socket-based data movement across varying levels of producer and consumer concurrency.},
	urldate = {2020-09-24},
	booktitle = {Proceedings of the {Workshop} on {In} {Situ} {Infrastructures} for {Enabling} {Extreme}-{Scale} {Analysis} and {Visualization}},
	publisher = {Association for Computing Machinery},
	author = {Gu, Junmin and Loring, Burlen and Wu, Kesheng and Bethel, E. Wes},
	month = nov,
	year = {2019},
	keywords = {in situ analysis, in situ visualization, SENSEI},
	pages = {39--43},
	file = {Full Text PDF:/local/home/ag262995/Zotero/storage/KV4Q3VF6/Gu et al. - 2019 - HDF5 as a vehicle for in transit data movement.pdf:application/pdf}
}

@inproceedings{dorier_lessons_2015,
	address = {Austin, TX, USA},
	title = {Lessons {Learned} from {Building} {In} {Situ} {Coupling} {Frameworks}},
	isbn = {978-1-4503-4003-8},
	url = {http://dl.acm.org/citation.cfm?doid=2828612.2828622},
	doi = {10.1145/2828612.2828622},
	abstract = {Over the past few years, the increasing amounts of data produced by large-scale simulations have motivated a shift from traditional oﬄine data analysis to in situ analysis and visualization. In situ processing began as the coupling of a parallel simulation with an analysis or visualization library, motivated primarily by avoiding the high cost of accessing storage. Going beyond this simple pairwise tight coupling, complex analysis workﬂows today are graphs with one or more data sources and several interconnected analysis components. In this paper, we review four tools that we have developed to address the challenges of coupling simulations with visualization packages or analysis workﬂows: Damaris, Decaf, FlowVR and Swift. This self-critical inquiry aims to shed light not only on their potential, but most importantly on the forthcoming software challenges that these and other in situ analysis and visualization frameworks will face in order to move toward exascale.},
	language = {en},
	urldate = {2020-09-24},
	booktitle = {Proceedings of the {First} {Workshop} on {In} {Situ} {Infrastructures} for {Enabling} {Extreme}-{Scale} {Analysis} and {Visualization} - {ISAV2015}},
	publisher = {ACM Press},
	author = {Dorier, Matthieu and Dreher, Matthieu and Peterka, Tom and Wozniak, Justin M. and Antoniu, Gabriel and Raffin, Bruno},
	year = {2015},
	pages = {19--24},
	file = {Dorier et al. - 2015 - Lessons Learned from Building In Situ Coupling Fra.pdf:/local/home/ag262995/Zotero/storage/YBB9GVSD/Dorier et al. - 2015 - Lessons Learned from Building In Situ Coupling Fra.pdf:application/pdf}
}

@inproceedings{staging_2016_manish,
	title = {In-{Staging} {Data} {Placement} for {Asynchronous} {Coupling} of {Task}-{Based} {Scientific} {Workflows}},
	doi = {10.1109/ESPM2.2016.006},
	abstract = {Coupled application workflows composed of applications implemented using task-based models present new coupling and data exchange challenges, due to the asynchronous interaction and coupling behaviors between tasks of the component applications. In this paper, we present an adaptive data placement approach that addresses these challenges by dynamically adjusting to the asynchronous coupling patterns. Specifically, it places data across a set of staging cores/nodes with an awareness of the application-specific data locality requirements and the runtime task executions at these staging cores/nodes, with the goal of reducing end-to-end execution time and data movement overhead of the workflow. We experimentally demonstrate the effectiveness of our approach on the Titan Cray XK7 system using representative data coupling patterns derived from current scientific workflows. The evaluation demonstrates that our approach efficiently improves performance by reducing the time-to-solution and increasing the quality of insights for scientific discovery.},
	booktitle = {2016 {Second} {International} {Workshop} on {Extreme} {Scale} {Programming} {Models} and {Middlewar} ({ESPM2})},
	author = {Sun, Qian and Romanus, Melissa and Jin, Tong and Yu, Hongfeng and Bremer, Peer-Timo and Petruzza, Steve and Klasky, Scott and Parashar, Manish},
	month = nov,
	year = {2016},
	keywords = {Data models, Adaptation models, asynchronous coupling patterns, asynchronous interaction, Couplings, data exchange, Data processing, Data storage systems, Distributed databases, electronic data interchange, end-to-end execution time reduction, in-staging data placement, Monitoring, representative data coupling patterns, Runtime, scientific discovery, staging cores, staging nodes, task analysis, task-based scientific workflows, Titan Cray XK7 system},
	pages = {2--9},
	file = {IEEE Xplore Full Text PDF:/local/home/ag262995/Zotero/storage/A9SWWKAM/Sun et al. - 2016 - In-Staging Data Placement for Asynchronous Couplin.pdf:application/pdf;IEEE Xplore Abstract Record:/local/home/ag262995/Zotero/storage/9DML7VB4/7831554.html:text/html}
}

@Inbook{Wu2017,
author="Wu, Dongyao
and Sakr, Sherif
and Zhu, Liming",
editor="Zomaya, Albert Y.
and Sakr, Sherif",
title="Big Data Programming Models",
bookTitle="Handbook of Big Data Technologies",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="31--63",
abstract="Big Data programming models represent the style of programming and present the interfaces paradigm for developers to write big data applications and programs. Programming models normally the core feature of big data frameworks as they implicitly affects the execution model of big data processing engines and also drives the way for users to express and construct the big data applications and programs. In this chapter, we comprehensively investigate different programming models for big data frameworks with comparison and concrete code examples.",
isbn="978-3-319-49340-4",
doi="10.1007/978-3-319-49340-4_2",
url="https://doi.org/10.1007/978-3-319-49340-4_2"
}


@article{babuji_parsl_2019,
	title = {Parsl: {Pervasive} {Parallel} {Programming} in {Python}},
	shorttitle = {Parsl},
	url = {http://arxiv.org/abs/1905.02158},
	doi = {10.1145/3307681.3325400},
	abstract = {High-level programming languages such as Python are increasingly used to provide intuitive interfaces to libraries written in lower-level languages and for assembling applications from various components. This migration towards orchestration rather than implementation, coupled with the growing need for parallel computing (e.g., due to big data and the end of Moore's law), necessitates rethinking how parallelism is expressed in programs. Here, we present Parsl, a parallel scripting library that augments Python with simple, scalable, and flexible constructs for encoding parallelism. These constructs allow Parsl to construct a dynamic dependency graph of components that it can then execute efficiently on one or many processors. Parsl is designed for scalability, with an extensible set of executors tailored to different use cases, such as low-latency, high-throughput, or extreme-scale execution. We show, via experiments on the Blue Waters supercomputer, that Parsl executors can allow Python scripts to execute components with as little as 5 ms of overhead, scale to more than 250 000 workers across more than 8000 nodes, and process upward of 1200 tasks per second. Other Parsl features simplify the construction and execution of composite programs by supporting elastic provisioning and scaling of infrastructure, fault-tolerant execution, and integrated wide-area data management. We show that these capabilities satisfy the needs of many-task, interactive, online, and machine learning applications in fields such as biology, cosmology, and materials science.},
	urldate = {2020-09-24},
	journal = {Proceedings of the 28th International Symposium on High-Performance Parallel and Distributed Computing},
	author = {Babuji, Yadu and Woodard, Anna and Li, Zhuozhao and Katz, Daniel S. and Clifford, Ben and Kumar, Rohan and Lacinski, Lukasz and Chard, Ryan and Wozniak, Justin M. and Foster, Ian and Wilde, Michael and Chard, Kyle},
	month = jun,
	year = {2019},
	note = {arXiv: 1905.02158},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Programming Languages},
	pages = {25--36},
	file = {arXiv Fulltext PDF:/local/home/ag262995/Zotero/storage/42HMURQS/Babuji et al. - 2019 - Parsl Pervasive Parallel Programming in Python.pdf:application/pdf;arXiv.org Snapshot:/local/home/ag262995/Zotero/storage/6Z87QFWM/1905.html:text/html}
}

@article{wu_diva_2020,
	title = {Diva: {A} {Declarative} and {Reactive} {Language} for {In}-{Situ} {Visualization}},
	shorttitle = {Diva},
	url = {http://arxiv.org/abs/2001.11604},
	abstract = {The use of adaptive workflow management for in situ visualization and analysis has been a growing trend in large-scale scientific simulations. However, coordinating adaptive workflows with traditional procedural programming languages can be difficult because system flow is determined by unpredictable scientific phenomena, which often appear in an unknown order and can evade event handling. This makes the implementation of adaptive workflows tedious and error-prone. Recently, reactive and declarative programming paradigms have been recognized as well-suited solutions to similar problems in other domains. However, there is a dearth of research on adapting these approaches to in situ visualization and analysis. With this paper, we present a language design and runtime system for developing adaptive systems through a declarative and reactive programming paradigm. We illustrate how an adaptive workflow programming system is implemented using our approach and demonstrate it with a use case from a combustion simulation.},
	urldate = {2020-09-24},
	journal = {arXiv:2001.11604 [cs]},
	author = {Wu, Qi and Neuroth, Tyson and Igouchkine, Oleg and Aditya, Konduri and Chen, Jacqueline H. and Ma, Kwan-Liu},
	month = sep,
	year = {2020},
	note = {arXiv: 2001.11604},
	keywords = {Computer Science - Programming Languages, Computer Science - Human-Computer Interaction},
	annote = {Comment: 11 pages, 5 figures, 6 listings, 1 table, to be published in LDAV 2020. The article has gone through 2 major revisions: Emphasized contributions, features and examples. Addressed connections between DIVA and FRP. In sec. 3, we fixed a design flaw and addressed it in sec. 3.3-3.4. Re-designed sec. 5 with a more concrete example and benchmark results. Simplified the syntax of DIVA},
	file = {arXiv Fulltext PDF:/local/home/ag262995/Zotero/storage/VPASSK2M/Wu et al. - 2020 - Diva A Declarative and Reactive Language for In-S.pdf:application/pdf;arXiv.org Snapshot:/local/home/ag262995/Zotero/storage/KJ6R3KMF/2001.html:text/html}
}

@inproceedings{zanuz_-transit_2018_flink,
	address = {Dallas Texas USA},
	title = {In-transit molecular dynamics analysis with {Apache} flink},
	isbn = {978-1-4503-6579-6},
	url = {https://dl.acm.org/doi/10.1145/3281464.3281469},
	doi = {10.1145/3281464.3281469},
	abstract = {In this paper, an on-line parallel analytics framework is proposed to process and store in transit all the data being generated by a Molecular Dynamics (MD) simulation run using staging nodes in the same cluster executing the simulation. The implementation and deployment of such a parallel workflow with standard HPC tools, managing problems such as data partitioning and load balancing, can be a hard task for scientists. In this paper we propose to leverage Apache Flink, a scalable stream processing engine from the Big Data domain, in this HPC context. Flink enables to program analyses within a simple window based map/reduce model, while the runtime takes care of the deployment, load balancing and fault tolerance. We build a complete in transit analytics workflow, connecting an MD simulation to Apache Flink and to a distributed database, Apache HBase, to persist all the desired data. To demonstrate the expressivity of this programming model and its suitability for HPC scientific environments, two common analytics in the MD field have been implemented. We assessed the performance of this framework, concluding that it can handle simulations of sizes used in the literature while providing an effective and versatile tool for scientists to easily incorporate on-line parallel analytics in their current workflows.},
	language = {en},
	urldate = {2020-12-22},
	booktitle = {Proceedings of the {Workshop} on {In} {Situ} {Infrastructures} for {Enabling} {Extreme}-{Scale} {Analysis} and {Visualization}},
	publisher = {ACM},
	author = {Zanúz, Henrique C. and Raffin, Bruno and Mures, Omar A. and Padrón, Emilio J.},
	month = nov,
	year = {2018},
	pages = {25--32},
	file = {Zanúz et al. - 2018 - In-transit molecular dynamics analysis with Apache.pdf:/local/home/ag262995/Zotero/storage/PAJIMXE8/Zanúz et al. - 2018 - In-transit molecular dynamics analysis with Apache.pdf:application/pdf}
}

@inproceedings{wang_smart_2015,
	title = {Smart: a {MapReduce}-like framework for in-situ scientific analytics},
	shorttitle = {Smart},
	doi = {10.1145/2807591.2807650},
	abstract = {In-situ analytics has lately been shown to be an effective approach to reduce both I/O and storage costs for scientific analytics. Developing an efficient in-situ implementation, however, involves many challenges, including parallelization, data movement or sharing, and resource allocation. Based on the premise that MapReduce can be an appropriate API for specifying scientific analytics applications, we present a novel MapReduce-like framework that supports efficient in-situ scientific analytics, and address several challenges that arise in applying the MapReduce idea for in-situ processing. Specifically, our implementation can load simulated data directly from distributed memory, and it uses a modified API that helps meet the strict memory constraints of in-situ analytics. The framework is designed so that analytics can be launched from the parallel code region of a simulation program. We have developed both time sharing and space sharing modes for maximizing the performance in different scenarios, with the former even avoiding any copying of data from simulation to the analytics program. We demonstrate the functionality, efficiency, and scalability of our system, by using different simulation and analytics programs, executed on clusters with multi-core and many-core nodes.},
	booktitle = {{SC} '15: {Proceedings} of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	author = {Wang, Y. and Agrawal, G. and Bicer, T. and Jiang, W.},
	month = nov,
	year = {2015},
	note = {ISSN: 2167-4337},
	keywords = {Analytical models, API, application program interfaces, data analysis, data handling, Data models, data movement, digital simulation, Distributed databases, in-situ scientific analytics, IO costs, Loading, many-core nodes, MapReduce-like framework, Memory management, microprocessor chips, multicore nodes, multiprocessing systems, natural sciences computing, parallel code region, parallel processing, Programming, resource allocation, simulation program, Smart, space sharing modes, Sparks, storage costs, time sharing modes},
	pages = {1--12},
	file = {IEEE Xplore Full Text PDF:/local/home/ag262995/Zotero/storage/S7SL2RWI/Wang et al. - 2015 - Smart a MapReduce-like framework for in-situ scie.pdf:application/pdf;IEEE Xplore Abstract Record:/local/home/ag262995/Zotero/storage/KBG96LZV/7832825.html:text/html}
}

@misc{PDI,
	title = {PDI Documentation},
	url = {https://pdi.julien-bigot.fr/master/},
	language = {en},
	urldate = {2021-05-12},
}

@misc{IPCA,
	title = {dask-ml 0.1 documentation - dask\_ml.decomposition.{IncrementalPCA}},
	url = {modules/generated/dask_ml.decomposition.IncrementalPCA.html},
	abstract = {dask\_ml.decomposition.IncrementalPCA},
	language = {en},
	urldate = {2021-05-06},
	journal = {dask-ml 0.1 documentation},
	file = {Snapshot:/local/home/ag262995/Zotero/storage/MHF6DVX8/dask_ml.decomposition.IncrementalPCA.html:text/html}
}

@Inbook{Robison2011tbb,
author="Robison, Arch D.",
editor="Padua, David",
title="Intel® Threading Building Blocks (TBB)",
bookTitle="Encyclopedia of Parallel Computing",
year="2011",
publisher="Springer US",
address="Boston, MA",
pages="955--964",
isbn="978-0-387-09766-4",
doi="10.1007/978-0-387-09766-4_51",
url="https://doi.org/10.1007/978-0-387-09766-4_51"
}


@incollection{yokota_tins_2018,
	address = {Cham},
	title = {{TINS}: {A} {Task}-{Based} {Dynamic} {Helper} {Core} {Strategy} for {In} {Situ} {Analytics}},
	volume = {10776},
	isbn = {978-3-319-69952-3 978-3-319-69953-0},
	shorttitle = {{TINS}},
	url = {http://link.springer.com/10.1007/978-3-319-69953-0_10},
	abstract = {The in situ paradigm proposes to co-locate simulation and analytics on the same compute node to analyze data while still resident in the compute node memory, hence reducing the need for postprocessing methods. A standard approach that proved eﬃcient for sharing resources on each node consists in running the analytics processes on a set of dedicated cores, called helper cores, to isolate them from the simulation processes. Simulation and analytics thus run concurrently with limited interference. In this paper we show that the performance can be improved through a dynamic helper core strategy. We rely on a work stealing scheduler to implement TINS, a task-based in situ framework with an on-demand analytics isolation. The helper cores are dedicated to analytics only when analytics tasks are available. Otherwise the helper cores join the other cores for processing simulation tasks. TINS relies on the Intel R TBB library. Experiments on up to 14,336 cores run a set of representative analytics parallelized with TBB coupled with the hybrid MPI+TBB ExaStamp molecular dynamics code. TINS shows up to 40\% performance improvement over various other approaches including the standard helper core.},
	language = {en},
	urldate = {2020-12-22},
	booktitle = {Supercomputing {Frontiers}},
	publisher = {Springer International Publishing},
	author = {Dirand, Estelle and Colombet, Laurent and Raffin, Bruno},
	editor = {Yokota, Rio and Wu, Weigang},
	year = {2018},
	doi = {10.1007/978-3-319-69953-0_10},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {159--178},
	file = {Dirand et al. - 2018 - TINS A Task-Based Dynamic Helper Core Strategy fo.pdf:/local/home/ag262995/Zotero/storage/VYX6LXXS/Dirand et al. - 2018 - TINS A Task-Based Dynamic Helper Core Strategy fo.pdf:application/pdf}
}

@inproceedings{dirand2018tins,
  title={Tins: A task-based dynamic helper core strategy for in situ analytics},
  author={Dirand, Estelle and Colombet, Laurent and Raffin, Bruno},
  booktitle={Asian Conference on Supercomputing Frontiers},
  pages={159--178},
  year={2018},
  organization={Springer}
}

@inproceedings{zheng_goldrush_2013,
	address = {Denver, Colorado},
	title = {{GoldRush}: resource efficient in situ scientific data analytics using fine-grained interference aware execution},
	isbn = {978-1-4503-2378-9},
	shorttitle = {{GoldRush}},
	url = {http://dl.acm.org/citation.cfm?doid=2503210.2503279},
	doi = {10.1145/2503210.2503279},
	abstract = {Severe I/O bottlenecks on High End Computing platforms call for running data analytics in situ. Demonstrating that there exist considerable resources in compute nodes un-used by typical high end scientific simulations, we leverage this fact by creating an agile runtime, termed GoldRush, that can harvest those otherwise wasted, idle resources to efficiently run in situ data analytics. GoldRush uses fine-grained scheduling to “steal” idle resources, in ways that minimize interference between the simulation and in situ analytics. This involves recognizing the potential causes of on-node resource contention and then using scheduling methods that prevent them. Experiments with representative science applications at large scales show that resources harvested on compute nodes can be leveraged to perform useful analytics, significantly improving resource efficiency, reducing data movement costs incurred by alternate solutions, and posing negligible impact on scientific simulations.},
	language = {en},
	urldate = {2020-12-22},
	booktitle = {Proceedings of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis} on - {SC} '13},
	publisher = {ACM Press},
	author = {Zheng, Fang and Yu, Hongfeng and Hantas, Can and Wolf, Matthew and Eisenhauer, Greg and Schwan, Karsten and Abbasi, Hasan and Klasky, Scott},
	year = {2013},
	pages = {1--12},
	file = {Zheng et al. - 2013 - GoldRush resource efficient in situ scientific da.pdf:/local/home/ag262995/Zotero/storage/8G686UPB/Zheng et al. - 2013 - GoldRush resource efficient in situ scientific da.pdf:application/pdf}
}

@INPROCEEDINGS{damaris_viz,
  author={Dorier, Matthieu and Sisneros, Robert and Peterka, Tom and Antoniu, Gabriel and Semeraro, Dave},
  booktitle={2013 IEEE Symposium on Large-Scale Data Analysis and Visualization (LDAV)}, 
  title={Damaris/Viz: A nonintrusive, adaptable and user-friendly in situ visualization framework}, 
  year={2013},
  volume={},
  number={},
  pages={67-75},
  doi={10.1109/LDAV.2013.6675160}}

@inproceedings{dreher_manala_2017,
	title = {Manala: {A} {Flexible} {Flow} {Control} {Library} for {Asynchronous} {Task} {Communication}},
	shorttitle = {Manala},
	doi = {10.1109/CLUSTER.2017.31},
	abstract = {Tasks coupled in an in situ workflow may not process data at the same speed, potentially causing overflows in the communication channel between them. To prevent this problem, software infrastructures for in situ workflows usually impose a strict FIFO policy that has the side-effect of slowing down faster tasks to the speed of the slower ones. This may not be the desired behavior; for example, a scientist may prefer to drop older data in the communication channel in order to visualize the latest snapshot of a simulation. In this paper, we present Manala, a flexible flow control library designed to manage the flow of messages between a producer and a consumer in an in situ workflow. Manala intercepts messages from the producer, stores them, and selects the message to forward to the consumer depending on the flow control policy. The library is designed to ease the creation of new flow control policies and buffering mechanisms. We demonstrate with three examples how changing the flow control policy between tasks can influence the performance and results of scientific workflows. The first example focuses on materials science with LAMMPS and a synthetic diffraction analysis code. The second example is an interactive visualization scenario with Gromacs as the producer and Damaris/Viz as consumer. Our third example studies different strategies to perform an asynchronous checkpoint with Gromacs.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Cluster} {Computing} ({CLUSTER})},
	author = {Dreher, M. and Sasikumar, K. and Sankaranarayanan, S. and Peterka, T.},
	month = sep,
	year = {2017},
	note = {ISSN: 2168-9253},
	keywords = {scientific workflows, Data models, Data visualization, Libraries, Adaptation models, Analytical models, Gromacs, parallel processing, workflow management software, asynchronous checkpoint, asynchronous task communication, buffering mechanisms, checkpointing, communication channel, Communication channels, data visualisation, FIFO policy, flexible flow control library, flow control policy, interactive systems, interactive visualization, LAMMPS, Manala, materials science, materials science computing, message flow management, message passing, software infrastructures, synthetic diffraction analysis code},
	pages = {509--519},
	file = {IEEE Xplore Full Text PDF:/local/home/ag262995/Zotero/storage/QTKTQNDF/Dreher et al. - 2017 - Manala A Flexible Flow Control Library for Asynch.pdf:application/pdf;IEEE Xplore Abstract Record:/local/home/ag262995/Zotero/storage/4LE7VNPA/8048963.html:text/html}
}

@inproceedings{lofstead_insights_2013_adios,
	title = {Insights for exascale {IO} {APIs} from building a petascale {IO} {API}},
	doi = {10.1145/2503210.2503238},
	abstract = {Near the dawn of the petascale era, IO libraries had reached a stability in their function and data layout with only incremental changes being incorporated. The shift in technology, particularly the scale of parallel file systems and the number of compute processes, prompted revisiting best practices for optimal IO performance. Among other efforts like PLFS, the project that led to ADIOS, the ADaptable IO System, was motivated by both the shift in technology and the historical requirement, for optimal IO performance, to change how simulations performed IO depending on the platform. To solve both issues, the ADIOS team, along with consultation with other leading IO experts, sought to build a new IO platform based on the assumptions inherent in the petascale hardware platforms. This paper helps inform the design of future IO platforms with a discussion of lessons learned as part of the process of designing and building ADIOS.},
	booktitle = {{SC} '13: {Proceedings} of the {International} {Conference} on {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	author = {Lofstead, J. and Ross, R.},
	month = nov,
	year = {2013},
	note = {ISSN: 2167-4337},
	keywords = {Arrays, Libraries, parallel processing, Standards, adaptable IO system, ADIOS, application program interfaces, data layout, exascale IO APIs, file organisation, Hardware, IO libraries, Layout, multiprocessing systems, optimal IO performance, parallel file systems, Performance evaluation, petascale hardware platforms, petascale IO API, PLFS, Writing},
	pages = {1--12},
	file = {IEEE Xplore Full Text PDF:/local/home/ag262995/Zotero/storage/JU2Z9GMC/Lofstead and Ross - 2013 - Insights for exascale IO APIs from building a peta.pdf:application/pdf;IEEE Xplore Abstract Record:/local/home/ag262995/Zotero/storage/LJBILI6U/6877520.html:text/html}
}

@article{ramoncortes_programming_2020_pycompss,
	title = {A {Programming} {Model} for {Hybrid} {Workflows}: combining {Task}-based {Workflows} and {Dataflows} all-in-one},
	volume = {113},
	issn = {0167739X},
	shorttitle = {A {Programming} {Model} for {Hybrid} {Workflows}},
	url = {http://arxiv.org/abs/2007.04939},
	doi = {10.1016/j.future.2020.07.007},
	abstract = {This paper tries to reduce the effort of learning, deploying, and integrating several frameworks for the development of e-Science applications that combine simulations with High-Performance Data Analytics (HPDA). We propose a way to extend task-based management systems to support continuous input and output data to enable the combination of task-based workflows and dataflows (Hybrid Workflows from now on) using a single programming model. Hence, developers can build complex Data Science workflows with different approaches depending on the requirements. To illustrate the capabilities of Hybrid Workflows, we have built a Distributed Stream Library and a fully functional prototype extending COMPSs, a mature, general-purpose, task-based, parallel programming model. The library can be easily integrated with existing task-based frameworks to provide support for dataflows. Also, it provides a homogeneous, generic, and simple representation of object and file streams in both Java and Python; enabling complex workflows to handle any data type without dealing directly with the streaming back-end.},
	urldate = {2020-12-22},
	journal = {Future Generation Computer Systems},
	author = {Ramon-Cortes, Cristian and Lordan, Francesc and Ejarque, Jorge and Badia, Rosa M.},
	month = dec,
	year = {2020},
	note = {arXiv: 2007.04939},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	pages = {281--297},
	annote = {Comment: Accepted in Future Generation Computer Systems (FGCS). Licensed under CC-BY-NC-ND},
	file = {arXiv Fulltext PDF:/local/home/ag262995/Zotero/storage/JGBXRJYZ/Ramon-Cortes et al. - 2020 - A Programming Model for Hybrid Workflows combinin.pdf:application/pdf;arXiv.org Snapshot:/local/home/ag262995/Zotero/storage/G2PQPR28/2007.html:text/html}
}

@misc{noauthor_actors_ray,
        title = {Actors — {Ray} 2.0.1},
        url = {https://docs.ray.io/en/latest/ray-core/actors.html},
        urldate = {2022-11-06},
}


@misc{noauthor_actors_nodate,
        title = {Actors: {A} unifying model for parallel and distributed computing - {ScienceDirect}},
        url = {https://www.sciencedirect.com/science/article/pii/S1383762198000678},
        urldate = {2022-11-06},
        file = {Actors\: A unifying model for parallel and distributed computing - ScienceDirect:/local/home/ag262995/Zotero/storage/GFQIBK3W/S1383762198000678.html:text/html},
}


@misc{noauthor_daskdistributed_nodate,
        title = {Dask.distributed — {Dask}.distributed 2022.10.2 documentation},
        url = {https://distributed.dask.org/en/stable/},
        urldate = {2022-11-06},
}


@misc{amal_distributed_2022,
        title = {Distributed},
        copyright = {BSD-3-Clause},
        url = {https://github.com/GueroudjiAmal/distributed},
        abstract = {A distributed task scheduler for Dask},
        urldate = {2022-11-06},
        author = {Amal, GUEROUDJI},
        month = jan,
        year = {2022},
        note = {original-date: 2021-12-02T09:44:00Z},
}

@phdthesis{dirand2018integration,
  title={Integration of high-performance task-based in situ for molecular dynamics on exascale computers},
  author={Dirand, Estelle},
  year={2018},
  school={Universit{\'e} Grenoble Alpes}
}

@article{docan2010enabling_dart,
  title={Enabling high-speed asynchronous data extraction and transfer using DART},
  author={Docan, Ciprian and Parashar, Manish and Klasky, Scott},
  journal={Concurrency and Computation: Practice and Experience},
  volume={22},
  number={9},
  pages={1181--1204},
  year={2010},
  publisher={Wiley Online Library}
}


@inproceedings{DART_2008,
author = {Docan, Ciprian and Parashar, Manish and Klasky, Scott},
title = {DART: A Substrate for High Speed Asynchronous Data IO},
year = {2008},
isbn = {9781595939975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1383422.1383454},
doi = {10.1145/1383422.1383454},
abstract = {Large scale simulations of complex physics phenomena have long run times and generate massive amounts of data. Saving this data to external storage systems or transferring it to remote locations for analysis is a costly operation that quickly becomes a performance bottleneck. In this paper, we present DART (Decoupled and Asynchronous Remote Transfers), an efficient data transfer substrate that effectively minimizes the data I/O overhead on the running simulations. DART is a thin software layer built on RDMA technology to enable fast, low-overhead and asynchronous access to data from a running simulation, and support high-throughput, low-latency data transfers.},
booktitle = {Proceedings of the 17th International Symposium on High Performance Distributed Computing},
pages = {219–220},
numpages = {2},
keywords = {rdma asynchronous data transfers},
location = {Boston, MA, USA},
series = {HPDC '08}
}


@techreport{dorier_damaris_2012_tech_report,
  TITLE = {{Damaris: Leveraging Multicore Parallelism to Mask I/O Jitter}},
  AUTHOR = {Dorier, Matthieu and Antoniu, Gabriel and Cappello, Franck and Snir, Marc and Orf, Leigh},
  URL = {https://hal.inria.fr/inria-00614597},
  TYPE = {Research Report},
  NUMBER = {RR-7706},
  PAGES = {36},
  INSTITUTION = {{INRIA}},
  YEAR = {2012},
  MONTH = Apr,
  KEYWORDS = {Exascale Computing ; Multicore Architectures ; I/O ; Variability ; Dedicated Cores},
  PDF = {https://hal.inria.fr/inria-00614597v3/file/RR-7706.pdf},
  HAL_ID = {inria-00614597},
  HAL_VERSION = {v3},
}

@inproceedings{cidfuentes_dislib_2019_pycompss,
	title = {dislib: Large Scale High Performance Machine Learning in Python},
	shorttitle = {dislib},
	doi = {10.1109/eScience.2019.00018},
	abstract = {In recent years, machine learning has proven to be an extremely useful tool for extracting knowledge from data. This can be leveraged in numerous research areas, such as genomics, earth sciences, and astrophysics, to gain valuable insight. At the same time, Python has become one of the most popular programming languages among researchers due to its high productivity and rich ecosystem. Unfortunately, existing machine learning libraries for Python do not scale to large data sets, are hard to use by non-experts, and are difficult to set up in high performance computing clusters. These limitations have prevented scientists to exploit the full potential of machine learning in their research. In this paper, we present and evaluate dislib, a distributed machine learning library on top of PyCOMPSs programming model that addresses the issues of other existing libraries. In our evaluation, we show that dislib can be up to 9 times faster, and can process data sets up to 16 times larger than other popular distributed machine learning libraries, such as MLlib. In addition to this, we also show how dislib can be used to reduce the computation time of a real scientific application from 18 hours to 17 minutes.},
	booktitle = {2019 15th {International} {Conference} on {eScience} ({eScience})},
	author = {Cid-Fuentes, J. Alvarez and Sola, S. and Alvarez, P. and Castro-Ginard, A. and Badia, R. M.},
	month = sep,
	year = {2019},
	keywords = {machine learning, parallel processing, big data, computation time, data mining, dislib, distributed machine learning libraries, distributed machine learning library, earth sciences, high performance computing, high performance computing clusters, hpc, large scale, large-scale high performance machine learning, learning (artificial intelligence), PyCOMPSs programming model, python, Python},
	pages = {96--105},
	file = {IEEE Xplore Full Text PDF:/local/home/ag262995/Zotero/storage/THIR4T7S/Cid-Fuentes et al. - 2019 - dislib Large Scale High Performance Machine Learn.pdf:application/pdf;IEEE Xplore Abstract Record:/local/home/ag262995/Zotero/storage/FEUN6MIV/9041782.html:text/html}
}

@inproceedings{elshazly_performance_2020_pycompss,
	title = {Performance {Meets} {Programmabilty}: {Enabling} {Native} {Python} {MPI} {Tasks} {In} {PyCOMPSs}},
	shorttitle = {Performance {Meets} {Programmabilty}},
	doi = {10.1109/PDP50117.2020.00016},
	abstract = {The increasing complexity of modern and future computing systems makes it challenging to develop applications that aim for maximum performance. Hybrid parallel programming models offer new ways to exploit the capabilities of the underlying infrastructure. However, the performance gain is sometimes accompanied by increased programming complexity. We introduce an extension to PyCOMPSs, a high-level task-based parallel programming model for Python applications, to support tasks that use MPI natively as part of the task model. Without compromising application's programmability, using Native MPI tasks in PyCOMPSs offers up to 3x improvement in total performance for compute intensive applications and up to 1.9x improvement in total performance for I/O intensive applications over sequential implementation of the tasks.},
	booktitle = {2020 28th {Euromicro} {International} {Conference} on {Parallel}, {Distributed} and {Network}-{Based} {Processing} ({PDP})},
	author = {Elshazly, H. and Lordan, F. and Ejarque, J. and Badia, R. M.},
	month = mar,
	year = {2020},
	note = {ISSN: 2377-5750},
	keywords = {Runtime, Computational modeling, message passing, application program interfaces, Python, application programmability, computing systems, Distributed Computing, High Performance Computing, high-level task-based parallel programming model, hybrid parallel programming models, Hybrid Programming Models, I-O intensive applications, MPI, native Python MPI tasks, Parallel processing, parallel programming, Parallel programming, Performance, Productivity, programming complexity, PyCOMPSs, Python applications, sequential implementation, Task analysis, Task-based Parallel Programming Models},
	pages = {63--66},
	file = {IEEE Xplore Full Text PDF:/local/home/ag262995/Zotero/storage/L2GLDBUD/Elshazly et al. - 2020 - Performance Meets Programmabilty Enabling Native .pdf:application/pdf;IEEE Xplore Abstract Record:/local/home/ag262995/Zotero/storage/Q7CHFXKZ/9092419.html:text/html}
}

@article{jaure_comparisons_nodate,
	title = {{COMPARISONS} {OF} {COUPLING} {STRATEGIES} {FOR} {MASSIVELY} {PARALLEL} {CONJUGATE} {HEAT} {TRANSFER} {WITH} {LARGE} {EDDY} {SIMULATION}},
	abstract = {The optimization of gas turbines is a complex multi-physic and multi-component problem that has long been based on engineer intuitions and expensive experiments or trial and error tests. Today, turbine experts commonly acknowledge that computer simulation is a very promising path for optimization, which can reduce costs and diminish the duration of the design process. Computations however remain a great challenge essentially because of the High Performance Computing (HPC) context, which is necessary for accurate estimates of real-life type of problems. Despite this diﬃculty, current highﬁdelity computer simulations become accessible for speciﬁc components of gas turbines [5]. These stand-alone simulations and solutions now face a new challenge: to improve the quality of the results, new physics must be introduced with speciﬁc and distinct numerical models. For example, in the context of multi-component simulations, further improving the accuracy of turbine wall temperature is of limited interest if wall temperature boundary conditions are still set approximately. Dealing with multi-physics, recent studies have shown interesting results by taking into account reactive ﬂow as well as radiative and conductive heat transfers to predict wall temperature of a helicopter combustion chamber [2, 1].},
	language = {en},
	author = {Jaure, S and Duchaine, F and Gicquel, L Y M},
	pages = {11},
	file = {Jaure et al. - COMPARISONS OF COUPLING STRATEGIES FOR MASSIVELY P.pdf:/local/home/ag262995/Zotero/storage/QAZH7ZI9/Jaure et al. - COMPARISONS OF COUPLING STRATEGIES FOR MASSIVELY P.pdf:application/pdf}
}
@article{deelman_pegasus_2005,
	title = {Pegasus: a framework for mapping complex scientific workflows onto distributed systems},
	volume = {13},
	shorttitle = {Pegasus},
	abstract = {This paper describes the Pegasus framework that can be used to map complex scientific workflows onto distributed resources. Pegasus enables users to represent the workflows at an abstract level without needing to worry about the particulars of the target execution systems. The paper describes general issues in mapping applications and the functionality of Pegasus. We present the results of improving application performance through workflow restructuring.},
	journal = {Scientific Programming Journal},
	author = {Deelman, Ewa and Singh, Gurmeet and Su, Mei-hui and Blythe, James and Gil, Yolanda and Kesselman, Carl and Mehta, Gaurang and Vahi, Karan and Berriman, G. Bruce and Good, John and Laity, Anastasia and Jacob, Joseph C. and Katz, Daniel S.},
	year = {2005},
	pages = {219--237},
	file = {Citeseer - Full Text PDF:/local/home/ag262995/Zotero/storage/KZDBKHWV/Deelman et al. - 2005 - Pegasus a framework for mapping complex scientifi.pdf:application/pdf;Citeseer - Snapshot:/local/home/ag262995/Zotero/storage/GAPIVIJV/download.html:text/html}
}
@inproceedings{deelman_pegasus_2004,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Pegasus: {Mapping} {Scientific} {Workflows} onto the {Grid}},
	isbn = {978-3-540-28642-4},
	shorttitle = {Pegasus},
	doi = {10.1007/978-3-540-28642-4_2},
	abstract = {In this paper we describe the Pegasus system that can map complex workflows onto the Grid. Pegasus takes an abstract description of a workflow and finds the appropriate data and Grid resources to execute the workflow. Pegasus is being released as part of the GriPhyN Virtual Data Toolkit and has been used in a variety of applications ranging from astronomy, biology, gravitational-wave science, and high-energy physics. A deferred planning mode of Pegasus is also introduced.},
	language = {en},
	booktitle = {Grid {Computing}},
	publisher = {Springer},
	author = {Deelman, Ewa and Blythe, James and Gil, Yolanda and Kesselman, Carl and Mehta, Gaurang and Patil, Sonal and Su, Mei-Hui and Vahi, Karan and Livny, Miron},
	editor = {Dikaiakos, Marios D.},
	year = {2004},
	keywords = {Gravitational Wave, Grid Environment, Grid Resource, High Performance Computing Application, Virtual Data},
	pages = {11--20},
	file = {Springer Full Text PDF:/local/home/ag262995/Zotero/storage/AL5XGVJW/Deelman et al. - 2004 - Pegasus Mapping Scientific Workflows onto the Gri.pdf:application/pdf}
}
@inproceedings{bauer_legion_2012,
        address = {Salt Lake City, UT},
        title = {Legion: {Expressing} locality and independence with logical regions},
        isbn = {978-1-4673-0805-2 978-1-4673-0806-9},
        shorttitle = {Legion},
        url = {http://ieeexplore.ieee.org/document/6468504/},
        doi = {10.1109/SC.2012.71},
        abstract = {Modern parallel architectures have both heterogeneous processors and deep, complex memory hierarchies. We present Legion, a programming model and runtime system for achieving high performance on these machines. Legion is organized around logical regions, which express both locality and independence of program data, and tasks, functions that perform computations on regions. We describe a runtime system that dynamically extracts parallelism from Legion programs, using a distributed, parallel scheduling algorithm that identiﬁes both independent tasks and nested parallelism. Legion also enables explicit, programmer controlled movement of data through the memory hierarchy and placement of tasks based on locality information via a novel mapping interface. We evaluate our Legion implementation on three applications: ﬂuid-ﬂow on a regular grid, a three-level AMR code solving a heat diffusion equation, and a circuit simulation.},
        language = {en},
        urldate = {2022-11-04},
        booktitle = {2012 {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
        publisher = {IEEE},
        author = {Bauer, Michael and Treichler, Sean and Slaughter, Elliott and Aiken, Alex},
        month = nov,
        year = {2012},
        pages = {1--11},
        file = {Bauer et al. - 2012 - Legion Expressing locality and independence with .pdf:/local/home/ag262995/Zotero/storage/49P6AIP8/Bauer et al. - 2012 - Legion Expressing locality and independence with .pdf:application/pdf},
}




@inproceedings{mehta_enabling_2012,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Enabling {Data} and {Compute} {Intensive} {Workflows} in {Bioinformatics}},
	isbn = {978-3-642-29740-3},
	doi = {10.1007/978-3-642-29740-3_4},
	abstract = {Accelerated growth in the field of bioinformatics has resulted in large data sets being produced and analyzed. With this rapid growth has come the need to analyze these data in a quick, easy, scalable, and reliable manner on a variety of computing infrastructures including desktops, clusters, grids and clouds. This paper presents the application of workflow technologies, and, specifically, Pegasus WMS, a robust scientific workflow management system, to a variety of bioinformatics projects from RNA sequencing, proteomics, and data quality control in population studies using GWAS data.},
	language = {en},
	booktitle = {Euro-{Par} 2011: {Parallel} {Processing} {Workshops}},
	publisher = {Springer},
	author = {Mehta, Gaurang and Deelman, Ewa and Knowles, James A. and Chen, Ting and Wang, Ying and Vöckler, Jens and Buyske, Steven and Matise, Tara},
	editor = {Alexander, Michael and D’Ambra, Pasqua and Belloum, Adam and Bosilca, George and Cannataro, Mario and Danelutto, Marco and Di Martino, Beniamino and Gerndt, Michael and Jeannot, Emmanuel and Namyst, Raymond and Roman, Jean and Scott, Stephen L. and Traff, Jesper Larsson and Vallée, Geoffroy and Weidendorfer, Josef},
	year = {2012},
	keywords = {bioinformatics, epigenetics, proteomics, sequencing, workflows},
	pages = {23--32},
	file = {Springer Full Text PDF:/local/home/ag262995/Zotero/storage/J3SPBBHT/Mehta et al. - 2012 - Enabling Data and Compute Intensive Workflows in B.pdf:application/pdf}
}

@article{vahi_case_2013,
	title = {A {Case} {Study} into {Using} {Common} {Real}-{Time} {Workflow} {Monitoring} {Infrastructure} for {Scientific} {Workflows}},
	volume = {11},
	issn = {1572-9184},
	url = {https://doi.org/10.1007/s10723-013-9265-4},
	doi = {10.1007/s10723-013-9265-4},
	abstract = {Scientific workflow systems support various workflow representations, operational modes, and configurations. Regardless of the system used, end users have common needs: to track the status of their workflows in real time, be notified of execution anomalies and failures automatically, perform troubleshooting, and automate the analysis of the workflow results. In this paper, we describe how the Stampede monitoring infrastructure was integrated with the Pegasus Workflow Management System and the Triana Workflow Systems, in order to add generic real time monitoring and troubleshooting capabilities across both systems. Stampede is an infrastructure that provides interoperable monitoring using a three-layer model: (1) a common data model to describe workflow and job executions; (2) high-performance tools to load workflow logs conforming to the data model into a data store; and (3) a common query interface. This paper describes the integration of Stampede monitoring architecture with Pegasus and Triana and shows the new analysis capabilities that Stampede provides to these workflow systems. The successful integration of Stampede with these workflow engines demonstrates the generic nature of the Stampede monitoring infrastructure and its potential to provide a common platform for monitoring across scientific workflow engines.},
	language = {en},
	number = {3},
	urldate = {2020-12-22},
	journal = {Journal of Grid Computing},
	author = {Vahi, Karan and Harvey, Ian and Samak, Taghrid and Gunter, Daniel and Evans, Kieran and Rogers, David and Taylor, Ian and Goode, Monte and Silva, Fabio and Al-Shakarchi, Eddie and Mehta, Gaurang and Deelman, Ewa and Jones, Andrew},
	month = sep,
	year = {2013},
	pages = {381--406},
	file = {Springer Full Text PDF:/local/home/ag262995/Zotero/storage/KBEKIBPK/Vahi et al. - 2013 - A Case Study into Using Common Real-Time Workflow .pdf:application/pdf}
}

@article{liu_survey_2015,
	title = {A {Survey} of {Data}-{Intensive} {Scientific} {Workflow} {Management}},
	volume = {13},
	issn = {1572-9184},
	url = {https://doi.org/10.1007/s10723-015-9329-8},
	doi = {10.1007/s10723-015-9329-8},
	abstract = {Nowadays, more and more computer-based scientific experiments need to handle massive amounts of data. Their data processing consists of multiple computational steps and dependencies within them. A data-intensive scientific workflow is useful for modeling such process. Since the sequential execution of data-intensive scientific workflows may take much time, Scientific Workflow Management Systems (SWfMSs) should enable the parallel execution of data-intensive scientific workflows and exploit the resources distributed in different infrastructures such as grid and cloud. This paper provides a survey of data-intensive scientific workflow management in SWfMSs and their parallelization techniques. Based on a SWfMS functional architecture, we give a comparative analysis of the existing solutions. Finally, we identify research issues for improving the execution of data-intensive scientific workflows in a multisite cloud.},
	language = {en},
	number = {4},
	urldate = {2020-12-22},
	journal = {Journal of Grid Computing},
	author = {Liu, Ji and Pacitti, Esther and Valduriez, Patrick and Mattoso, Marta},
	month = dec,
	year = {2015},
	pages = {457--493},
	file = {Springer Full Text PDF:/local/home/ag262995/Zotero/storage/ZACQDGYR/Liu et al. - 2015 - A Survey of Data-Intensive Scientific Workflow Man.pdf:application/pdf}
}

@misc{noauthor_elsevier_nodate,
	title = {Elsevier {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S2352711019302560?token=BED53BE7D684122FAE145804037BA8FA4398DEF4B459FAF82849EEFDC598D9A54F047471D1DDB103B80B8464ECE9D438},
	language = {en},
	urldate = {2020-12-22},
	doi = {10.1016/j.softx.2020.100561},
	file = {Snapshot:/local/home/ag262995/Zotero/storage/NC67JMAL/S2352711019302560.html:text/html}
}

@inproceedings{boyuka_transparent_2014_adios,
	title = {Transparent in {Situ} {Data} {Transformations} in {ADIOS}},
	doi = {10.1109/CCGrid.2014.73},
	abstract = {Though an abundance of novel "data transformation" technologies have been developed (such as compression, level-of-detail, layout optimization, and indexing), there remains a notable gap in the adoption of such services by scientific applications. In response, we develop an in situ data transformation framework in the ADIOS I/O middleware with a "plug in" interface, thus greatly simplifying both the deployment and use of data transform services in scientific applications. Our approach ensures user-transparency, runtime-configurability, compatibility with existing I/O optimizations, and the potential for exploiting read-optimizing transforms (such as level-of-detail) to achieve I/O reduction. We demonstrate use of our framework with the QLG simulation at up to 8,192 cores on the leadership-class Titan supercomputer, showing negligible overhead. We also explore the read performance implications of data transforms with respect to parameters such as chunk size, access pattern, and the "opacity" of different transform methods including compression and level-of-detail.},
	booktitle = {2014 14th {IEEE}/{ACM} {International} {Symposium} on {Cluster}, {Cloud} and {Grid} {Computing}},
	author = {Boyuka, D. A. and Lakshminarasimham, S. and Zou, X. and Gong, Z. and Jenkins, J. and Schendel, E. R. and Podhorszki, N. and Liu, Q. and Klasky, S. and Samatova, N. F.},
	month = may,
	year = {2014},
	keywords = {Arrays, Data models, middleware, Middleware, Runtime, input-output programs, ADIOS, Layout, access pattern, chunk size, compression, compression method, data handling, data transformations, data transforms, I/O middleware, I/O optimizations, I/O reduction, indexing, leadership-class Titan supercomputer, level-of-detail, level-of-detail method, plug in interface, QLG simulation, read performance implications, read-optimizing transforms, runtime-configurability, storage layout optimization, transform method opacity, Transforms, user-transparency, XML},
	pages = {256--266},
	file = {IEEE Xplore Full Text PDF:/local/home/ag262995/Zotero/storage/CWXA5WJ9/Boyuka et al. - 2014 - Transparent in Situ Data Transformations in ADIOS.pdf:application/pdf;IEEE Xplore Abstract Record:/local/home/ag262995/Zotero/storage/MIUIGEGJ/6846461.html:text/html}
}

@inproceedings{larsen_alpine_2017,
	address = {Denver CO USA},
	title = {The {ALPINE} {In} {Situ} {Infrastructure}: {Ascending} from the {Ashes} of {Strawman}},
	isbn = {978-1-4503-5139-3},
	shorttitle = {The {ALPINE} {In} {Situ} {Infrastructure}},
	url = {https://dl.acm.org/doi/10.1145/3144769.3144778},
	doi = {10.1145/3144769.3144778},
	abstract = {This paper introduces ALPINE, a flyweight in situ infrastructure. The infrastructure is designed for leading-edge supercomputers, and has support for both distributed-memory and shared-memory parallelism. It can take advantage of computing power on both conventional CPU architectures and on many-core architectures such as NVIDIA GPUs or the Intel Xeon Phi. Further, it has a flexible design that supports for integration of new visualization and analysis routines and libraries. The paper describes ALPINE’s interface choices and architecture, and also reports on initial experiments performed using the infrastructure.},
	language = {en},
	urldate = {2020-12-23},
	booktitle = {Proceedings of the {In} {Situ} {Infrastructures} on {Enabling} {Extreme}-{Scale} {Analysis} and {Visualization}},
	publisher = {ACM},
	author = {Larsen, Matthew and Ahrens, James and Ayachit, Utkarsh and Brugger, Eric and Childs, Hank and Geveci, Berk and Harrison, Cyrus},
	month = nov,
	year = {2017},
	pages = {42--46},
	file = {Larsen et al. - 2017 - The ALPINE In Situ Infrastructure Ascending from .pdf:/local/home/ag262995/Zotero/storage/TUA4YUA9/Larsen et al. - 2017 - The ALPINE In Situ Infrastructure Ascending from .pdf:application/pdf}
}

@inproceedings{ahrens_image-based_2014,
	title = {An {Image}-{Based} {Approach} to {Extreme} {Scale} in {Situ} {Visualization} and {Analysis}},
	doi = {10.1109/SC.2014.40},
	abstract = {Extreme scale scientific simulations are leading a charge to exascale computation, and data analytics runs the risk of being a bottleneck to scientific discovery. Due to power and I/O constraints, we expect in situ visualization and analysis will be a critical component of these workflows. Options for extreme scale data analysis are often presented as a stark contrast: write large files to disk for interactive, exploratory analysis, or perform in situ analysis to save detailed data about phenomena that a scientists knows about in advance. We present a novel framework for a third option - a highly interactive, image-based approach that promotes exploration of simulation results, and is easily accessed through extensions to widely used open source tools. This in situ approach supports interactive exploration of a wide range of results, while still significantly reducing data movement and storage.},
	booktitle = {{SC} '14: {Proceedings} of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	author = {Ahrens, J. and Jourdain, S. and O'Leary, P. and Patchett, J. and Rogers, D. H. and Petersen, M.},
	month = nov,
	year = {2014},
	note = {ISSN: 2167-4337},
	keywords = {Data models, Data visualization, Analytical models, Computational modeling, data analysis, data visualisation, Atmospheric modeling, Cameras, Databases, extreme scale data analysis, extreme scale in situ visualization, extreme scale scientific simulations, image processing, interactive image-based approach, open source tools, public domain software, software tools},
	pages = {424--434},
	file = {IEEE Xplore Full Text PDF:/local/home/ag262995/Zotero/storage/REERKL7J/Ahrens et al. - 2014 - An Image-Based Approach to Extreme Scale in Situ V.pdf:application/pdf;IEEE Xplore Abstract Record:/local/home/ag262995/Zotero/storage/8DQRDR7C/7013022.html:text/html}
}

@INPROCEEDINGS{daskpp1,
       author = {{Redl}, Robert and {Keil}, Christian and {Craig}, George and {Lerch}, Sebastian and {Eichhorn}, Joachim},
        title = "{Towards a Framework for Parallelized Post-Processing and Evaluation of Ensemble Forecasts}",
    booktitle = {EGU General Assembly Conference Abstracts},
         year = 2018,
       series = {EGU General Assembly Conference Abstracts},
        month = apr,
        pages = {12322},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018EGUGA..2012322R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{daskpp2,
author = {Paraskevakos, Ioannis and Luckow, Andre and Khoshlessan, Mahzad and Chantzialexiou, George and Cheatham, Thomas E. and Beckstein, Oliver and Fox, Geoffrey C. and Jha, Shantenu},
title = {Task-Parallel Analysis of Molecular Dynamics Trajectories},
year = {2018},
isbn = {9781450365109},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3225058.3225128},
doi = {10.1145/3225058.3225128},
abstract = {Different parallel frameworks for implementing data analysis applications have been proposed by the HPC and Big Data communities. In this paper, we investigate three task-parallel frameworks: Spark, Dask and RADICAL-Pilot with respect to their ability to support data analytics on HPC resources and compare them to MPI. We investigate the data analysis requirements of Molecular Dynamics (MD) simulations which are significant consumers of supercomputing cycles, producing immense amounts of data. A typical large-scale MD simulation of a physical system of O(100k) atoms over μsecs can produce from O(10) GB to O(1000) GBs of data. We propose and evaluate different approaches for parallelization of a representative set of MD trajectory analysis algorithms, in particular the computation of path similarity and leaflet identification. We evaluate Spark, Dask and RADICAL-Pilot with respect to their abstractions and runtime engine capabilities to support these algorithms. We provide a conceptual basis for comparing and understanding different frameworks that enable users to select the optimal system for each application. We also provide a quantitative performance analysis of the different algorithms across the three frameworks.},
booktitle = {Proceedings of the 47th International Conference on Parallel Processing},
articleno = {49},
numpages = {10},
keywords = {Data analytics, task-parallel, MD Simulations Analysis, MD analysis},
location = {Eugene, OR, USA},
series = {ICPP 2018}
}
@inproceedings{daskpp3,
author = {Khoshlessan, Mahzad and Paraskevakos, Ioannis and Jha, Shantenu and Beckstein, Oliver},
year = {2017},
month = {01},
pages = {64-72},
title = {Parallel Analysis in MDAnalysis using the Dask Parallel Computing Library},
doi = {10.25080/shinma-7f4c6e7-00a}
}

@INPROCEEDINGS{daskpp4,
       author = {{Barnes}, W. and {Cheung}, C.~M.~M. and {Bobra}, M.},
        title = "{The Sun at Scale: Interactive Analysis of High Resolution EUV Imaging Data on HPC Platforms with Dask}",
     keywords = {1976 Software tools and services, INFORMATICS, 1982 Standards, INFORMATICS, 7594 Instruments and techniques, SOLAR PHYSICS, ASTROPHYSICS, AND ASTRONOMY},
    booktitle = {AGU Fall Meeting Abstracts},
         year = 2019,
       volume = {2019},
        month = dec,
          eid = {SH41C-3317},
        pages = {SH41C-3317},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019AGUFMSH41C3317B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@INPROCEEDINGS{daskpp5,
    author={Nguyen, Mai H. and Li, Jiaxin and Crawl, Daniel and Block, Jessica and Altintas, Ilkay},
    booktitle={2019 IEEE International Conference on Big Data (Big Data)},
    title={Scaling Deep Learning-Based Analysis of High-Resolution Satellite Imagery with Distributed Processing},
    year={2019},
    pages={5437-5443},
    doi={10.1109/BigData47090.2019.9006205}
}

@article{YuuichiPCA,
author = {Asahi,Yuuichi  and Fujii,Keisuke  and Heim,Dennis Manuel  and Maeyama,Shinya  and Garbet,Xavier  and Grandgirard,Virginie  and Sarazin,Yanick  and Dif-Pradalier,Guilhem  and Idomura,Yasuhiro  and Yagi,Masatoshi },
title = {Compressing the time series of five dimensional distribution function data from gyrokinetic simulation using principal component analysis},
journal = {Physics of Plasmas},
volume = {28},
number = {1},
pages = {012304},
year = {2021},
doi = {10.1063/5.0023166},
URL = { https://doi.org/10.1063/5.0023166},
eprint = { https://doi.org/10.1063/5.0023166}
} 

@inproceedings{bigot:hal-01050322-gysela,
 address = {Luminy, France},
 author = {Bigot, Julien and Grandgirard, Virginie and Latu, Guillaume and Passeron, Chantal and Rozar, Fabien and Thomine, Olivier},
 booktitle = {ESAIM: PROCEEDINGS},
 doi = {10.1051/proc/201343007},
 hal_id = {hal-01050322},
 hal_version = {v1},
 month = {July},
 pages = {117-135},
 pdf = {https://hal.inria.fr/hal-01050322/file/proc134308.pdf},
 series = {43},
 title = {Scaling GYSELA code beyond 32K-cores on Blue Gene/Q},
 url = {https://hal.inria.fr/hal-01050322},
 volume = {CEMRACS 2012},
 year = {2012}
}

@inproceedings{latu:hal-01719208-gysela,
 address = {Lyon, France},
 author = {Latu, Guillaume and ASAHI, Yuuichi and Bigot, Julien and Fehér, Tamás and Grandgirard, Virginie},
 booktitle = {SBAC-PAD 2018, WAMCA workshop},
 hal_id = {hal-01719208},
 hal_version = {v2},
 keywords = {SIMD ; KNL ; plasma physics ; vectorization ; many-core},
 month = {September},
 pdf = {https://hal.inria.fr/hal-01719208v2/file/wamca18_gl.pdf},
 series = {SBAC-PAD 2018 proceedings},
 title = {Scaling and optimizing the Gysela code on a cluster of many-core processors},
 url = {https://hal.inria.fr/hal-01719208},
 year = {2018}
}

@inproceedings{latu:hal-01834323-gysela,
 address = {Lausanne, France},
 author = {Latu, Guillaume and Bigot, Julien and Bouzat, Nicolas and Gimenez, Judit and Grandgirard, Virginie},
 booktitle = {PASC '16 - Proceedings of the Platform for Advanced Scientific Computing Conference},
 doi = {10.1145/2929908.2929912},
 hal_id = {hal-01834323},
 hal_version = {v1},
 month = {June},
 publisher = {ACM Press},
 title = {Benefits of SMT and of Parallel Transpose Algorithm for the Large-Scale GYSELA Application},
 url = {https://hal.archives-ouvertes.fr/hal-01834323},
 year = {2016}
}


