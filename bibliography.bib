@INPROCEEDINGS{deisa,
  author={Gueroudji, Amal and Bigot, Julien and Raffin, Bruno},
  booktitle={2021 IEEE 28th International Conference on High Performance Computing, Data, and Analytics (HiPC)}, 
  title={DEISA: Dask-Enabled In Situ Analytics}, 
  year={2021},
  volume={},
  number={},
  pages={11-20},
  doi={10.1109/HiPC53243.2021.00015}}


@incollection{goos_learning_2001,
	address = {Berlin, Heidelberg},
	title = {Learning from the {Success} of {MPI}},
	volume = {2228},
	isbn = {978-3-540-43009-4 978-3-540-45307-9},
	url = {http://link.springer.com/10.1007/3-540-45307-5_8},
	abstract = {The Message Passing Interface (MPI) has been extremely successful as a portable way to program high-performance parallel computers. This success has occurred in spite of the view of many that message passing is di cult and that other approaches, including automatic parallelization and directive-based parallelism, are easier to use. This paper argues that MPI has succeeded because it addresses all of the important issues in providing a parallel programming model.},
	language = {en},
	urldate = {2022-09-21},
	booktitle = {High {Performance} {Computing} — {HiPC} 2001},
	publisher = {Springer Berlin Heidelberg},
	author = {Gropp, William D.},
	editor = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Monien, Burkhard and Prasanna, Viktor K. and Vajapeyam, Sriram},
	year = {2001},
	doi = {10.1007/3-540-45307-5_8},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {81--92},
	file = {Gropp - 2001 - Learning from the Success of MPI.pdf:/local/home/ag262995/Zotero/storage/MX4ZLGVD/Gropp - 2001 - Learning from the Success of MPI.pdf:application/pdf},
}

@book{traff_recent_2012,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Recent {Advances} in the {Message} {Passing} {Interface}: 19th {European} {MPI} {Users}’ {Group} {Meeting}, {EuroMPI} 2012, {Vienna}, {Austria}, {September} 23-26, 2012. {Proceedings}},
	volume = {7490},
	isbn = {978-3-642-33517-4 978-3-642-33518-1},
	shorttitle = {Recent {Advances} in the {Message} {Passing} {Interface}},
	url = {http://link.springer.com/10.1007/978-3-642-33518-1},
	language = {en},
	urldate = {2022-09-21},
	publisher = {Springer Berlin Heidelberg},
	editor = {Träff, Jesper Larsson and Benkner, Siegfried and Dongarra, Jack J. and Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
	year = {2012},
	doi = {10.1007/978-3-642-33518-1},
	file = {Träff et al. - 2012 - Recent Advances in the Message Passing Interface .pdf:/local/home/ag262995/Zotero/storage/YISDFM83/Träff et al. - 2012 - Recent Advances in the Message Passing Interface .pdf:application/pdf},
}

@article{ahrens_paraview_2005,
	title = {{ParaView}: {An} {End}-{User} {Tool} for {Large} {Data} {Visualization}},
	shorttitle = {{ParaView}},
	journal = {Visualization Handbook},
	author = {Ahrens, J. and Geveci, Berk and Law, Charles},
	month = jan,
	year = {2005},
	file = {Full Text PDF:/local/home/ag262995/Zotero/storage/BZEIQCFG/Ahrens et al. - 2005 - ParaView An End-User Tool for Large Data Visualiz.pdf:application/pdf}
}

@misc{using_ucxdask,
	title = {Using with {Dask} — ucx-py 0.20.0a+32.g46e361b.dirty documentation},
	url = {https://ucx-py.readthedocs.io/en/latest/dask.html},
	urldate = {2021-05-19},
	file = {Using with Dask — ucx-py 0.20.0a+32.g46e361b.dirty documentation:/local/home/ag262995/Zotero/storage/6QB94YJE/dask.html:text/html}
}

@misc{noauthor_dask_chunk,
	title = {Dask documentation - {Chunks}},
	url = {array-chunks.html},
	abstract = {Chunks},
	language = {en},
	urldate = {2021-05-19},
	journal = {Dask  documentation},
	file = {Snapshot:/local/home/ag262995/Zotero/storage/H78KLE52/array-chunks.html:text/html}
}

@misc{noauthor_openucx,
	title = {{OpenUCX} — {OpenUCX} documentation},
	url = {https://openucx.readthedocs.io/en/master/},
	urldate = {2021-05-19},
	file = {OpenUCX — OpenUCX documentation:/local/home/ag262995/Zotero/storage/2SAGC4W6/master.html:text/html}
}


@inproceedings{barroso_benchmarking_2016_omnipath,
	address = {Padova, Italy},
	title = {Benchmarking message queue libraries and network technologies to transport large data volume in the {ALICE} {O} system},
	isbn = {978-1-5090-2014-0},
	url = {http://ieeexplore.ieee.org/document/7543162/},
	doi = {10.1109/RTC.2016.7543162},
	abstract = {ALICE (A Large Ion Collider Experiment) is the heavy-ion detector designed to study the physics of strongly interacting matter and the quark-gluon plasma at the CERN LHC (Large Hadron Collider).},
	language = {en},
	urldate = {2021-05-19},
	booktitle = {2016 {IEEE}-{NPSS} {Real} {Time} {Conference} ({RT})},
	publisher = {IEEE},
	author = {Barroso, V. Chibante and Fuchs, U. and Wegrzynek, A.},
	month = jun,
	year = {2016},
	pages = {1--5},
	file = {Barroso et al. - 2016 - Benchmarking message queue libraries and network t.pdf:/local/home/ag262995/Zotero/storage/ZDZD5ZZM/Barroso et al. - 2016 - Benchmarking message queue libraries and network t.pdf:application/pdf}
}

@misc{noauthor_pdi_nodate,
	title = {{PDI}: {Core} {Concepts}},
	url = {https://pdi.dev/master/Concepts.html},
	urldate = {2022-09-21},
	file = {PDI\: Core Concepts:/local/home/ag262995/Zotero/storage/QMNIVU5B/Concepts.html:text/html},
}

@unpublished{roussel:hal-01587075,
  TITLE = {{PDI, an approach to decouple I/O concerns from high-performance simulation codes}},
  AUTHOR = {Roussel, Corentin and Keller, Kai and Gaalich, Mohamed and Bautista Gomez, Leonardo and Bigot, Julien},
  URL = {https://hal.archives-ouvertes.fr/hal-01587075},
  NOTE = {working paper or preprint},
  YEAR = {2017},
  MONTH = Sep,
  KEYWORDS = {I/O ; Separation of concern ; HPC ; Checkpointing ; Software- engineering},
  PDF = {https://hal.archives-ouvertes.fr/hal-01587075/file/paper.pdf},
  HAL_ID = {hal-01587075},
  HAL_VERSION = {v1},
}

@article{grandgirard20165d,
  title={A 5D gyrokinetic full-f global semi-Lagrangian code for flux-driven ion turbulence simulations},
  author={Grandgirard, Virginie and Abiteboul, J{\'e}r{\'e}mie and Bigot, Julien and Cartier-Michaud, Thomas and Crouseilles, Nicolas and Dif-Pradalier, Guilhem and Ehrlacher, Ch and Esteve, Damien and Garbet, Xavier and Ghendrih, Ph and others},
  journal={Computer Physics Communications},
  volume={207},
  pages={35--68},
  year={2016},
  publisher={Elsevier}
}
@INPROCEEDINGS{arrayUDF-SC2018,
  author={X. {Xing} and B. {Dong} and J. {Ajo-Franklin} and K. {Wu}},
  booktitle={2018 IEEE/ACM Machine Learning in HPC Environments (MLHPC)}, 
  title={Automated Parallel Data Processing Engine with Application to Large-Scale Feature Extraction}, 
  year={2018},
  volume={},
  number={},
  pages={37-46},
  doi={10.1109/MLHPC.2018.8638638}}

@inproceedings{Heirich-isav2017,
 author = {A. Heirich and E. Slaughter and M. Papadakis and W. Lee  and T. Biedert and A. Aiken},
 title = {In Situ Visualization with Task-based Parallelism},
 booktitle = {Workshop on In Situ Infrastructures on Enabling Extreme-Scale Analysis and Visualization (ISAV'17)},
 series = {ISAV'17},
 year = {2017},
 isbn = {978-1-4503-5139-3},
 location = {Denver, CO, USA},
 pages = {17--21},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/3144769.3144771},
 doi = {10.1145/3144769.3144771},
 acmid = {3144771},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Legion, exascale, image compositor, in situ visualization, sort-last, task-based},
} 

@inproceedings{zheng2013goldrush,
 author = {Zheng, Fang and Yu, Hongfeng and Hantas, Can and Wolf, Matthew and Eisenhauer, Greg and Schwan, Karsten and Abbasi, Hasan and Klasky, Scott},
 title = {{G}old{R}ush: {R}esource {E}fficient in {S}itu {S}cientific {D}ata {A}nalytics {U}sing {F}ine-grained {I}nterference {A}ware {E}xecution},
 booktitle = {Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},
 series = {SC '13},
 year = {2013},
 isbn = {978-1-4503-2378-9},
 location = {Denver, Colorado},
 pages = {78:1--78:12},
 articleno = {78},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/2503210.2503279},
 doi = {10.1145/2503210.2503279},
 acmid = {2503279},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{slaughter_pygion_2019,
	address = {Denver, CO, USA},
	title = {Pygion: {Flexible}, {Scalable} {Task}-{Based} {Parallelism} with {Python}},
	isbn = {978-1-72815-979-9},
	shorttitle = {Pygion},
	url = {https://ieeexplore.ieee.org/document/9062721/},
	doi = {10.1109/PAW-ATM49560.2019.00011},
	language = {en},
	urldate = {2021-05-24},
	booktitle = {2019 {IEEE}/{ACM} {Parallel} {Applications} {Workshop}, {Alternatives} {To} {MPI} ({PAW}-{ATM})},
	publisher = {IEEE},
	author = {Slaughter, Elliott and Aiken, Alex},
	month = nov,
	year = {2019},
	pages = {58--72},
	file = {Slaughter and Aiken - 2019 - Pygion Flexible, Scalable Task-Based Parallelism .pdf:/local/home/ag262995/Zotero/storage/NNV83MGT/Slaughter and Aiken - 2019 - Pygion Flexible, Scalable Task-Based Parallelism .pdf:application/pdf}
}


@article{InSituLiuMa:2007,
  author={Kwan-Liu Ma and Chaoli Wang and Hongfeng Yu and Anna Tikhonova},
  title={In-situ processing and visualization for ultrascale simulations},
  journal={Journal of Physics: Conference Series},
  volume={78},
  number={1},
  pages={012043},
  url={http://stacks.iop.org/1742-6596/78/i=1/a=012043},
  year={2007},
  abstract={The growing power of parallel supercomputers gives scientists the ability to simulate more complex problems at higher fidelity, leading to many high-impact scientific advances. To maximize the utilization of the vast amount of data generated by these simulations, scientists also need scalable solutions for studying their data to different extents and at different abstraction levels. As we move into peta- and exa-scale computing, simply dumping as much raw simulation data as the storage capacity allows for post-processing analysis and visualization is no longer a viable approach. A common practice is to use a separate parallel computer to prepare data for subsequent analysis and visualization. A naive realization of this strategy not only limits the amount of data that can be saved, but also turns I/O into a performance bottleneck when using a large parallel system. We conjecture that the most plausible solution for the peta- and exa-scale data problem is to reduce or transform the data in-situ as it is being generated, so the amount of data that must be transferred over the network is kept to a minimum. In this paper, we discuss different approaches to in-situ processing and visualization as well as the results of our preliminary study using large-scale simulation codes on massively parallel supercomputers.}
}


@article{godoy_adios2_2020,
	title = {{ADIOS} 2: {The} {Adaptable} {Input} {Output} {System}. {A} framework for high-performance data management},
	volume = {12},
	issn = {2352-7110},
	shorttitle = {{ADIOS} 2},
	url = {https://www.sciencedirect.com/science/article/pii/S2352711019302560},
	doi = {10.1016/j.softx.2020.100561},
	abstract = {We present ADIOS 2, the latest version of the Adaptable Input Output (I/O) System. ADIOS 2 addresses scientific data management needs ranging from scalable I/O in supercomputers, to data analysis in personal computer and cloud systems. Version 2 introduces a unified application programming interface (API) that enables seamless data movement through files, wide-area-networks, and direct memory access, as well as high-level APIs for data analysis. The internal architecture provides a set of reusable and extendable components for managing data presentation and transport mechanisms for new applications. ADIOS 2 bindings are available in C++11, C, Fortran, Python, and Matlab and are currently used across different scientific communities. ADIOS 2 provides a communal framework to tackle data management challenges as we approach the exascale era of supercomputing.},
	language = {en},
	urldate = {2021-02-10},
	journal = {SoftwareX},
	author = {Godoy, William F. and Podhorszki, Norbert and Wang, Ruonan and Atkins, Chuck and Eisenhauer, Greg and Gu, Junmin and Davis, Philip and Choi, Jong and Germaschewski, Kai and Huck, Kevin and Huebl, Axel and Kim, Mark and Kress, James and Kurc, Tahsin and Liu, Qing and Logan, Jeremy and Mehta, Kshitij and Ostrouchov, George and Parashar, Manish and Poeschel, Franz and Pugmire, David and Suchyta, Eric and Takahashi, Keichi and Thompson, Nick and Tsutsumi, Seiji and Wan, Lipeng and Wolf, Matthew and Wu, Kesheng and Klasky, Scott},
	month = jul,
	year = {2020},
	keywords = {Data science, Exascale computing, High-performance computing (HPC), In-situ, Luster GPFS file systems, RDMA, Scalable I/O, Staging},
	pages = {100561},
	file = {ScienceDirect Full Text PDF:/local/home/ag262995/Zotero/storage/C6XWMHXF/Godoy et al. - 2020 - ADIOS 2 The Adaptable Input Output System. A fram.pdf:application/pdf;ScienceDirect Snapshot:/local/home/ag262995/Zotero/storage/SQXQ7VTN/S2352711019302560.html:text/html}
}




@inproceedings{libsim11,
 author = {Whitlock, Brad and Favre, Jean M. and Meredith, Jeremy S.},
 title = {{P}arallel {I}n {S}itu {C}oupling of {S}imulation with a {F}ully {F}eatured {V}isualization {S}ystem},
 booktitle = {11th Eurographics conference on Parallel Graphics and Visualization},
 OPTseries = {EG PGV'11},
 year = {2011},
 isbn = {978-3-905674-32-3},
 location = {Llandudno},
 pages = {101--109},
 numpages = {9},
 OPTurl = {http://dx.doi.org/10.2312/EGPGV/EGPGV11/101-109},
 OPTdoi = {10.2312/EGPGV/EGPGV11/101-109},
 acmid = {2386245},
 OPTpublisher = {Eurographics Association},
 OPTaddress = {Aire-la-Ville, Switzerland, Switzerland},
}


@INPROCEEDINGS{catalyst11, 
author={Fabian, N. and Moreland, K. and Thompson, D. and Bauer, A.C. and Marion, P. and Geveci, B. and Rasquin, M. and Jansen, K.E.}, 
booktitle={Large Data Analysis and Visualization Workshop(LDAV'11)}, 
title={{T}he {P}araView {C}oprocessing {L}ibrary: a {S}calable, {G}eneral {P}urpose {I}n {S}itu {V}isualization {L}ibrary}, 
year={2011}, 
OPTmonth={Oct}, 
pages={89-96}, 
keywords={data analysis;data compression;data visualisation;feature extraction;multiprocessing systems;open systems;software libraries;CPU capability;ParaView coprocessing library;data compression;disk write speed;high performance computing;in situ visualization library;interactive post-processing;library scalability;readable data extraction;scientific simulation;Adaptation models;Data mining;Data models;Data visualization;Libraries;Pipelines;Sockets;coprocessing;in situ;scaling;simulation}, 
OPTdoi={10.1109/LDAV.2011.6092322},}


@inproceedings{Larsen-alpine-isav17,
author = {Larsen, Matthew and Ahrens, James and Ayachit, Utkarsh and Brugger, Eric and Childs, Hank and Geveci, Berk and Harrison, Cyrus},
title = {The ALPINE In Situ Infrastructure: Ascending from the Ashes of Strawman},
year = {2017},
isbn = {9781450351393},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3144769.3144778},
doi = {10.1145/3144769.3144778},
abstract = {This paper introduces ALPINE, a flyweight in situ infrastructure. The infrastructure is designed for leading-edge supercomputers, and has support for both distributed-memory and shared-memory parallelism. It can take advantage of computing power on both conventional CPU architectures and on many-core architectures such as NVIDIA GPUs or the Intel Xeon Phi. Further, it has a flexible design that supports for integration of new visualization and analysis routines and libraries. The paper describes ALPINE's interface choices and architecture, and also reports on initial experiments performed using the infrastructure.},
booktitle = {Proceedings of the In Situ Infrastructures on Enabling Extreme-Scale Analysis and Visualization},
pages = {42–46},
numpages = {5},
keywords = {HPC, Scientific Visualization, In Situ},
location = {Denver, CO, USA},
series = {ISAV'17}
}


@inproceedings{loring-pythoninsitu-isav2018,
author = {Loring, Burlen and Myers, Andrew and Camp, David and Bethel, E. Wes},
title = {Python-Based in Situ Analysis and Visualization},
year = {2018},
isbn = {9781450365796},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281464.3281465},
doi = {10.1145/3281464.3281465},
abstract = {This work focuses on enabling the use of Python-based methods for the purpose of performing in situ analysis and visualization. This approach facilitates access to and use of a rapidly growing collection of Python-based, third-party libraries for analysis and visualization, as well as lowering the barrier to entry for user-written Python analysis codes. Beginning with a simulation code that is instrumented to use the SENSEI in situ interface, we present how to couple it with a Python-based data consumer, which may be run in situ, and in parallel at the same concurrency as the simulation. We present two examples that demonstrate the new capability. One is an analysis of the reaction rate in a proxy simulation of a chemical reaction on a 2D substrate, while the other is a coupling of an AMR simulation to Yt, a parallel visualization and analysis library written in Python. In the examples, both the simulation and Python in situ method run in parallel on a large-scale HPC platform.},
booktitle = {Proceedings of the Workshop on In Situ Infrastructures for Enabling Extreme-Scale Analysis and Visualization},
pages = {19–24},
numpages = {6},
keywords = {Python, in situ visualization, in situ analysis},
location = {Dallas, Texas, USA},
series = {ISAV '18}
}



@article{schroeder_visualizing_2000_vtk,
	title = {Visualizing with {VTK}: a tutorial},
	volume = {20},
	issn = {1558-1756},
	shorttitle = {Visualizing with {VTK}},
	doi = {10.1109/38.865875},
	abstract = {We introduce basic concepts behind the Visualization Toolkit (VTK). An overview of the system, plus some detailed examples, will assist in learning this system. The tutorial targets researchers of any discipline who have 2D or 3D data and want more control over the visualization process than a turn-key system can provide. It also assists developers who would like to incorporate VTK into an application as a visualization or data processing engine.},
	number = {5},
	journal = {IEEE Computer Graphics and Applications},
	author = {Schroeder, W. J. and Avila, L. S. and Hoffman, W.},
	month = sep,
	year = {2000},
	note = {Conference Name: IEEE Computer Graphics and Applications},
	keywords = {2D data, 3D data, Business, Cameras, Companies, Computer graphics, data processing engine, Data visualization, Linux, Open source software, Programming, Rendering (computer graphics), Tutorial, Visualization, visualization engine, Visualization Toolkit},
	pages = {20--27},
	file = {IEEE Xplore Full Text PDF:/local/home/ag262995/Zotero/storage/9DT5RHBB/Schroeder et al. - 2000 - Visualizing with VTK a tutorial.pdf:application/pdf;IEEE Xplore Abstract Record:/local/home/ag262995/Zotero/storage/MHR8EU4K/865875.html:text/html}
}

@article{moreland_vtk-m_2016,
	title = {{VTK}-m: {Accelerating} the {Visualization} {Toolkit} for {Massively} {Threaded} {Architectures}},
	volume = {36},
	issn = {1558-1756},
	shorttitle = {{VTK}-m},
	doi = {10.1109/MCG.2016.48},
	abstract = {One of the most critical challenges for high-performance computing (HPC) scientific visualization is execution on massively threaded processors. Of the many fundamental changes we are seeing in HPC systems, one of the most profound is a reliance on new processor types optimized for execution bandwidth over latency hiding. Our current production scientific visualization software is not designed for these new types of architectures. To address this issue, the VTK-m framework serves as a container for algorithms, provides flexible data representation, and simplifies the design of visualization algorithms on new and future computer architecture.},
	number = {3},
	journal = {IEEE Computer Graphics and Applications},
	author = {Moreland, K. and Sewell, C. and Usher, W. and Lo, L. and Meredith, J. and Pugmire, D. and Kress, J. and Schroots, H. and Ma, K. and Childs, H. and Larsen, M. and Chen, C. and Maynard, R. and Geveci, B.},
	month = may,
	year = {2016},
	note = {Conference Name: IEEE Computer Graphics and Applications},
	keywords = {Algorithm design and analysis, algorithmic structures, Computational modeling, computer architecture, Computer architecture, computer graphics, data structures, data visualisation, Data visualization, execution bandwidth over latency hiding, flexible data representation, high-performance computing, high-performance computing scientific visualization, HPC scientific visualization, HPC systems, massively threaded architectures, massively threaded processors, Message systems, multi-threading, parallel algorithms, parallel processing, Parallel processing, scientific visualization software, Software engineering, visualization software, visualization toolkit, VTK-m framework},
	pages = {48--58},
	file = {IEEE Xplore Full Text PDF:/local/home/ag262995/Zotero/storage/422WW42Z/Moreland et al. - 2016 - VTK-m Accelerating the Visualization Toolkit for .pdf:application/pdf;IEEE Xplore Abstract Record:/local/home/ag262995/Zotero/storage/ZLHUXE8Y/7466740.html:text/html}
}

@inproceedings {222605_ray,
author = {Philipp Moritz and Robert Nishihara and Stephanie Wang and Alexey Tumanov and Richard Liaw and Eric Liang and Melih Elibol and Zongheng Yang and William Paul and Michael I. Jordan and Ion Stoica},
title = {Ray: A Distributed Framework for Emerging {AI} Applications},
booktitle = {13th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 18)},
year = {2018},
isbn = {978-1-939133-08-3},
address = {Carlsbad, CA},
pages = {561--577},
url = {https://www.usenix.org/conference/osdi18/presentation/moritz},
publisher = {{USENIX} Association},
month = oct,
}


@inproceedings{rocklin_dask_2015,
	address = {Austin, Texas},
	title = {Dask: {Parallel} {Computation} with {Blocked} algorithms and {Task} {Scheduling}},
	shorttitle = {Dask},
	url = {https://conference.scipy.org/proceedings/scipy2015/matthew_rocklin.html},
	doi = {10.25080/Majora-7b98e3ed-013},
	abstract = {Dask enables parallel and out-of-core computation. We couple blocked algorithms with dynamic and memory aware task scheduling to achieve a parallel and out-of-core NumPy clone. We show how this extends the effective scale of modern hardware to larger datasets and discuss how these ideas can be more broadly applied to other parallel collections.},
	language = {en},
	urldate = {2020-12-27},
	author = {Rocklin, Matthew},
	year = {2015},
	pages = {126--132},
	file = {Rocklin - 2015 - Dask Parallel Computation with Blocked algorithms.pdf:/local/home/ag262995/Zotero/storage/E6FCJHJE/Rocklin - 2015 - Dask Parallel Computation with Blocked algorithms.pdf:application/pdf}
}
@inproceedings{ayachit_sensei_2016,
	address = {Salt Lake City, UT, USA},
	title = {The {SENSEI} {Generic} {In} {Situ} {Interface}},
	isbn = {978-1-5090-3872-5},
	url = {http://ieeexplore.ieee.org/document/7836400/},
	doi = {10.1109/ISAV.2016.013},
	abstract = {The SENSEI generic in situ interface is an API that promotes code portability and reusability. From the simulation view, a developer can instrument their code with the SENSEI API and then make make use of any number of in situ infrastructures. From the method view, a developer can write an in situ method using the SENSEI API, then expect it to run in any number of in situ infrastructures, or be invoked directly from a simulation code, with little or no modiﬁcation. This paper presents the design principles underlying the SENSEI generic interface, along with some simpliﬁed coding examples.},
	language = {en},
	urldate = {2020-12-27},
	booktitle = {2016 {Second} {Workshop} on {In} {Situ} {Infrastructures} for {Enabling} {Extreme}-{Scale} {Analysis} and {Visualization} ({ISAV})},
	publisher = {IEEE},
	author = {Ayachit, Utkarsh and Whitlock, Brad and Wolf, Matthew and Loring, Burlen and Geveci, Berk and Lonie, David and Bethel, E. Wes},
	month = nov,
	year = {2016},
	pages = {40--44},
	file = {Ayachit et al. - 2016 - The SENSEI Generic In Situ Interface.pdf:/local/home/ag262995/Zotero/storage/V6HXT3T9/Ayachit et al. - 2016 - The SENSEI Generic In Situ Interface.pdf:application/pdf}
}

@article{hanwell_visualization_2015_vtk,
	title = {The {Visualization} {Toolkit} ({VTK}): {Rewriting} the rendering code for modern graphics cards},
	volume = {1-2},
	issn = {2352-7110},
	shorttitle = {The {Visualization} {Toolkit} ({VTK})},
	url = {http://www.sciencedirect.com/science/article/pii/S2352711015000035},
	doi = {10.1016/j.softx.2015.04.001},
	abstract = {The Visualization Toolkit (VTK) is an open source, permissively licensed, cross-platform toolkit for scientific data processing, visualization, and data analysis. It is over two decades old, originally developed for a very different graphics card architecture. Modern graphics cards feature fully programmable, highly parallelized architectures with large core counts. VTK’s rendering code was rewritten to take advantage of modern graphics cards, maintaining most of the toolkit’s programming interfaces. This offers the opportunity to compare the performance of old and new rendering code on the same systems/cards. Significant improvements in rendering speeds and memory footprints mean that scientific data can be visualized in greater detail than ever before. The widespread use of VTK means that these improvements will reap significant benefits.},
	language = {en},
	urldate = {2020-12-27},
	journal = {SoftwareX},
	author = {Hanwell, Marcus D. and Martin, Kenneth M. and Chaudhary, Aashish and Avila, Lisa S.},
	month = sep,
	year = {2015},
	keywords = {Data analysis, Scientific data, Toolkit, Visualization},
	pages = {9--12},
	file = {ScienceDirect Full Text PDF:/local/home/ag262995/Zotero/storage/FXN2XDZE/Hanwell et al. - 2015 - The Visualization Toolkit (VTK) Rewriting the ren.pdf:application/pdf;ScienceDirect Snapshot:/local/home/ag262995/Zotero/storage/V76AF729/S2352711015000035.html:text/html}
}
@article{childs_visit_nodate,
	title = {{VisIt}: {An} {End}-{User} {Tool} for {Visualizing} and {Analyzing} {Very} {Large} {Data}},
	language = {en},
	author = {Childs, Hank},
	pages = {17},
	file = {Childs - VisIt An End-User Tool for Visualizing and Analyz.pdf:/local/home/ag262995/Zotero/storage/ENAG3GC3/Childs - VisIt An End-User Tool for Visualizing and Analyz.pdf:application/pdf}
}

@inproceedings{dorier_damaris_2012,
	address = {Beijing, China},
	title = {Damaris: {How} to {Efficiently} {Leverage} {Multicore} {Parallelism} to {Achieve} {Scalable}, {Jitter}-free {I}/{O}},
	shorttitle = {Damaris},
	url = {https://hal.inria.fr/hal-00715252},
	abstract = {With exascale computing on the horizon, the performance variability of I/O systems represents a key challenge in sustaining high performance. In many HPC applications, I/O is concurrently performed by all processes, which leads to I/O bursts. This causes resource contention and substantial variability of I/O performance, which significantly impacts the overall application performance and, most importantly, its predictability over time. In this paper, we propose a new approach to I/O, called Damaris, which leverages dedicated I/O cores on each multicore SMP node, along with the use of shared-memory, to efficiently perform asynchronous data processing and I/O in order to hide this variability. We evaluate our approach on three different platforms including the Kraken Cray XT5 supercomputer (ranked 11th in Top500), with the CM1 atmospheric model, one of the target HPC applications for the Blue Waters postpetascale supercomputer project. By overlapping I/O with computation and by gathering data into large files while avoiding synchronization between cores, our solution brings several benefits: 1) it fully hides jitter as well as all I/O-related costs, which makes simulation performance predictable; 2) it increases the sustained write throughput by a factor of 15 compared to standard approaches; 3) it allows almost perfect scalability of the simulation up to over 9,000 cores, as opposed to state-of-the-art approaches which fail to scale; 4) it enables a 600{\textbackslash}\% compression ratio without any additional overhead, leading to a major reduction of storage requirements.},
	urldate = {2020-09-24},
	booktitle = {{CLUSTER} 2012 - {IEEE} {International} {Conference} on {Cluster} {Computing}},
	publisher = {IEEE},
	author = {Dorier, Matthieu and Antoniu, Gabriel and Cappello, Franck and Snir, Marc and Orf, Leigh},
	month = sep,
	year = {2012},
	file = {HAL PDF Full Text:/local/home/ag262995/Zotero/storage/9B6Y3GPJ/Dorier et al. - 2012 - Damaris How to Efficiently Leverage Multicore Par.pdf:application/pdf}
}

@phdthesis{dorier_addressing_2014,
	type = {phdthesis},
	title = {Addressing the {Challenges} of {I}/{O} {Variability} in {Post}-{Petascale} {HPC} {Simulations}},
	url = {https://tel.archives-ouvertes.fr/tel-01099105},
	abstract = {Million-core supercomputers have become a reality in 2012 with LLNL's Sequoia supercomputer. Following Moore's law, Exascale machines (capable of 10E18 floating point operations per second) are expected by 2018. Such an immense computational power is used in many research areas, including earth sciences, biology, climate, or cosmology, where large-scale simulations are conducted to understand physical phenomena better. These simulations aim to replace real experiments that are either too expensive, irreproducible or simply unfeasible. But larger simulations on larger machines lead to the production of larger amounts of data. These data need to be efficiently stored and processed in order to retrieve scientific insights. The traditional approach to data management consists of storing the output of the simulation during its run, move it and analyze it later offline. With an increasing gap between the performance of storage systems and the computation capabilities of recent post-Petascale supercomputers, this approach becomes unsustainable. This Ph.D. thesis explores new approaches to data management for post-Petascale supercomputers. We first introduce the Damaris approach, which leverages the multicore nature of recent machines to offload data-management tasks into dedicated cores. We study in particular how Damaris can be used to hide the variability in I/O (Input/Output) performance, and to provide in situ visualization capabilities to simulations in a way that does not impact their performance. We then use Damaris to evaluate the energy consumption of various data management approaches, including the use of dedicated I/O nodes. We then study the effect of multi-application I/O contention on the performance of the storage system. We propose the CALCioM approach, which provides a coordination layer between distinct applications to mitigate I/O interference. In regard to access patterns, it has been observed that most applications have a repetitive behavior with respect to I/O, and that a model of this behavior can be useful to many systems (including CALCioM, but also any scheduler, caching or prefetching system). Based on this, we propose Omnisc'IO, an approach that leverages grammars to predict the spatial and temporal access patterns of HPC simulations at run time. This thesis includes results of experiments conducted with real scientific simulations, including CM1, GTC, LAMMPS and Nek5000, on real petascale and post-petascale platforms, including NCSA's Blue Waters, ORNL's Titan, NICS's Kraken and ANL's Intrepid.},
	language = {en},
	urldate = {2020-09-24},
	school = {Ecole Normale Supérieure de Rennes},
	author = {Dorier, Matthieu},
	month = dec,
	year = {2014},
	file = {Full Text PDF:/local/home/ag262995/Zotero/storage/MCA8W8PJ/Dorier - 2014 - Addressing the Challenges of IO Variability in Po.pdf:application/pdf;Snapshot:/local/home/ag262995/Zotero/storage/LA57TXFT/tel-01099105.html:text/html}
}

@article{mckinney_python_nodate,
	title = {Python for {Data} {Analysis}},
	language = {en},
	author = {McKinney, Wes},
	pages = {541},
	file = {McKinney - Python for Data Analysis.pdf:/local/home/ag262995/Zotero/storage/BZMEMBLB/McKinney - Python for Data Analysis.pdf:application/pdf}
}

@article{docan_dataspaces_2012,
	title = {{DataSpaces}: an interaction and coordination framework for coupled simulation workflows},
	volume = {15},
	issn = {1573-7543},
	shorttitle = {{DataSpaces}},
	url = {https://doi.org/10.1007/s10586-011-0162-y},
	doi = {10.1007/s10586-011-0162-y},
	abstract = {Emerging high-performance distributed computing environments are enabling new end-to-end formulations in science and engineering that involve multiple interacting processes and data-intensive application workflows. For example, current fusion simulation efforts are exploring coupled models and codes that simultaneously simulate separate application processes, such as the core and the edge turbulence. These components run on different high performance computing resources, need to interact at runtime with each other and with services for data monitoring, data analysis and visualization, and data archiving. As a result, they require efficient and scalable support for dynamic and flexible couplings and interactions, which remains a challenge. This paper presents DataSpaces a flexible interaction and coordination substrate that addresses this challenge. DataSpaces essentially implements a semantically specialized virtual shared space abstraction that can be associatively accessed by all components and services in the application workflow. It enables live data to be extracted from running simulation components, indexes this data online, and then allows it to be monitored, queried and accessed by other components and services via the space using semantically meaningful operators. The underlying data transport is asynchronous, low-overhead and largely memory-to-memory. The design, implementation, and experimental evaluation of DataSpaces using a coupled fusion simulation workflow is presented.},
	language = {en},
	number = {2},
	urldate = {2020-09-24},
	journal = {Cluster Computing},
	author = {Docan, Ciprian and Parashar, Manish and Klasky, Scott},
	month = jun,
	year = {2012},
	pages = {163--181},
	file = {Springer Full Text PDF:/local/home/ag262995/Zotero/storage/F4NK8XIE/Docan et al. - 2012 - DataSpaces an interaction and coordination framew.pdf:application/pdf}
}

@inproceedings{romanus_persistent_2016,
	address = {New York, NY, USA},
	series = {{DIDC} '16},
	title = {Persistent {Data} {Staging} {Services} for {Data} {Intensive} {In}-situ {Scientific} {Workflows}},
	isbn = {978-1-4503-4352-7},
	url = {https://doi.org/10.1145/2912152.2912157},
	doi = {10.1145/2912152.2912157},
	abstract = {Scientific simulation workflows executing on very large scale computing systems are essential modalities for scientific investigation. The increasing scales and resolution of these simulations provide new opportunities for accurately modeling complex natural and engineered phenomena. However, the increasing complexity necessitates managing, transporting, and processing unprecedented amounts of data, and as a result, researchers are increasingly exploring data-staging and in-situ workflows to reduce data movement and data-related overheads. However, as these workflows become more dynamic in their structures and behaviors, data staging and in-situ solutions must evolve to support new requirements. In this paper, we explore how the service-oriented concept can be applied to extreme-scale in-situ workflows. Specifically, we explore persistent data staging as a service and present the design and implementation of DataSpaces as a Service, a service-oriented data staging framework. We use a dynamically coupled fusion simulation workflow to illustrate the capabilities of this framework and evaluate its performance and scalability.},
	urldate = {2020-09-24},
	booktitle = {Proceedings of the {ACM} {International} {Workshop} on {Data}-{Intensive} {Distributed} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Romanus, Melissa and Zhang, Fan and Jin, Tong and Sun, Qian and Bui, Hoang and Parashar, Manish and Choi, Jong and Janhunen, Saloman and Hager, Robert and Klasky, Scott and Chang, Choong-Seock and Rodero, Ivan},
	month = jun,
	year = {2016},
	keywords = {as a service, coupled workflows, data management, data staging, dynamic workflow, in-memory, in-situ, node-local, scientific workflows},
	pages = {37--44},
	file = {Full Text PDF:/local/home/ag262995/Zotero/storage/ZG9CR3JY/Romanus et al. - 2016 - Persistent Data Staging Services for Data Intensiv.pdf:application/pdf}
}

@techreport{peterka_ascr_2019,
	title = {{ASCR} {Workshop} on {In} {Situ} {Data} {Management}: {Enabling} {Scientific} {Discovery} from {Diverse} {Data} {Sources}},
	shorttitle = {{ASCR} {Workshop} on {In} {Situ} {Data} {Management}},
	url = {https://www.osti.gov/biblio/1493245},
	abstract = {The U.S. Department of Energy's Office of Scientific and Technical Information},
	language = {English},
	urldate = {2020-09-24},
	institution = {USDOE Office of Science (SC) (United States)},
	author = {Peterka, Tom and Bard, Deborah and Bennett, Janine and Bethel, E. Wes and Oldfield, Ron and Pouchard, Line and Sweeney, Christine and Wolf, Matthew},
	month = feb,
	year = {2019},
	doi = {10.2172/1493245},
	file = {Full Text PDF:/local/home/ag262995/Zotero/storage/2LDDSGC4/Peterka et al. - 2019 - ASCR Workshop on In Situ Data Management Enabling.pdf:application/pdf;Snapshot:/local/home/ag262995/Zotero/storage/KRXI9VWP/1493245.html:text/html}
}

@article{liu_hello_2014_adios,
	title = {Hello {ADIOS}: the challenges and lessons of developing leadership class {I}/{O} frameworks: {HELLO} {ADIOS}},
	volume = {26},
	issn = {15320626},
	shorttitle = {Hello {ADIOS}},
	url = {http://doi.wiley.com/10.1002/cpe.3125},
	doi = {10.1002/cpe.3125},
	abstract = {Applications running on leadership platforms are more and more bottlenecked by storage I/O. In an effort to combat the increasing disparity between I/O throughput and compute capability, we created ADIOS in 2005. Focusing on putting users ﬁrst with a Service Oriented Architecture, we combined cutting edge research into new I/O techniques with a design effort to create near optimal I/O methods. As a result, ADIOS provides the highest level of synchronous I/O performance for a number of mission critical applications at various DOE Leadership Computing Facilities. Meanwhile ADIOS is leading the push for next generation techniques including staging and data processing pipelines. In this paper we describe the startling observations we have made in the last half decade of I/O research and development, and elaborate the lessons we have learned along this journey. We also detail some of the challenges that remain as we look towards the coming Exascale era. Copyright c 0000 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {7},
	urldate = {2020-09-24},
	journal = {Concurrency and Computation: Practice and Experience},
	author = {Liu, Qing and Logan, Jeremy and Tian, Yuan and Abbasi, Hasan and Podhorszki, Norbert and Choi, Jong Youl and Klasky, Scott and Tchoua, Roselyne and Lofstead, Jay and Oldfield, Ron and Parashar, Manish and Samatova, Nagiza and Schwan, Karsten and Shoshani, Arie and Wolf, Matthew and Wu, Kesheng and Yu, Weikuan},
	month = may,
	year = {2014},
	pages = {1453--1473},
	file = {Liu et al. - 2014 - Hello ADIOS the challenges and lessons of develop.pdf:/local/home/ag262995/Zotero/storage/G99SMT4M/Liu et al. - 2014 - Hello ADIOS the challenges and lessons of develop.pdf:application/pdf}
}


@inproceedings{mommessin_automatic_2017,
	title = {Automatic {Data} {Filtering} for {In} {Situ} {Workflows}},
	doi = {10.1109/CLUSTER.2017.35},
	abstract = {In situ workflows contain tasks that exchange messages composed of several data fields. However, a consumer task may not necessarily need all the data fields from its producer. For example, a molecular dynamics simulation can produce atom positions, velocities, and forces; but some analyses require only atom positions. The user should decide whether to specialize the output of a producer task for a particular consumer and get better performance or to send more data than required by the consumer. The first option limits task portability, while the second wastes resources. In this paper, we introduce contracts for in situ tasks. A contract specifies for a producer each data field available for output and for a consumer the data fields needed as input. Comparing a producer and consumer contract allows automatic selection of the data fields a producer has to send for that consumer. We integrated our contracts mechanism within Decaf, a middleware for building and executing in situ workflows. Contracts enable to automatically extract at the producer the data the consumer needs. We evaluate the cost and performance of message extraction at runtime with both synthetic examples and a real scientific workflow coupling a molecular dynamics simulation with three different data analytics codes. Our contract-based automatic data extraction removes the need to specialize producers while entailing small overheads.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Cluster} {Computing} ({CLUSTER})},
	author = {Mommessin, C. and Dreher, M. and Raffin, B. and Peterka, T.},
	month = sep,
	year = {2017},
	note = {ISSN: 2168-9253},
	keywords = {Analytical models, atom positions, automatic data extraction, automatic data filtering, automatic selection, Communication channels, Computational modeling, Contracts, contracts mechanism, data fields, data handling, Data models, Decaf, exchange messages, information filtering, middleware, Middleware, molecular dynamics simulation, several data fields, situ workflows, workflow management software},
	pages = {370--378},
	file = {Submitted Version:/local/home/ag262995/Zotero/storage/ERY8ACYA/Mommessin et al. - 2017 - Automatic Data Filtering for In Situ Workflows.pdf:application/pdf;IEEE Xplore Abstract Record:/local/home/ag262995/Zotero/storage/MNK98V37/8048949.html:text/html}
}

@inproceedings{zhang_miqs_2019,
	address = {New York, NY, USA},
	series = {{SC} '19},
	title = {{MIQS}: metadata indexing and querying service for self-describing file formats},
	isbn = {978-1-4503-6229-0},
	shorttitle = {{MIQS}},
	url = {https://doi.org/10.1145/3295500.3356146},
	doi = {10.1145/3295500.3356146},
	abstract = {Scientific applications often store datasets in self-describing data file formats, such as HDF5 and netCDF. Regrettably, to efficiently search the metadata within these files remains challenging due to the sheer size of the datasets. Existing solutions extract the metadata and store it in external database management systems (DBMS) to locate desired data. However, this practice introduces significant overhead and complexity in extraction and querying. In this research, we propose a novel {\textless}u{\textgreater}M{\textless}/u{\textgreater}etadata {\textless}u{\textgreater}I{\textless}/u{\textgreater}ndexing and {\textless}u{\textgreater}Q{\textless}/u{\textgreater}uerying {\textless}u{\textgreater}S{\textless}/u{\textgreater}ervice (MIQS), which removes the external DBMS and utilizes in-memory index to achieve efficient metadata searching. MIQS follows the self-contained data management paradigm and provides portable and schema-free metadata indexing and querying functionalities for self-describing file formats. We have evaluated MIQS with the state-of-the-art MongoDB-based metadata indexing solution. MIQS achieved up to 99\% time reduction in index construction and up to 172kx search performance improvement with up to 75\% reduction in memory footprint.},
	urldate = {2020-09-24},
	booktitle = {Proceedings of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Wei and Byna, Suren and Tang, Houjun and Williams, Brody and Chen, Yong},
	month = nov,
	year = {2019},
	keywords = {HDF5 metadata management, metadata search},
	pages = {1--24},
	file = {Full Text PDF:/local/home/ag262995/Zotero/storage/FEWN5RKS/Zhang et al. - 2019 - MIQS metadata indexing and querying service for s.pdf:application/pdf}
}

@inproceedings{dreher_flexible_2014,
	title = {A {Flexible} {Framework} for {Asynchronous} {In} {Situ} and {In} {Transit} {Analytics} for {Scientific} {Simulations}},
	url = {https://hal.inria.fr/hal-00941413},
	abstract = {High performance computing systems are today composed of tens of thousands of processors and deep memory hierarchies. The next generation of machines will further increase the unbalance between I/O capabilities and processing power. To reduce the pressure on I/Os, the in situ analytics paradigm proposes to process the data as closely as possible to where and when the data are produced. Processing can be embedded in the simulation code, executed asynchronously on helper cores on the same nodes, or performed in transit on staging nodes dedicated to analytics. Today, software environ- nements as well as usage scenarios still need to be investigated before in situ analytics become a standard practice. In this paper we introduce a framework for designing, deploying and executing in situ scenarios. Based on a com- ponent model, the scientist designs analytics workflows by first developing processing components that are next assembled in a dataflow graph through a Python script. At runtime the graph is instantiated according to the execution context, the framework taking care of deploying the application on the target architecture and coordinating the analytics workflows with the simulation execution. Component coordination, zero- copy intra-node communications or inter-nodes data transfers rely on per-node distributed daemons. We evaluate various scenarios performing in situ and in transit analytics on large molecular dynamics systems sim- ulated with Gromacs using up to 1664 cores. We show in particular that analytics processing can be performed on the fraction of resources the simulation does not use well, resulting in a limited impact on the simulation performance (less than 6\%). Our more advanced scenario combines in situ and in transit processing to compute a molecular surface based on the Quicksurf algorithm.},
	language = {en},
	urldate = {2020-09-24},
	publisher = {IEEE Computer Science Press},
	author = {Dreher, Matthieu and Raffin, Bruno},
	month = may,
	year = {2014},
	file = {Full Text PDF:/local/home/ag262995/Zotero/storage/5W8JVW6X/Dreher and Raffin - 2014 - A Flexible Framework for Asynchronous In Situ and .pdf:application/pdf;Snapshot:/local/home/ag262995/Zotero/storage/NJV2NKK5/en.html:text/html}
}

@phdthesis{dreher_methodes_2015,
	type = {These de doctorat},
	title = {Méthodes {In}-{Situ} et {In}-{Transit} : vers un continuum entre les applications interactives et offines à grande échelle},
	copyright = {Licence Etalab},
	shorttitle = {Méthodes {In}-{Situ} et {In}-{Transit}},
	url = {https://www.theses.fr/2015GREAM076},
	abstract = {Les simulations paralllèles sont devenues des outils indispensables dans de nombreux domaines scientifiques. Pour simuler des phénomènes complexes, ces simulations sont exécutées sur de grandes machines parallèles. La puissance de calcul de ces machines n'a cessé de monter permettant ainsi le traitement de simulations de plus en plus imposantes. En revanche, les systèmes d'I/O nécessaires à la sauvegarde des données produites par les simulations ont suivit une croissance beaucoup plus faible. Actuellement déjà, il est difficile pour les scientifiques de sauvegarder l'ensemble des données désirées et d'avoir suffisament de puissance de calcul pour les analyser par la suite. A l'ère de l'Exascale, on estime que moins de 1\% des données produites par une simulation pourronts être sauvegardées. Ces données sont pourtant une des clés vers des découvertes scientifiques majeures. Les traitements in-situ sont une solution prometteuse à ce problème. Le principe est d'effectuer des analyses alors que la simulation est en cours d'exécution et que les données sont encore en mémoire. Cette approche permet d'une part d'éviter le goulot d'étranglement au niveau des I/O mais aussi de profiter de la puissance de calcul offerte par les machines parallèles pour effectuer des traitements lourds en calcul. Dans cette thèse, nous proposons d'utiliser le paradigme du dataflow pour permettre la construction d'applications in-situ complexes. Pour cela, nous utilisons l'intergiciel FlowVR permettant de coupler des codes parallèles hétérogènes en créant des canaux de communication entre eux afin de former un graphe. FlowVR dispose de suffisament de flexibilité pour permettre plusieurs stratégies de placement des processus d'analyses que cela soit sur les nœuds de la simulation, sur des cœurs dédiés ou des nœuds dédiés. De plus, les traitements in-situ peuvent être exécutés de manière asynchrone permettant ainsi un faible impact sur les performances de la simulation. Pour démontrer la flexibilité de notre approche, nous nous sommes intéressés au cas à la dynamique moléculaire et plus particulièrement Gromacs, un code de simulation de dynamique moléculaire couramment utilisé par les biologistes pouvant passer à l'échelle sur plusieurs milliers de coeurs. En étroite collaboration avec des experts du domaine biologique, nous avons contruit plusieurs applications. Notre première application consiste à permettre à un utilisateur de guider une simulation de dynamique moléculaire vers une configuration souhaitée. Pour cela, nous avons couplé Gromacs à un visualiseur et un bras haptique. Grâce à l'intégration de forces émises par l'utilisateur, celui ci peut guider des systèmes moléculaires de plus d'un million d'atomes. Notre deuxième application se concentre sur les simulations longues sur les grandes machines parallèles. Nous proposons de remplacer la méthode native d'écriture de Gromacs et de la déporter dans notre infrastructure en utilisant deux méthodes distinctes. Nous proposons également un algorithme de rendu parallèle pouvant s'adapter à différentes configurations de placements. Notre troisième application vise à étudier les usages que peuvent avoir les biologistes avec les applications in-situ. Nous avons développé une infrastructure unifiée permettant d'effectuer des traitements aussi bien sur des simulations intéractives, des simulations longues et en post-mortem.},
	urldate = {2020-09-24},
	school = {Université Grenoble Alpes (ComUE)},
	author = {Dreher, Matthieu},
	collaborator = {Raffin, Bruno},
	month = feb,
	year = {2015},
	keywords = {004, Data visualisation, In-Situ, Parallelism, Parallélisme, Parallélisme (informatique), Visualisation, Visualization},
	annote = {Sous la direction de  Bruno Raffin. Soutenue le 25-02-2015,à l'Université Grenoble Alpes (ComUE) , dans le cadre de   École doctorale mathématiques, sciences et technologies de l'information, informatique (Grenoble) , en partenariat avec  Institut national de recherche en informatique et en automatique (France). Unité de recherche (Grenoble, Isère)   (laboratoire)  .}
}

@techreport{dreher_decaf_2017,
	title = {Decaf: {Decoupled} {Dataflows} for {In} {Situ} {High}-{Performance} {Workflows}},
	shorttitle = {Decaf},
	url = {https://www.osti.gov/biblio/1372113-decaf-decoupled-dataflows-situ-high-performance-workflows},
	abstract = {The U.S. Department of Energy's Office of Scientific and Technical Information},
	language = {English},
	number = {ANL/MCS-TM-371},
	urldate = {2020-09-24},
	institution = {Argonne National Lab. (ANL), Argonne, IL (United States)},
	author = {Dreher, M. and Peterka, T.},
	month = jul,
	year = {2017},
	doi = {10.2172/1372113},
	file = {Full Text PDF:/local/home/ag262995/Zotero/storage/N9T3AW4W/Dreher and Peterka - 2017 - Decaf Decoupled Dataflows for In Situ High-Perfor.pdf:application/pdf;Snapshot:/local/home/ag262995/Zotero/storage/MDMHV73Z/1372113-decaf-decoupled-dataflows-situ-high-performance-workflows.html:text/html}
}

@inproceedings{dreher_bredala_2016,
	title = {Bredala: {Semantic} {Data} {Redistribution} for {In} {Situ} {Applications}},
	shorttitle = {Bredala},
	doi = {10.1109/CLUSTER.2016.30},
	abstract = {In situ processing is a promising solution to the problem of imbalance between computational capabilities and I/O bandwidth in current and future supercomputers. Initially designed for staging I/O, in situ middleware now can support a wide range of domains such as visualization, machine learning, filtering, and feature tracking. Doing so requires in situ middleware to manage complex heterogeneous codes using different data structures. Data need to be transformed and reorganized along the data path to fit the analysis needs. However, redistributing complex data structures is difficult. In many cases, arbitrarily splitting the arrays of a data structure destroys the semantic integrity of the data. We present Bredala, a lightweight library to annotate a data model with enough information to preserve the semantic integrity of the data during a redistribution. Bredala allows developers to describe how to split and merge a data model safely, operations usually done by in situ middleware. We evaluate the cost and performance of our library in a molecular dynamics application. We show that our data model can simplify the workflow graph of large-scale applications, improve the reusability of tasks, and offer an efficient alternative to redistribute the data.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Cluster} {Computing} ({CLUSTER})},
	author = {Dreher, Matthieu and Peterka, Tom},
	month = sep,
	year = {2016},
	note = {ISSN: 2168-9253},
	keywords = {Arrays, Bredala, code coupling, complex data structure redistribution, complex heterogeneous codes, computational capabilities, data integrity, data model, data model merging, data model splitting, data models, Data models, data structures, Data visualization, feature tracking, flow graphs, I-O bandwidth, in situ, in situ applications, in situ middleware, large-scale applications, Libraries, machine learning, merging, middleware, Middleware, molecular dynamics application, parallel machines, semantic data integrity, semantic data redistribution, Semantics, supercomputers, task reusability, workflow, workflow graph},
	pages = {279--288},
	file = {IEEE Xplore Full Text PDF:/local/home/ag262995/Zotero/storage/UNXAYLHH/Dreher and Peterka - 2016 - Bredala Semantic Data Redistribution for In Situ .pdf:application/pdf;IEEE Xplore Abstract Record:/local/home/ag262995/Zotero/storage/IP239H3P/7776520.html:text/html}
}

@inproceedings{gu_hdf5_2019,
	address = {New York, NY, USA},
	series = {{ISAV} '19},
	title = {{HDF5} as a vehicle for in transit data movement},
	isbn = {978-1-4503-7723-2},
	url = {https://doi.org/10.1145/3364228.3364237},
	doi = {10.1145/3364228.3364237},
	abstract = {For in transit processing, one of the fundamental challenges is the efficient movement of data from producers to consumers. Exploiting the flexibility offered by the SENSEI generic in situ framework, we have developed a number of different in transit data transport mechanisms. In this work, we focus on the transport mechanism that leverages the HDF5 parallel I/O library, and investigate the performance characteristics of this transport mechanism. For in transit use cases at scale on HPC platforms, one might expect that an in transit data transport mechanism that uses faster layers of the storage hierarchy, such as DRAM memory, would always outperform a transport that uses slower layers of the storage hierarchy, such as an NVRAM-based persistent storage presented as a distributed file system. However, our test results show that the performance of the transport using NVRAM is competitive with the transport that uses socket-based data movement across varying levels of producer and consumer concurrency.},
	urldate = {2020-09-24},
	booktitle = {Proceedings of the {Workshop} on {In} {Situ} {Infrastructures} for {Enabling} {Extreme}-{Scale} {Analysis} and {Visualization}},
	publisher = {Association for Computing Machinery},
	author = {Gu, Junmin and Loring, Burlen and Wu, Kesheng and Bethel, E. Wes},
	month = nov,
	year = {2019},
	keywords = {in situ analysis, in situ visualization, SENSEI},
	pages = {39--43},
	file = {Full Text PDF:/local/home/ag262995/Zotero/storage/KV4Q3VF6/Gu et al. - 2019 - HDF5 as a vehicle for in transit data movement.pdf:application/pdf}
}

@inproceedings{dorier_lessons_2015,
	address = {Austin, TX, USA},
	title = {Lessons {Learned} from {Building} {In} {Situ} {Coupling} {Frameworks}},
	isbn = {978-1-4503-4003-8},
	url = {http://dl.acm.org/citation.cfm?doid=2828612.2828622},
	doi = {10.1145/2828612.2828622},
	abstract = {Over the past few years, the increasing amounts of data produced by large-scale simulations have motivated a shift from traditional oﬄine data analysis to in situ analysis and visualization. In situ processing began as the coupling of a parallel simulation with an analysis or visualization library, motivated primarily by avoiding the high cost of accessing storage. Going beyond this simple pairwise tight coupling, complex analysis workﬂows today are graphs with one or more data sources and several interconnected analysis components. In this paper, we review four tools that we have developed to address the challenges of coupling simulations with visualization packages or analysis workﬂows: Damaris, Decaf, FlowVR and Swift. This self-critical inquiry aims to shed light not only on their potential, but most importantly on the forthcoming software challenges that these and other in situ analysis and visualization frameworks will face in order to move toward exascale.},
	language = {en},
	urldate = {2020-09-24},
	booktitle = {Proceedings of the {First} {Workshop} on {In} {Situ} {Infrastructures} for {Enabling} {Extreme}-{Scale} {Analysis} and {Visualization} - {ISAV2015}},
	publisher = {ACM Press},
	author = {Dorier, Matthieu and Dreher, Matthieu and Peterka, Tom and Wozniak, Justin M. and Antoniu, Gabriel and Raffin, Bruno},
	year = {2015},
	pages = {19--24},
	file = {Dorier et al. - 2015 - Lessons Learned from Building In Situ Coupling Fra.pdf:/local/home/ag262995/Zotero/storage/YBB9GVSD/Dorier et al. - 2015 - Lessons Learned from Building In Situ Coupling Fra.pdf:application/pdf}
}

@inproceedings{sunstaging_2016,
	title = {In-{Staging} {Data} {Placement} for {Asynchronous} {Coupling} of {Task}-{Based} {Scientific} {Workflows}},
	doi = {10.1109/ESPM2.2016.006},
	abstract = {Coupled application workflows composed of applications implemented using task-based models present new coupling and data exchange challenges, due to the asynchronous interaction and coupling behaviors between tasks of the component applications. In this paper, we present an adaptive data placement approach that addresses these challenges by dynamically adjusting to the asynchronous coupling patterns. Specifically, it places data across a set of staging cores/nodes with an awareness of the application-specific data locality requirements and the runtime task executions at these staging cores/nodes, with the goal of reducing end-to-end execution time and data movement overhead of the workflow. We experimentally demonstrate the effectiveness of our approach on the Titan Cray XK7 system using representative data coupling patterns derived from current scientific workflows. The evaluation demonstrates that our approach efficiently improves performance by reducing the time-to-solution and increasing the quality of insights for scientific discovery.},
	booktitle = {2016 {Second} {International} {Workshop} on {Extreme} {Scale} {Programming} {Models} and {Middlewar} ({ESPM2})},
	author = {Sun, Qian and Romanus, Melissa and Jin, Tong and Yu, Hongfeng and Bremer, Peer-Timo and Petruzza, Steve and Klasky, Scott and Parashar, Manish},
	month = nov,
	year = {2016},
	keywords = {Data models, Adaptation models, asynchronous coupling patterns, asynchronous interaction, Couplings, data exchange, Data processing, Data storage systems, Distributed databases, electronic data interchange, end-to-end execution time reduction, in-staging data placement, Monitoring, representative data coupling patterns, Runtime, scientific discovery, staging cores, staging nodes, task analysis, task-based scientific workflows, Titan Cray XK7 system},
	pages = {2--9},
	file = {IEEE Xplore Full Text PDF:/local/home/ag262995/Zotero/storage/A9SWWKAM/Sun et al. - 2016 - In-Staging Data Placement for Asynchronous Couplin.pdf:application/pdf;IEEE Xplore Abstract Record:/local/home/ag262995/Zotero/storage/9DML7VB4/7831554.html:text/html}
}

@article{babuji_parsl_2019,
	title = {Parsl: {Pervasive} {Parallel} {Programming} in {Python}},
	shorttitle = {Parsl},
	url = {http://arxiv.org/abs/1905.02158},
	doi = {10.1145/3307681.3325400},
	abstract = {High-level programming languages such as Python are increasingly used to provide intuitive interfaces to libraries written in lower-level languages and for assembling applications from various components. This migration towards orchestration rather than implementation, coupled with the growing need for parallel computing (e.g., due to big data and the end of Moore's law), necessitates rethinking how parallelism is expressed in programs. Here, we present Parsl, a parallel scripting library that augments Python with simple, scalable, and flexible constructs for encoding parallelism. These constructs allow Parsl to construct a dynamic dependency graph of components that it can then execute efficiently on one or many processors. Parsl is designed for scalability, with an extensible set of executors tailored to different use cases, such as low-latency, high-throughput, or extreme-scale execution. We show, via experiments on the Blue Waters supercomputer, that Parsl executors can allow Python scripts to execute components with as little as 5 ms of overhead, scale to more than 250 000 workers across more than 8000 nodes, and process upward of 1200 tasks per second. Other Parsl features simplify the construction and execution of composite programs by supporting elastic provisioning and scaling of infrastructure, fault-tolerant execution, and integrated wide-area data management. We show that these capabilities satisfy the needs of many-task, interactive, online, and machine learning applications in fields such as biology, cosmology, and materials science.},
	urldate = {2020-09-24},
	journal = {Proceedings of the 28th International Symposium on High-Performance Parallel and Distributed Computing},
	author = {Babuji, Yadu and Woodard, Anna and Li, Zhuozhao and Katz, Daniel S. and Clifford, Ben and Kumar, Rohan and Lacinski, Lukasz and Chard, Ryan and Wozniak, Justin M. and Foster, Ian and Wilde, Michael and Chard, Kyle},
	month = jun,
	year = {2019},
	note = {arXiv: 1905.02158},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Programming Languages},
	pages = {25--36},
	file = {arXiv Fulltext PDF:/local/home/ag262995/Zotero/storage/42HMURQS/Babuji et al. - 2019 - Parsl Pervasive Parallel Programming in Python.pdf:application/pdf;arXiv.org Snapshot:/local/home/ag262995/Zotero/storage/6Z87QFWM/1905.html:text/html}
}

@article{wu_diva_2020,
	title = {Diva: {A} {Declarative} and {Reactive} {Language} for {In}-{Situ} {Visualization}},
	shorttitle = {Diva},
	url = {http://arxiv.org/abs/2001.11604},
	abstract = {The use of adaptive workflow management for in situ visualization and analysis has been a growing trend in large-scale scientific simulations. However, coordinating adaptive workflows with traditional procedural programming languages can be difficult because system flow is determined by unpredictable scientific phenomena, which often appear in an unknown order and can evade event handling. This makes the implementation of adaptive workflows tedious and error-prone. Recently, reactive and declarative programming paradigms have been recognized as well-suited solutions to similar problems in other domains. However, there is a dearth of research on adapting these approaches to in situ visualization and analysis. With this paper, we present a language design and runtime system for developing adaptive systems through a declarative and reactive programming paradigm. We illustrate how an adaptive workflow programming system is implemented using our approach and demonstrate it with a use case from a combustion simulation.},
	urldate = {2020-09-24},
	journal = {arXiv:2001.11604 [cs]},
	author = {Wu, Qi and Neuroth, Tyson and Igouchkine, Oleg and Aditya, Konduri and Chen, Jacqueline H. and Ma, Kwan-Liu},
	month = sep,
	year = {2020},
	note = {arXiv: 2001.11604},
	keywords = {Computer Science - Programming Languages, Computer Science - Human-Computer Interaction},
	annote = {Comment: 11 pages, 5 figures, 6 listings, 1 table, to be published in LDAV 2020. The article has gone through 2 major revisions: Emphasized contributions, features and examples. Addressed connections between DIVA and FRP. In sec. 3, we fixed a design flaw and addressed it in sec. 3.3-3.4. Re-designed sec. 5 with a more concrete example and benchmark results. Simplified the syntax of DIVA},
	file = {arXiv Fulltext PDF:/local/home/ag262995/Zotero/storage/VPASSK2M/Wu et al. - 2020 - Diva A Declarative and Reactive Language for In-S.pdf:application/pdf;arXiv.org Snapshot:/local/home/ag262995/Zotero/storage/KJ6R3KMF/2001.html:text/html}
}

@inproceedings{zanuz_-transit_2018_flink,
	address = {Dallas Texas USA},
	title = {In-transit molecular dynamics analysis with {Apache} flink},
	isbn = {978-1-4503-6579-6},
	url = {https://dl.acm.org/doi/10.1145/3281464.3281469},
	doi = {10.1145/3281464.3281469},
	abstract = {In this paper, an on-line parallel analytics framework is proposed to process and store in transit all the data being generated by a Molecular Dynamics (MD) simulation run using staging nodes in the same cluster executing the simulation. The implementation and deployment of such a parallel workflow with standard HPC tools, managing problems such as data partitioning and load balancing, can be a hard task for scientists. In this paper we propose to leverage Apache Flink, a scalable stream processing engine from the Big Data domain, in this HPC context. Flink enables to program analyses within a simple window based map/reduce model, while the runtime takes care of the deployment, load balancing and fault tolerance. We build a complete in transit analytics workflow, connecting an MD simulation to Apache Flink and to a distributed database, Apache HBase, to persist all the desired data. To demonstrate the expressivity of this programming model and its suitability for HPC scientific environments, two common analytics in the MD field have been implemented. We assessed the performance of this framework, concluding that it can handle simulations of sizes used in the literature while providing an effective and versatile tool for scientists to easily incorporate on-line parallel analytics in their current workflows.},
	language = {en},
	urldate = {2020-12-22},
	booktitle = {Proceedings of the {Workshop} on {In} {Situ} {Infrastructures} for {Enabling} {Extreme}-{Scale} {Analysis} and {Visualization}},
	publisher = {ACM},
	author = {Zanúz, Henrique C. and Raffin, Bruno and Mures, Omar A. and Padrón, Emilio J.},
	month = nov,
	year = {2018},
	pages = {25--32},
	file = {Zanúz et al. - 2018 - In-transit molecular dynamics analysis with Apache.pdf:/local/home/ag262995/Zotero/storage/PAJIMXE8/Zanúz et al. - 2018 - In-transit molecular dynamics analysis with Apache.pdf:application/pdf}
}

@inproceedings{wang_smart_2015,
	title = {Smart: a {MapReduce}-like framework for in-situ scientific analytics},
	shorttitle = {Smart},
	doi = {10.1145/2807591.2807650},
	abstract = {In-situ analytics has lately been shown to be an effective approach to reduce both I/O and storage costs for scientific analytics. Developing an efficient in-situ implementation, however, involves many challenges, including parallelization, data movement or sharing, and resource allocation. Based on the premise that MapReduce can be an appropriate API for specifying scientific analytics applications, we present a novel MapReduce-like framework that supports efficient in-situ scientific analytics, and address several challenges that arise in applying the MapReduce idea for in-situ processing. Specifically, our implementation can load simulated data directly from distributed memory, and it uses a modified API that helps meet the strict memory constraints of in-situ analytics. The framework is designed so that analytics can be launched from the parallel code region of a simulation program. We have developed both time sharing and space sharing modes for maximizing the performance in different scenarios, with the former even avoiding any copying of data from simulation to the analytics program. We demonstrate the functionality, efficiency, and scalability of our system, by using different simulation and analytics programs, executed on clusters with multi-core and many-core nodes.},
	booktitle = {{SC} '15: {Proceedings} of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	author = {Wang, Y. and Agrawal, G. and Bicer, T. and Jiang, W.},
	month = nov,
	year = {2015},
	note = {ISSN: 2167-4337},
	keywords = {Analytical models, API, application program interfaces, data analysis, data handling, Data models, data movement, digital simulation, Distributed databases, in-situ scientific analytics, IO costs, Loading, many-core nodes, MapReduce-like framework, Memory management, microprocessor chips, multicore nodes, multiprocessing systems, natural sciences computing, parallel code region, parallel processing, Programming, resource allocation, simulation program, Smart, space sharing modes, Sparks, storage costs, time sharing modes},
	pages = {1--12},
	file = {IEEE Xplore Full Text PDF:/local/home/ag262995/Zotero/storage/S7SL2RWI/Wang et al. - 2015 - Smart a MapReduce-like framework for in-situ scie.pdf:application/pdf;IEEE Xplore Abstract Record:/local/home/ag262995/Zotero/storage/KBG96LZV/7832825.html:text/html}
}

@misc{PDI,
	title = {PDI Documentation},
	url = {https://pdi.julien-bigot.fr/master/},
	language = {en},
	urldate = {2021-05-12},
}

@misc{IPCA,
	title = {dask-ml 0.1 documentation - dask\_ml.decomposition.{IncrementalPCA}},
	url = {modules/generated/dask_ml.decomposition.IncrementalPCA.html},
	abstract = {dask\_ml.decomposition.IncrementalPCA},
	language = {en},
	urldate = {2021-05-06},
	journal = {dask-ml 0.1 documentation},
	file = {Snapshot:/local/home/ag262995/Zotero/storage/MHF6DVX8/dask_ml.decomposition.IncrementalPCA.html:text/html}
}


@incollection{yokota_tins_2018,
	address = {Cham},
	title = {{TINS}: {A} {Task}-{Based} {Dynamic} {Helper} {Core} {Strategy} for {In} {Situ} {Analytics}},
	volume = {10776},
	isbn = {978-3-319-69952-3 978-3-319-69953-0},
	shorttitle = {{TINS}},
	url = {http://link.springer.com/10.1007/978-3-319-69953-0_10},
	abstract = {The in situ paradigm proposes to co-locate simulation and analytics on the same compute node to analyze data while still resident in the compute node memory, hence reducing the need for postprocessing methods. A standard approach that proved eﬃcient for sharing resources on each node consists in running the analytics processes on a set of dedicated cores, called helper cores, to isolate them from the simulation processes. Simulation and analytics thus run concurrently with limited interference. In this paper we show that the performance can be improved through a dynamic helper core strategy. We rely on a work stealing scheduler to implement TINS, a task-based in situ framework with an on-demand analytics isolation. The helper cores are dedicated to analytics only when analytics tasks are available. Otherwise the helper cores join the other cores for processing simulation tasks. TINS relies on the Intel R TBB library. Experiments on up to 14,336 cores run a set of representative analytics parallelized with TBB coupled with the hybrid MPI+TBB ExaStamp molecular dynamics code. TINS shows up to 40\% performance improvement over various other approaches including the standard helper core.},
	language = {en},
	urldate = {2020-12-22},
	booktitle = {Supercomputing {Frontiers}},
	publisher = {Springer International Publishing},
	author = {Dirand, Estelle and Colombet, Laurent and Raffin, Bruno},
	editor = {Yokota, Rio and Wu, Weigang},
	year = {2018},
	doi = {10.1007/978-3-319-69953-0_10},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {159--178},
	file = {Dirand et al. - 2018 - TINS A Task-Based Dynamic Helper Core Strategy fo.pdf:/local/home/ag262995/Zotero/storage/VYX6LXXS/Dirand et al. - 2018 - TINS A Task-Based Dynamic Helper Core Strategy fo.pdf:application/pdf}
}

@inproceedings{zheng_goldrush_2013,
	address = {Denver, Colorado},
	title = {{GoldRush}: resource efficient in situ scientific data analytics using fine-grained interference aware execution},
	isbn = {978-1-4503-2378-9},
	shorttitle = {{GoldRush}},
	url = {http://dl.acm.org/citation.cfm?doid=2503210.2503279},
	doi = {10.1145/2503210.2503279},
	abstract = {Severe I/O bottlenecks on High End Computing platforms call for running data analytics in situ. Demonstrating that there exist considerable resources in compute nodes un-used by typical high end scientific simulations, we leverage this fact by creating an agile runtime, termed GoldRush, that can harvest those otherwise wasted, idle resources to efficiently run in situ data analytics. GoldRush uses fine-grained scheduling to “steal” idle resources, in ways that minimize interference between the simulation and in situ analytics. This involves recognizing the potential causes of on-node resource contention and then using scheduling methods that prevent them. Experiments with representative science applications at large scales show that resources harvested on compute nodes can be leveraged to perform useful analytics, significantly improving resource efficiency, reducing data movement costs incurred by alternate solutions, and posing negligible impact on scientific simulations.},
	language = {en},
	urldate = {2020-12-22},
	booktitle = {Proceedings of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis} on - {SC} '13},
	publisher = {ACM Press},
	author = {Zheng, Fang and Yu, Hongfeng and Hantas, Can and Wolf, Matthew and Eisenhauer, Greg and Schwan, Karsten and Abbasi, Hasan and Klasky, Scott},
	year = {2013},
	pages = {1--12},
	file = {Zheng et al. - 2013 - GoldRush resource efficient in situ scientific da.pdf:/local/home/ag262995/Zotero/storage/8G686UPB/Zheng et al. - 2013 - GoldRush resource efficient in situ scientific da.pdf:application/pdf}
}

@inproceedings{dreher_manala_2017,
	title = {Manala: {A} {Flexible} {Flow} {Control} {Library} for {Asynchronous} {Task} {Communication}},
	shorttitle = {Manala},
	doi = {10.1109/CLUSTER.2017.31},
	abstract = {Tasks coupled in an in situ workflow may not process data at the same speed, potentially causing overflows in the communication channel between them. To prevent this problem, software infrastructures for in situ workflows usually impose a strict FIFO policy that has the side-effect of slowing down faster tasks to the speed of the slower ones. This may not be the desired behavior; for example, a scientist may prefer to drop older data in the communication channel in order to visualize the latest snapshot of a simulation. In this paper, we present Manala, a flexible flow control library designed to manage the flow of messages between a producer and a consumer in an in situ workflow. Manala intercepts messages from the producer, stores them, and selects the message to forward to the consumer depending on the flow control policy. The library is designed to ease the creation of new flow control policies and buffering mechanisms. We demonstrate with three examples how changing the flow control policy between tasks can influence the performance and results of scientific workflows. The first example focuses on materials science with LAMMPS and a synthetic diffraction analysis code. The second example is an interactive visualization scenario with Gromacs as the producer and Damaris/Viz as consumer. Our third example studies different strategies to perform an asynchronous checkpoint with Gromacs.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Cluster} {Computing} ({CLUSTER})},
	author = {Dreher, M. and Sasikumar, K. and Sankaranarayanan, S. and Peterka, T.},
	month = sep,
	year = {2017},
	note = {ISSN: 2168-9253},
	keywords = {scientific workflows, Data models, Data visualization, Libraries, Adaptation models, Analytical models, Gromacs, parallel processing, workflow management software, asynchronous checkpoint, asynchronous task communication, buffering mechanisms, checkpointing, communication channel, Communication channels, data visualisation, FIFO policy, flexible flow control library, flow control policy, interactive systems, interactive visualization, LAMMPS, Manala, materials science, materials science computing, message flow management, message passing, software infrastructures, synthetic diffraction analysis code},
	pages = {509--519},
	file = {IEEE Xplore Full Text PDF:/local/home/ag262995/Zotero/storage/QTKTQNDF/Dreher et al. - 2017 - Manala A Flexible Flow Control Library for Asynch.pdf:application/pdf;IEEE Xplore Abstract Record:/local/home/ag262995/Zotero/storage/4LE7VNPA/8048963.html:text/html}
}

@inproceedings{lofstead_insights_2013_adios,
	title = {Insights for exascale {IO} {APIs} from building a petascale {IO} {API}},
	doi = {10.1145/2503210.2503238},
	abstract = {Near the dawn of the petascale era, IO libraries had reached a stability in their function and data layout with only incremental changes being incorporated. The shift in technology, particularly the scale of parallel file systems and the number of compute processes, prompted revisiting best practices for optimal IO performance. Among other efforts like PLFS, the project that led to ADIOS, the ADaptable IO System, was motivated by both the shift in technology and the historical requirement, for optimal IO performance, to change how simulations performed IO depending on the platform. To solve both issues, the ADIOS team, along with consultation with other leading IO experts, sought to build a new IO platform based on the assumptions inherent in the petascale hardware platforms. This paper helps inform the design of future IO platforms with a discussion of lessons learned as part of the process of designing and building ADIOS.},
	booktitle = {{SC} '13: {Proceedings} of the {International} {Conference} on {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	author = {Lofstead, J. and Ross, R.},
	month = nov,
	year = {2013},
	note = {ISSN: 2167-4337},
	keywords = {Arrays, Libraries, parallel processing, Standards, adaptable IO system, ADIOS, application program interfaces, data layout, exascale IO APIs, file organisation, Hardware, IO libraries, Layout, multiprocessing systems, optimal IO performance, parallel file systems, Performance evaluation, petascale hardware platforms, petascale IO API, PLFS, Writing},
	pages = {1--12},
	file = {IEEE Xplore Full Text PDF:/local/home/ag262995/Zotero/storage/JU2Z9GMC/Lofstead and Ross - 2013 - Insights for exascale IO APIs from building a peta.pdf:application/pdf;IEEE Xplore Abstract Record:/local/home/ag262995/Zotero/storage/LJBILI6U/6877520.html:text/html}
}

@article{ramoncortes_programming_2020_pycompss,
	title = {A {Programming} {Model} for {Hybrid} {Workflows}: combining {Task}-based {Workflows} and {Dataflows} all-in-one},
	volume = {113},
	issn = {0167739X},
	shorttitle = {A {Programming} {Model} for {Hybrid} {Workflows}},
	url = {http://arxiv.org/abs/2007.04939},
	doi = {10.1016/j.future.2020.07.007},
	abstract = {This paper tries to reduce the effort of learning, deploying, and integrating several frameworks for the development of e-Science applications that combine simulations with High-Performance Data Analytics (HPDA). We propose a way to extend task-based management systems to support continuous input and output data to enable the combination of task-based workflows and dataflows (Hybrid Workflows from now on) using a single programming model. Hence, developers can build complex Data Science workflows with different approaches depending on the requirements. To illustrate the capabilities of Hybrid Workflows, we have built a Distributed Stream Library and a fully functional prototype extending COMPSs, a mature, general-purpose, task-based, parallel programming model. The library can be easily integrated with existing task-based frameworks to provide support for dataflows. Also, it provides a homogeneous, generic, and simple representation of object and file streams in both Java and Python; enabling complex workflows to handle any data type without dealing directly with the streaming back-end.},
	urldate = {2020-12-22},
	journal = {Future Generation Computer Systems},
	author = {Ramon-Cortes, Cristian and Lordan, Francesc and Ejarque, Jorge and Badia, Rosa M.},
	month = dec,
	year = {2020},
	note = {arXiv: 2007.04939},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	pages = {281--297},
	annote = {Comment: Accepted in Future Generation Computer Systems (FGCS). Licensed under CC-BY-NC-ND},
	file = {arXiv Fulltext PDF:/local/home/ag262995/Zotero/storage/JGBXRJYZ/Ramon-Cortes et al. - 2020 - A Programming Model for Hybrid Workflows combinin.pdf:application/pdf;arXiv.org Snapshot:/local/home/ag262995/Zotero/storage/G2PQPR28/2007.html:text/html}
}

@inproceedings{cidfuentes_dislib_2019_pycompss,
	title = {dislib: Large Scale High Performance Machine Learning in Python},
	shorttitle = {dislib},
	doi = {10.1109/eScience.2019.00018},
	abstract = {In recent years, machine learning has proven to be an extremely useful tool for extracting knowledge from data. This can be leveraged in numerous research areas, such as genomics, earth sciences, and astrophysics, to gain valuable insight. At the same time, Python has become one of the most popular programming languages among researchers due to its high productivity and rich ecosystem. Unfortunately, existing machine learning libraries for Python do not scale to large data sets, are hard to use by non-experts, and are difficult to set up in high performance computing clusters. These limitations have prevented scientists to exploit the full potential of machine learning in their research. In this paper, we present and evaluate dislib, a distributed machine learning library on top of PyCOMPSs programming model that addresses the issues of other existing libraries. In our evaluation, we show that dislib can be up to 9 times faster, and can process data sets up to 16 times larger than other popular distributed machine learning libraries, such as MLlib. In addition to this, we also show how dislib can be used to reduce the computation time of a real scientific application from 18 hours to 17 minutes.},
	booktitle = {2019 15th {International} {Conference} on {eScience} ({eScience})},
	author = {Cid-Fuentes, J. Alvarez and Sola, S. and Alvarez, P. and Castro-Ginard, A. and Badia, R. M.},
	month = sep,
	year = {2019},
	keywords = {machine learning, parallel processing, big data, computation time, data mining, dislib, distributed machine learning libraries, distributed machine learning library, earth sciences, high performance computing, high performance computing clusters, hpc, large scale, large-scale high performance machine learning, learning (artificial intelligence), PyCOMPSs programming model, python, Python},
	pages = {96--105},
	file = {IEEE Xplore Full Text PDF:/local/home/ag262995/Zotero/storage/THIR4T7S/Cid-Fuentes et al. - 2019 - dislib Large Scale High Performance Machine Learn.pdf:application/pdf;IEEE Xplore Abstract Record:/local/home/ag262995/Zotero/storage/FEUN6MIV/9041782.html:text/html}
}

@inproceedings{elshazly_performance_2020_pycompss,
	title = {Performance {Meets} {Programmabilty}: {Enabling} {Native} {Python} {MPI} {Tasks} {In} {PyCOMPSs}},
	shorttitle = {Performance {Meets} {Programmabilty}},
	doi = {10.1109/PDP50117.2020.00016},
	abstract = {The increasing complexity of modern and future computing systems makes it challenging to develop applications that aim for maximum performance. Hybrid parallel programming models offer new ways to exploit the capabilities of the underlying infrastructure. However, the performance gain is sometimes accompanied by increased programming complexity. We introduce an extension to PyCOMPSs, a high-level task-based parallel programming model for Python applications, to support tasks that use MPI natively as part of the task model. Without compromising application's programmability, using Native MPI tasks in PyCOMPSs offers up to 3x improvement in total performance for compute intensive applications and up to 1.9x improvement in total performance for I/O intensive applications over sequential implementation of the tasks.},
	booktitle = {2020 28th {Euromicro} {International} {Conference} on {Parallel}, {Distributed} and {Network}-{Based} {Processing} ({PDP})},
	author = {Elshazly, H. and Lordan, F. and Ejarque, J. and Badia, R. M.},
	month = mar,
	year = {2020},
	note = {ISSN: 2377-5750},
	keywords = {Runtime, Computational modeling, message passing, application program interfaces, Python, application programmability, computing systems, Distributed Computing, High Performance Computing, high-level task-based parallel programming model, hybrid parallel programming models, Hybrid Programming Models, I-O intensive applications, MPI, native Python MPI tasks, Parallel processing, parallel programming, Parallel programming, Performance, Productivity, programming complexity, PyCOMPSs, Python applications, sequential implementation, Task analysis, Task-based Parallel Programming Models},
	pages = {63--66},
	file = {IEEE Xplore Full Text PDF:/local/home/ag262995/Zotero/storage/L2GLDBUD/Elshazly et al. - 2020 - Performance Meets Programmabilty Enabling Native .pdf:application/pdf;IEEE Xplore Abstract Record:/local/home/ag262995/Zotero/storage/Q7CHFXKZ/9092419.html:text/html}
}

@article{jaure_comparisons_nodate,
	title = {{COMPARISONS} {OF} {COUPLING} {STRATEGIES} {FOR} {MASSIVELY} {PARALLEL} {CONJUGATE} {HEAT} {TRANSFER} {WITH} {LARGE} {EDDY} {SIMULATION}},
	abstract = {The optimization of gas turbines is a complex multi-physic and multi-component problem that has long been based on engineer intuitions and expensive experiments or trial and error tests. Today, turbine experts commonly acknowledge that computer simulation is a very promising path for optimization, which can reduce costs and diminish the duration of the design process. Computations however remain a great challenge essentially because of the High Performance Computing (HPC) context, which is necessary for accurate estimates of real-life type of problems. Despite this diﬃculty, current highﬁdelity computer simulations become accessible for speciﬁc components of gas turbines [5]. These stand-alone simulations and solutions now face a new challenge: to improve the quality of the results, new physics must be introduced with speciﬁc and distinct numerical models. For example, in the context of multi-component simulations, further improving the accuracy of turbine wall temperature is of limited interest if wall temperature boundary conditions are still set approximately. Dealing with multi-physics, recent studies have shown interesting results by taking into account reactive ﬂow as well as radiative and conductive heat transfers to predict wall temperature of a helicopter combustion chamber [2, 1].},
	language = {en},
	author = {Jaure, S and Duchaine, F and Gicquel, L Y M},
	pages = {11},
	file = {Jaure et al. - COMPARISONS OF COUPLING STRATEGIES FOR MASSIVELY P.pdf:/local/home/ag262995/Zotero/storage/QAZH7ZI9/Jaure et al. - COMPARISONS OF COUPLING STRATEGIES FOR MASSIVELY P.pdf:application/pdf}
}
@article{deelman_pegasus_2005,
	title = {Pegasus: a framework for mapping complex scientific workflows onto distributed systems},
	volume = {13},
	shorttitle = {Pegasus},
	abstract = {This paper describes the Pegasus framework that can be used to map complex scientific workflows onto distributed resources. Pegasus enables users to represent the workflows at an abstract level without needing to worry about the particulars of the target execution systems. The paper describes general issues in mapping applications and the functionality of Pegasus. We present the results of improving application performance through workflow restructuring.},
	journal = {Scientific Programming Journal},
	author = {Deelman, Ewa and Singh, Gurmeet and Su, Mei-hui and Blythe, James and Gil, Yolanda and Kesselman, Carl and Mehta, Gaurang and Vahi, Karan and Berriman, G. Bruce and Good, John and Laity, Anastasia and Jacob, Joseph C. and Katz, Daniel S.},
	year = {2005},
	pages = {219--237},
	file = {Citeseer - Full Text PDF:/local/home/ag262995/Zotero/storage/KZDBKHWV/Deelman et al. - 2005 - Pegasus a framework for mapping complex scientifi.pdf:application/pdf;Citeseer - Snapshot:/local/home/ag262995/Zotero/storage/GAPIVIJV/download.html:text/html}
}
@inproceedings{deelman_pegasus_2004,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Pegasus: {Mapping} {Scientific} {Workflows} onto the {Grid}},
	isbn = {978-3-540-28642-4},
	shorttitle = {Pegasus},
	doi = {10.1007/978-3-540-28642-4_2},
	abstract = {In this paper we describe the Pegasus system that can map complex workflows onto the Grid. Pegasus takes an abstract description of a workflow and finds the appropriate data and Grid resources to execute the workflow. Pegasus is being released as part of the GriPhyN Virtual Data Toolkit and has been used in a variety of applications ranging from astronomy, biology, gravitational-wave science, and high-energy physics. A deferred planning mode of Pegasus is also introduced.},
	language = {en},
	booktitle = {Grid {Computing}},
	publisher = {Springer},
	author = {Deelman, Ewa and Blythe, James and Gil, Yolanda and Kesselman, Carl and Mehta, Gaurang and Patil, Sonal and Su, Mei-Hui and Vahi, Karan and Livny, Miron},
	editor = {Dikaiakos, Marios D.},
	year = {2004},
	keywords = {Gravitational Wave, Grid Environment, Grid Resource, High Performance Computing Application, Virtual Data},
	pages = {11--20},
	file = {Springer Full Text PDF:/local/home/ag262995/Zotero/storage/AL5XGVJW/Deelman et al. - 2004 - Pegasus Mapping Scientific Workflows onto the Gri.pdf:application/pdf}
}

@inproceedings{mehta_enabling_2012,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Enabling {Data} and {Compute} {Intensive} {Workflows} in {Bioinformatics}},
	isbn = {978-3-642-29740-3},
	doi = {10.1007/978-3-642-29740-3_4},
	abstract = {Accelerated growth in the field of bioinformatics has resulted in large data sets being produced and analyzed. With this rapid growth has come the need to analyze these data in a quick, easy, scalable, and reliable manner on a variety of computing infrastructures including desktops, clusters, grids and clouds. This paper presents the application of workflow technologies, and, specifically, Pegasus WMS, a robust scientific workflow management system, to a variety of bioinformatics projects from RNA sequencing, proteomics, and data quality control in population studies using GWAS data.},
	language = {en},
	booktitle = {Euro-{Par} 2011: {Parallel} {Processing} {Workshops}},
	publisher = {Springer},
	author = {Mehta, Gaurang and Deelman, Ewa and Knowles, James A. and Chen, Ting and Wang, Ying and Vöckler, Jens and Buyske, Steven and Matise, Tara},
	editor = {Alexander, Michael and D’Ambra, Pasqua and Belloum, Adam and Bosilca, George and Cannataro, Mario and Danelutto, Marco and Di Martino, Beniamino and Gerndt, Michael and Jeannot, Emmanuel and Namyst, Raymond and Roman, Jean and Scott, Stephen L. and Traff, Jesper Larsson and Vallée, Geoffroy and Weidendorfer, Josef},
	year = {2012},
	keywords = {bioinformatics, epigenetics, proteomics, sequencing, workflows},
	pages = {23--32},
	file = {Springer Full Text PDF:/local/home/ag262995/Zotero/storage/J3SPBBHT/Mehta et al. - 2012 - Enabling Data and Compute Intensive Workflows in B.pdf:application/pdf}
}

@article{vahi_case_2013,
	title = {A {Case} {Study} into {Using} {Common} {Real}-{Time} {Workflow} {Monitoring} {Infrastructure} for {Scientific} {Workflows}},
	volume = {11},
	issn = {1572-9184},
	url = {https://doi.org/10.1007/s10723-013-9265-4},
	doi = {10.1007/s10723-013-9265-4},
	abstract = {Scientific workflow systems support various workflow representations, operational modes, and configurations. Regardless of the system used, end users have common needs: to track the status of their workflows in real time, be notified of execution anomalies and failures automatically, perform troubleshooting, and automate the analysis of the workflow results. In this paper, we describe how the Stampede monitoring infrastructure was integrated with the Pegasus Workflow Management System and the Triana Workflow Systems, in order to add generic real time monitoring and troubleshooting capabilities across both systems. Stampede is an infrastructure that provides interoperable monitoring using a three-layer model: (1) a common data model to describe workflow and job executions; (2) high-performance tools to load workflow logs conforming to the data model into a data store; and (3) a common query interface. This paper describes the integration of Stampede monitoring architecture with Pegasus and Triana and shows the new analysis capabilities that Stampede provides to these workflow systems. The successful integration of Stampede with these workflow engines demonstrates the generic nature of the Stampede monitoring infrastructure and its potential to provide a common platform for monitoring across scientific workflow engines.},
	language = {en},
	number = {3},
	urldate = {2020-12-22},
	journal = {Journal of Grid Computing},
	author = {Vahi, Karan and Harvey, Ian and Samak, Taghrid and Gunter, Daniel and Evans, Kieran and Rogers, David and Taylor, Ian and Goode, Monte and Silva, Fabio and Al-Shakarchi, Eddie and Mehta, Gaurang and Deelman, Ewa and Jones, Andrew},
	month = sep,
	year = {2013},
	pages = {381--406},
	file = {Springer Full Text PDF:/local/home/ag262995/Zotero/storage/KBEKIBPK/Vahi et al. - 2013 - A Case Study into Using Common Real-Time Workflow .pdf:application/pdf}
}

@article{liu_survey_2015,
	title = {A {Survey} of {Data}-{Intensive} {Scientific} {Workflow} {Management}},
	volume = {13},
	issn = {1572-9184},
	url = {https://doi.org/10.1007/s10723-015-9329-8},
	doi = {10.1007/s10723-015-9329-8},
	abstract = {Nowadays, more and more computer-based scientific experiments need to handle massive amounts of data. Their data processing consists of multiple computational steps and dependencies within them. A data-intensive scientific workflow is useful for modeling such process. Since the sequential execution of data-intensive scientific workflows may take much time, Scientific Workflow Management Systems (SWfMSs) should enable the parallel execution of data-intensive scientific workflows and exploit the resources distributed in different infrastructures such as grid and cloud. This paper provides a survey of data-intensive scientific workflow management in SWfMSs and their parallelization techniques. Based on a SWfMS functional architecture, we give a comparative analysis of the existing solutions. Finally, we identify research issues for improving the execution of data-intensive scientific workflows in a multisite cloud.},
	language = {en},
	number = {4},
	urldate = {2020-12-22},
	journal = {Journal of Grid Computing},
	author = {Liu, Ji and Pacitti, Esther and Valduriez, Patrick and Mattoso, Marta},
	month = dec,
	year = {2015},
	pages = {457--493},
	file = {Springer Full Text PDF:/local/home/ag262995/Zotero/storage/ZACQDGYR/Liu et al. - 2015 - A Survey of Data-Intensive Scientific Workflow Man.pdf:application/pdf}
}

@misc{noauthor_elsevier_nodate,
	title = {Elsevier {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S2352711019302560?token=BED53BE7D684122FAE145804037BA8FA4398DEF4B459FAF82849EEFDC598D9A54F047471D1DDB103B80B8464ECE9D438},
	language = {en},
	urldate = {2020-12-22},
	doi = {10.1016/j.softx.2020.100561},
	file = {Snapshot:/local/home/ag262995/Zotero/storage/NC67JMAL/S2352711019302560.html:text/html}
}

@inproceedings{boyuka_transparent_2014_adios,
	title = {Transparent in {Situ} {Data} {Transformations} in {ADIOS}},
	doi = {10.1109/CCGrid.2014.73},
	abstract = {Though an abundance of novel "data transformation" technologies have been developed (such as compression, level-of-detail, layout optimization, and indexing), there remains a notable gap in the adoption of such services by scientific applications. In response, we develop an in situ data transformation framework in the ADIOS I/O middleware with a "plug in" interface, thus greatly simplifying both the deployment and use of data transform services in scientific applications. Our approach ensures user-transparency, runtime-configurability, compatibility with existing I/O optimizations, and the potential for exploiting read-optimizing transforms (such as level-of-detail) to achieve I/O reduction. We demonstrate use of our framework with the QLG simulation at up to 8,192 cores on the leadership-class Titan supercomputer, showing negligible overhead. We also explore the read performance implications of data transforms with respect to parameters such as chunk size, access pattern, and the "opacity" of different transform methods including compression and level-of-detail.},
	booktitle = {2014 14th {IEEE}/{ACM} {International} {Symposium} on {Cluster}, {Cloud} and {Grid} {Computing}},
	author = {Boyuka, D. A. and Lakshminarasimham, S. and Zou, X. and Gong, Z. and Jenkins, J. and Schendel, E. R. and Podhorszki, N. and Liu, Q. and Klasky, S. and Samatova, N. F.},
	month = may,
	year = {2014},
	keywords = {Arrays, Data models, middleware, Middleware, Runtime, input-output programs, ADIOS, Layout, access pattern, chunk size, compression, compression method, data handling, data transformations, data transforms, I/O middleware, I/O optimizations, I/O reduction, indexing, leadership-class Titan supercomputer, level-of-detail, level-of-detail method, plug in interface, QLG simulation, read performance implications, read-optimizing transforms, runtime-configurability, storage layout optimization, transform method opacity, Transforms, user-transparency, XML},
	pages = {256--266},
	file = {IEEE Xplore Full Text PDF:/local/home/ag262995/Zotero/storage/CWXA5WJ9/Boyuka et al. - 2014 - Transparent in Situ Data Transformations in ADIOS.pdf:application/pdf;IEEE Xplore Abstract Record:/local/home/ag262995/Zotero/storage/MIUIGEGJ/6846461.html:text/html}
}

@inproceedings{larsen_alpine_2017,
	address = {Denver CO USA},
	title = {The {ALPINE} {In} {Situ} {Infrastructure}: {Ascending} from the {Ashes} of {Strawman}},
	isbn = {978-1-4503-5139-3},
	shorttitle = {The {ALPINE} {In} {Situ} {Infrastructure}},
	url = {https://dl.acm.org/doi/10.1145/3144769.3144778},
	doi = {10.1145/3144769.3144778},
	abstract = {This paper introduces ALPINE, a flyweight in situ infrastructure. The infrastructure is designed for leading-edge supercomputers, and has support for both distributed-memory and shared-memory parallelism. It can take advantage of computing power on both conventional CPU architectures and on many-core architectures such as NVIDIA GPUs or the Intel Xeon Phi. Further, it has a flexible design that supports for integration of new visualization and analysis routines and libraries. The paper describes ALPINE’s interface choices and architecture, and also reports on initial experiments performed using the infrastructure.},
	language = {en},
	urldate = {2020-12-23},
	booktitle = {Proceedings of the {In} {Situ} {Infrastructures} on {Enabling} {Extreme}-{Scale} {Analysis} and {Visualization}},
	publisher = {ACM},
	author = {Larsen, Matthew and Ahrens, James and Ayachit, Utkarsh and Brugger, Eric and Childs, Hank and Geveci, Berk and Harrison, Cyrus},
	month = nov,
	year = {2017},
	pages = {42--46},
	file = {Larsen et al. - 2017 - The ALPINE In Situ Infrastructure Ascending from .pdf:/local/home/ag262995/Zotero/storage/TUA4YUA9/Larsen et al. - 2017 - The ALPINE In Situ Infrastructure Ascending from .pdf:application/pdf}
}

@inproceedings{ahrens_image-based_2014,
	title = {An {Image}-{Based} {Approach} to {Extreme} {Scale} in {Situ} {Visualization} and {Analysis}},
	doi = {10.1109/SC.2014.40},
	abstract = {Extreme scale scientific simulations are leading a charge to exascale computation, and data analytics runs the risk of being a bottleneck to scientific discovery. Due to power and I/O constraints, we expect in situ visualization and analysis will be a critical component of these workflows. Options for extreme scale data analysis are often presented as a stark contrast: write large files to disk for interactive, exploratory analysis, or perform in situ analysis to save detailed data about phenomena that a scientists knows about in advance. We present a novel framework for a third option - a highly interactive, image-based approach that promotes exploration of simulation results, and is easily accessed through extensions to widely used open source tools. This in situ approach supports interactive exploration of a wide range of results, while still significantly reducing data movement and storage.},
	booktitle = {{SC} '14: {Proceedings} of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	author = {Ahrens, J. and Jourdain, S. and O'Leary, P. and Patchett, J. and Rogers, D. H. and Petersen, M.},
	month = nov,
	year = {2014},
	note = {ISSN: 2167-4337},
	keywords = {Data models, Data visualization, Analytical models, Computational modeling, data analysis, data visualisation, Atmospheric modeling, Cameras, Databases, extreme scale data analysis, extreme scale in situ visualization, extreme scale scientific simulations, image processing, interactive image-based approach, open source tools, public domain software, software tools},
	pages = {424--434},
	file = {IEEE Xplore Full Text PDF:/local/home/ag262995/Zotero/storage/REERKL7J/Ahrens et al. - 2014 - An Image-Based Approach to Extreme Scale in Situ V.pdf:application/pdf;IEEE Xplore Abstract Record:/local/home/ag262995/Zotero/storage/8DQRDR7C/7013022.html:text/html}
}

@INPROCEEDINGS{daskpp1,
       author = {{Redl}, Robert and {Keil}, Christian and {Craig}, George and {Lerch}, Sebastian and {Eichhorn}, Joachim},
        title = "{Towards a Framework for Parallelized Post-Processing and Evaluation of Ensemble Forecasts}",
    booktitle = {EGU General Assembly Conference Abstracts},
         year = 2018,
       series = {EGU General Assembly Conference Abstracts},
        month = apr,
        pages = {12322},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018EGUGA..2012322R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{daskpp2,
author = {Paraskevakos, Ioannis and Luckow, Andre and Khoshlessan, Mahzad and Chantzialexiou, George and Cheatham, Thomas E. and Beckstein, Oliver and Fox, Geoffrey C. and Jha, Shantenu},
title = {Task-Parallel Analysis of Molecular Dynamics Trajectories},
year = {2018},
isbn = {9781450365109},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3225058.3225128},
doi = {10.1145/3225058.3225128},
abstract = {Different parallel frameworks for implementing data analysis applications have been proposed by the HPC and Big Data communities. In this paper, we investigate three task-parallel frameworks: Spark, Dask and RADICAL-Pilot with respect to their ability to support data analytics on HPC resources and compare them to MPI. We investigate the data analysis requirements of Molecular Dynamics (MD) simulations which are significant consumers of supercomputing cycles, producing immense amounts of data. A typical large-scale MD simulation of a physical system of O(100k) atoms over μsecs can produce from O(10) GB to O(1000) GBs of data. We propose and evaluate different approaches for parallelization of a representative set of MD trajectory analysis algorithms, in particular the computation of path similarity and leaflet identification. We evaluate Spark, Dask and RADICAL-Pilot with respect to their abstractions and runtime engine capabilities to support these algorithms. We provide a conceptual basis for comparing and understanding different frameworks that enable users to select the optimal system for each application. We also provide a quantitative performance analysis of the different algorithms across the three frameworks.},
booktitle = {Proceedings of the 47th International Conference on Parallel Processing},
articleno = {49},
numpages = {10},
keywords = {Data analytics, task-parallel, MD Simulations Analysis, MD analysis},
location = {Eugene, OR, USA},
series = {ICPP 2018}
}
@inproceedings{daskpp3,
author = {Khoshlessan, Mahzad and Paraskevakos, Ioannis and Jha, Shantenu and Beckstein, Oliver},
year = {2017},
month = {01},
pages = {64-72},
title = {Parallel Analysis in MDAnalysis using the Dask Parallel Computing Library},
doi = {10.25080/shinma-7f4c6e7-00a}
}

@INPROCEEDINGS{daskpp4,
       author = {{Barnes}, W. and {Cheung}, C.~M.~M. and {Bobra}, M.},
        title = "{The Sun at Scale: Interactive Analysis of High Resolution EUV Imaging Data on HPC Platforms with Dask}",
     keywords = {1976 Software tools and services, INFORMATICS, 1982 Standards, INFORMATICS, 7594 Instruments and techniques, SOLAR PHYSICS, ASTROPHYSICS, AND ASTRONOMY},
    booktitle = {AGU Fall Meeting Abstracts},
         year = 2019,
       volume = {2019},
        month = dec,
          eid = {SH41C-3317},
        pages = {SH41C-3317},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019AGUFMSH41C3317B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@INPROCEEDINGS{daskpp5,
    author={Nguyen, Mai H. and Li, Jiaxin and Crawl, Daniel and Block, Jessica and Altintas, Ilkay},
    booktitle={2019 IEEE International Conference on Big Data (Big Data)},
    title={Scaling Deep Learning-Based Analysis of High-Resolution Satellite Imagery with Distributed Processing},
    year={2019},
    pages={5437-5443},
    doi={10.1109/BigData47090.2019.9006205}
}

@article{YuuichiPCA,
author = {Asahi,Yuuichi  and Fujii,Keisuke  and Heim,Dennis Manuel  and Maeyama,Shinya  and Garbet,Xavier  and Grandgirard,Virginie  and Sarazin,Yanick  and Dif-Pradalier,Guilhem  and Idomura,Yasuhiro  and Yagi,Masatoshi },
title = {Compressing the time series of five dimensional distribution function data from gyrokinetic simulation using principal component analysis},
journal = {Physics of Plasmas},
volume = {28},
number = {1},
pages = {012304},
year = {2021},
doi = {10.1063/5.0023166},
URL = { https://doi.org/10.1063/5.0023166},
eprint = { https://doi.org/10.1063/5.0023166}
} 

@inproceedings{bigot:hal-01050322,
 address = {Luminy, France},
 author = {Bigot, Julien and Grandgirard, Virginie and Latu, Guillaume and Passeron, Chantal and Rozar, Fabien and Thomine, Olivier},
 booktitle = {ESAIM: PROCEEDINGS},
 doi = {10.1051/proc/201343007},
 hal_id = {hal-01050322},
 hal_version = {v1},
 month = {July},
 pages = {117-135},
 pdf = {https://hal.inria.fr/hal-01050322/file/proc134308.pdf},
 series = {43},
 title = {Scaling GYSELA code beyond 32K-cores on Blue Gene/Q},
 url = {https://hal.inria.fr/hal-01050322},
 volume = {CEMRACS 2012},
 year = {2012}
}

@inproceedings{latu:hal-01719208,
 address = {Lyon, France},
 author = {Latu, Guillaume and ASAHI, Yuuichi and Bigot, Julien and Fehér, Tamás and Grandgirard, Virginie},
 booktitle = {SBAC-PAD 2018, WAMCA workshop},
 hal_id = {hal-01719208},
 hal_version = {v2},
 keywords = {SIMD ; KNL ; plasma physics ; vectorization ; many-core},
 month = {September},
 pdf = {https://hal.inria.fr/hal-01719208v2/file/wamca18_gl.pdf},
 series = {SBAC-PAD 2018 proceedings},
 title = {Scaling and optimizing the Gysela code on a cluster of many-core processors},
 url = {https://hal.inria.fr/hal-01719208},
 year = {2018}
}

@inproceedings{latu:hal-01834323,
 address = {Lausanne, France},
 author = {Latu, Guillaume and Bigot, Julien and Bouzat, Nicolas and Gimenez, Judit and Grandgirard, Virginie},
 booktitle = {PASC '16 - Proceedings of the Platform for Advanced Scientific Computing Conference},
 doi = {10.1145/2929908.2929912},
 hal_id = {hal-01834323},
 hal_version = {v1},
 month = {June},
 publisher = {ACM Press},
 title = {Benefits of SMT and of Parallel Transpose Algorithm for the Large-Scale GYSELA Application},
 url = {https://hal.archives-ouvertes.fr/hal-01834323},
 year = {2016}
}


