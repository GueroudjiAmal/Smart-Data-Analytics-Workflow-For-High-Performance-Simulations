\chapter{Deisa}



\epigraph{\textit{If I have seen further, it is by standing on the shoulders of Giants.}} {Issac Newton}



\section{introduction}

\section{The Bridging Model between BSP and Distributed Task-Based Paradigms}

\subsection{Motivation}
As already mentioned in the previous chapters, the in situ paradigm is a brilliant alternative to the post hoc. However, it becomes less relevant due to its setup complexity, which is due to the incompatibility of the MPI programming model with data analytics algorithms. To be able to explore the in situ paradigm with reduced complexity, we have mainly two possibilities: build both the simulation and the in situ tool with a higher-level programming model that makes the analytics easier to design, or keep the simulation built on MPI and propose a bridging model between MPI and another programming model which is more adapted for analytics.

The first possibility is not a relevant solution because it means that we will need to write simulation codes in another programming model, likely higher-level and slower, which is not a good idea for the reasons we mentioned in section\ref{BSP} about the success of MPI and BSP in general. 
The second possibility is more interesting as it will keep the BSP programming model for the simulations, choose a more adapted model for analytics and propose a bridging model to couple them. 

In this work, we have opted for the second solution and chosen the task-based programming model for the in situ analytics for the easiness it brings as well as the variety of tools that are already available and usable for different data processing.     

\subsection{The Bulk Task Parallel bridging model}

We define the Bulk Task Parallel(BTP), the new bridging model that couples BSP with distributed task-based programs. In the frame of this work, we only consider a producer/consumer scheme where the producer is parallelized following the BSP model and the consumer is in a task-based model.  The BTP is built on top of basic concepts, namely: BTP tasks (section~\ref{btptask}), exchange points (section~\ref{EP}) and delivery facilities (section~\ref{DF}).  

Let an \textbf{\textit{Actor}} be the union of a code and all its necessary resources: processing and memory units.   
For instance, let $A_{BSP}^{R_{1}}$ be an \textit{Actor} where $A$ is a BSP implementation of a problem $P_{1}$, and $R1$ is the needed resources for $P_{1}$ to be run, and let $B_{Task}^{R_{2}}$ be an \textit{Actor} where $B$ is a Task-based implementation of a problem $P_{2}$, and $R_{2}$ is the needed resources.

\subsubsection{BTP Tasks}\label{btptask}
Let $S_{BSP}^{R_{s}}$ be an iterative code (a simulation code for instance). Let \textbf{C} be a computation and let \textbf{Cmn} be a communication performed internally to the \textit{Actor} (MPI communication for instance). The smallest \textit{Task} that can be defined in a BSP model is the union of all computations \textbf{C} that are delimited by two communications \textbf{Cmn}. 

A \textit{BTP task} can be seen as a \textit{Task} where we distinguish internal and external communications. It is a set of computations \textbf{C} and internal communications \textbf{Cmn}, delimited by two \textit{Exchange Point} (a possible external communication). In \textit{Task-based} model we distinguish the internal (to the \textit{Actor}) input/output data from the external ones (where the source/destination) is another \textit{Actor}. Hence, a \textit{BTP task} is a subgraph that is delimited by two external communications, done through \textit{Exchange Points}.


\begin{figure}[tb]\centering
\includegraphics[width=0.75\columnwidth]{figures/BTPTaskGraph.pdf}
\caption{BTP task graph}
\label{figWUG}
\end{figure}

\subsubsection{Exchange Point}\label{EP}

An \textit{Exchange Point} (EP) can be defined as an entry point to BTP model. An \textit{Actor} shares through it an internal data with other \textit{Actors}. The main motivation to introduce \textit{Exchange points} is to keep a good separation of concerns. It provides a clean way to couple \textit{Actors}. For instance \textit{Actor A} can through its \textit{EPs} expose an array \textit{A} for external use. \textit{Actor B} does not need to know how \textit{Actor A} computes this array, and \textit{Actor A} needs to know neither which data \textit{Actor B} needs nor how it will use it.

The only way to get information about data an \textit{Actor} generates is by establishing a connection with it through a \textit{Delivery Facility} and checking available data (the data that \textit{A} wants to share) in the \textit{Exchange points}.

\subsubsection{Delivery Facility}\label{DF}

In addition of ensuring the establishment of connections between \textit{Actors}, the \textit{Delivery Facility} (DF) ensures a global identification and redistribution of data between \textit{Actors}. Hence,  it can be split to 3 main components : a \textit{Connection Facility} a \textit{Data Identification Component}, a \textit{Data Redistribution Component}.
\begin{itemize}
   \item \textit{Connection Facility} (CF): it establishes connections and communication with remote actors.
   \item \textit{Data Identification Component} (DIC): is responsible for the identification of a piece of data internally in an \textit{Actor} and translate its identifier to a global ID understandable in other \textit{Actors}. The step of identification is essential due to the different ways data is defined in the two paradigms. 
   
   \item \textit{Data Redistribution Component} (DRC): implements a data redistribution scheme. It is aware of the number of the resources $R$ (processes or workers) in the \textit{Actor} $Consumer_{Paradigm}^{R}$ and maps each data to a set of resources.
\end{itemize}

\subsection{Porting a BSP code to BTP semantic: producer use case}

\begin{figure*}[tb]
\centerline{\includegraphics[width=\textwidth]{figures/unrolled.pdf}}
\caption{Example of BTP representation for an iterative BSP code}
\label{figunroll}
\end{figure*}

We suppose that the BSP \textit{Actor} is an iterative code parallelized in MPI. The code does not need to be rewritten to be designed in the BTP semantic. What needs to be done is only to tag the data we want to share externally and specify when this will be done. Concretely, calls to \texttt{Exchange\_point} are added in the simulation code to make data available for external use. By definition, a \textit{BTP task}, in BSP words, is a set of computations and internal communications that are delimited by two \textit{EPs}; concretely, it is all the code that is delimited by two calls of \texttt{Exchange\_point}. The call to \texttt{Exchange\_point} that is added at the end of each iteration in the loop in fig.~\ref{figunroll} is enough to construct a list of \textit{BTP tasks}, one per each time step. It is similar to an unrolled loop over time.


\subsection{Producer Consumer Full Example}

 
\begin{figure}[tb]\centering
\includegraphics[width=\columnwidth]{figures/BTP.pdf}
\caption{producer consumer example}
\label{figBTP}
\end{figure}

Figure~\ref{figBTP} shows an example coupling $A_{BSP}^{R_{n}}$ and $B_{Task}^{R_{m}}$ where \textit{Actor A} is a producer and \textit{Actor B} is a consumer. The BSP \textit{Actor} has $R_{n}$ resources. Each $Task_k$ is scheduled explicitly to a set of resources $R_{k}$. In scientific applications, we usually have an iterative code. Each \textit{Task} generates a block of data at a given timestep $t$ (only one \textit{Task} is shown in the figure, with a loop mark). Those blocks of the data (small blue boxes $D_{i,j}$) are shared through the \textit{Exchange points} (not represented in the figure) and sent to the \textit{Delivery Facility}.
\textit{Actor B} is the consumer of the data. It is a task-based \textit{Actor}, has $R_{m}$ resources that are managed implicitly by a runtime (blue boxes with grey hachures). A task graph is represented as a BTP \textit{Task} graph (yellow graph), with dependencies on external inputs. Those inputs (in red) are data with new keys (IDs) that are easily recognizable, thus usable in \textit{Actor B}.  

The data are sent through the network between \textit{Actors}. The \textit{DFs} ensure the connection to a distant \textit{Actor}, the identification and redistribution of data between \textit{Actors}. 
In this figure, the data identification is done in two steps from each side. The small blue boxes $D_{i,j}$ are identified by three elements: $D$ is the name of the data, $i$ is the MPI rank (for instance or the position of this block of data in the global distribution), and the $j$ corresponds to the timestep. These keys can be considered as local to the \textit{Actor A}. 
In the same Actor, a new key has been created $I_{l,m}$. It is a global key recognizable in the \textit{DF} of \textit{Actor B}. In the \textit{Actor B}, those keys are translated to new keys internally understandable $D'_{k}$. 
This identification and translation process can be done in fewer steps. For example, if $I_{l,m}$ is recognizable by \textit{Actor B}, then there is no need for further translation at reception.

\section{BTP Implementation}


\subsection{Data Model}\label{sec:datamodel}
Data is one of the core concepts of our work; its definition, identification, representation, and communication are as important as its processing. 
\subsubsection{Data}
from ptr - pdi pybind -  pycall - numpy - dask

\subsubsection{Metadata}
what and how and why 
keys we get from a scatter, the size and position are sent 
used for dask array creation 
\subsubsection{Deisa virtual arrays}
from the simulation side as a mirror of the dask array that we create in the client side.

\subsection{Data and control communication}
\subsubsection{Control}
All control goes through the scheduler, but we can optimize it through mpi as future work 
\paragraph{Queues} 

\paragraph{Variables}
when several clients need the same data 

\subsubsection{Data communication}
\paragraph{Deisa Bridge}
deisa plugin/ pycall
light client + far heartbeat 
limit 512 ..

\subsection{Data semantic}
in /out  data in a task vs same var and different values per time step in mpi

\subsection{Time-independent task submission}

\subsection{Steering}
limit generated data, but steering in the other sense can be done easily  
\subsubsection{Queues}
\subsubsection{Contracts}
\subsection{Dynamic scheduling}
triggers I have to do that 

\amal{pas implementation je ne sais pas trop }
\subsection{Implementation}

\cite{deisa}

\subsubsection{Architecture}
\begin{figure}[tb]\centering
\includegraphics{figures/ArchiectureDeisa.pdf}
\caption{\deisa}
\label{figdeida}
\end{figure}

\begin{figure}[tb]\centering
\includegraphics{figures/ArchiectureDeisaV2.pdf}
\caption{\deisa v2}
\label{figdeidav2}
\end{figure}
\subsubsection{API}
just the last one 
\subsubsection{Instrumentation}
\begin{lstlisting}[float, label=ymldata, language=yaml, caption=Data description in \pdi \deisa YAML file]
types: #[...] including config_t description
metadata: {step: int, cfg: config_t, rank: int} |\label{ymldata:metadata}|
data: 
plugins:
  mpi: # get MPI rand and size
  deisa:
    scheduler_info: scheduler.json
    init_on: init 
    time_step: $step 
    deisa_arrays: # Deisa Virtual arrays
      G_temp: # Field name
        type: array
        subtype: double
        size:
          -timedim 
          -'$cfg.loc[0] * ($rank % $cfg.proc[0])'
          -'$cfg.loc[1] * ($rank / $cfg.proc[0])'
        subsize: [1, '$cfg.loc[0]', '$cfg.loc[1]'] # chunk size
        start:  # chunk start
          -$step
          -'$cfg.loc[0] * ($rank % $cfg.proc[0])'
          -'$cfg.loc[1] * ($rank / $cfg.proc[0])'
        +timedim: 0 # a tag for the time dimension
    map_in: # deisa array mapping
      temp: G_temp
        
\end{lstlisting}
\subsubsection{Evaluation}
\subsubsection{Limitations}

\section{Conclusion}