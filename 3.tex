\chapter{Theory Framework}

\epigraph{\textit{If I have seen further, it is by standing on the shoulders of Giants.}} {Issac Newton}
\newpage

 \amal{refs ici}

\section{In situ processing}
The in situ paradigm stands for processing the data generated by a simulation code as close as possible to when and where they are generated. The paradigm has been used first for visualization\cite{} and then extended for general purpose processing. There are several ways to perform in situ analysis, either by time or space sharing with the simulation processes. They can share the same thread, the same process, or just the same node; the processing in those cases is called In situ. If they only share the same platform, then we say in-transit processing.



\section{Bulk Synchronous Parallel Paradigm}

\subsection{History (do u have refs?)}
\subsection{BSP paradigm} 
A Bulk-Synchronous Parallel computer combines three attributes : 
\begin{itemize}
    \item A number of components, each performing processing and/or memory functions.
    \item A router that delivers messages point to point between pairs of components.
    \item Facilities for synchronizing all or a subset of the components at regular intervals of L time units where L is the Periodicity parameter.
\end{itemize}

A BSP machine can be implemented using the well-known communication interface: Massage Passing Interface (MPI), where we define our components as a set of $P$ processes defined through a common communicator. Each process has its separate local memory, performs a set of computations on its local data, and then exchanges some results with other processes. A local/global synchronization is performed periodically. 

Such a paradigm suits very well the scientific applications where models are regular. The global domain is decomposed within the processes. Each process has its local buffers updated as the simulation progresses, and parts of those buffers are exchanged with a subset of processes when needed. 
Not only does the paradigm seem natural to design regular applications (which is the case for high-performance simulations), but its implementations (specifically using MPI) respond to a critical requirement of HPC application, which is the high performance itself.

In MPI, the user manages resources and scheduling explicitly; while this is not a big issue for regular algorithms, it can quickly become complicated to manage for irregular ones.

\section{Distributed Task-based Programming}

\subsection{History (do u have refs?)}

\subsection{Task-based Paradigm} 

In the task-based paradigm, we define three main attributes as well :
\begin{itemize}
    \item A number of tasks, each task performs a given job. It is defined as a sequence of instructions. A task can be either fine-grained or of a coarser granularity. 
    \item Dependencies between tasks. A task can only be executed when all its dependencies are resolved.
    \item An engine that manages and schedules the execution of those tasks on a set of physical processes.
\end{itemize}

One of the main motivations for introducing such a model is to create higher-level abstractions that make the design and implementation of parallel algorithms easier. The resource and the task scheduling are managed by a runtime, implicitly from the user's point of view. The task-based models can be less efficient in terms of performance compared to BSP-based models due to the overheads that may be introduced during runtime. However, they widely increase productivity with the simplicity they introduce in designing and maintaining non-trivial applications.   


\section{Discussion} \label{discussion}

The BSP and the Task-based paradigms differ not only in terms of abstraction levels, development effort, and performance but also in defining key concepts such as parallelism and the data and how they are managed.

First of all, the type of parallelism in BSP, let us take MPI as an example; the application is represented as a set of $P$ processes, each with its local memory and buffers. The parallelism in the task-based model is expressed in terms of tasks and dependencies. While the user is responsible for creating the processes and their management in MPI, a runtime ensures that job in task-based models. We talk about explicit parallelism in MPI and implicit one in the task-based programming model. 

Secondly, the data semantics and representation in those models are different too. As defined in MPI, the data are buffers that keep the same name during execution and whose values are updated as the simulation progresses. Hence the data in MPI can be defined as the value of a given variable at a specific moment.
In task-based models, data are immutable and defined either as an input or an output of a given task. While the timestep is important to identify needed data in MPI, it does not have a similar signification in a task-based model. 

The third big difference is related to the view we may have about an application in both cases. While a task graph describes all the tasks that will be run in a task-based model, it is complex and sometimes impossible for BSP applications to have such a view from the beginning. And this makes the coupling more challenging, as we do not have any a priori idea about what will be executed at runtime, the data that will be generated, and when they are shared.

Those conceptual differences make coupling codes coming from the two paradigms challenging. To reduce this complexity and take advantage of both paradigms, we propose the Bulk Task Parallel paradigm that brings together the BSP and the task-based paradigms. 


\section{Conclusion}


